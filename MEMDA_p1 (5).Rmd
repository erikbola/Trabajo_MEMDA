---
title: "proyecto regression"
author: "Erik Bola, Vega Carmona, Emma Carretero, Sandra González y Raquel Purroy"
date: 
output:
  html_document: default
  pdf_document: default
---
# 0. Paquetes y Datos
```{r setup, include=FALSE}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
library(DataExplorer)
library(inspectdf)
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)

# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
```

```{r}
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# 1. Descripción del problema
```{r, warning=FALSE}
str(datos)
```
Preliminares
```{r}
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)

clases <- sapply(datos, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```


Comprobación
```{r}
str(datos)
```

# 2. Análisis Exploratorio

## 2.1. Univariant Analysis

### 2.1.1. Numerical

Descrpition
```{r}
library(psych)
psych::describe(datos[, varNum])
```

En base a este resumen estadístico se puede afirmar que la mayoría de las canciones no son en vivo, dado que el promedio de la variable $liveness$ es bastante bajo. Respecto a el volumen de las canciones, teniendo en cuenta que $loudness$ es una característica que oscila entre -36.73dB y 1.34dB, su valor medio parece indicar que es relativamente alto. 

La media de $danceability$ apunta que las canciones son bastante bailables, y el valor esperado de $audio_valence$ muestra que las canciones tienden a tener emociones más alegres y positivas, ya que es ligeramente superior a 0,5. 

Asimismo, haciendo referencia a cómo de energéticas o animadas son dichas canciones, $energy$ señala que, en promedio, puede decirse que lo son bastante. 

En cuanto al grado acústico, observando que el valor medio es bajo, se podría deducir que mayoritariamente las canciones son producidas electrónicamente. 

Por último, se podría añadir que gran parte de las canciones no son habladas, pues la variable $speechiness$ tiene un promedio bajo. Destaca la variabilidad del nivel de popularidad ($song_popularity$), con canciones desde poco conocidas hasta muy populares.

Graphics (base)
```{r}
for (var in varNum[-1]) {
  # Acceder a la columna por nombre y asegurarse de que es numérica
  column_data <- as.numeric(datos[[var]])
  
  # Comprobar si la columna es numérica
  if (is.numeric(column_data)) {
    # Eliminar NAs
    column_data <- na.omit(column_data)
    
    # Comprobar si quedan datos después de eliminar NAs
    if (length(column_data) > 0) {
      hist(column_data, main = paste0("Histograma variable ", var))
      boxplot(column_data, main = paste0("Boxplot variable ", var))
    } else {
      warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
    }
  } else {
    warning(paste("La variable", var, "no es numérica y será ignorada"))
  }
}

par(mfrow = c(1, 1))


```

Graphics (ggplot2 + patchwork)
```{r, eval=FALSE}
library(ggplot2)
library(patchwork)

plots <- list()

for (var in varNum) {
  histo <- ggplot(datos, aes(x = .data[[var]])) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
    geom_density(alpha = .2, fill = "#FF6666") +
    geom_vline(aes(xintercept = mean(.data[[var]], na.rm = TRUE)),
               color = "blue", linetype = "dashed", linewidth = 1) +
    ggtitle(paste("Histograma de", var))
  boxp <- ggplot(datos, aes(x = .data[[var]])) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) +
    ggtitle(paste("Boxplot de", var))

  plots <- append(plots, list(histo, boxp))
  print(plots)
}
```

Combinar en un grid automático con 2 columnas
```{r, eval=FALSE, include=FALSE}
final_plot <- Reduce(`+`, plots) + plot_layout(ncol = 2)
final_plot
```

### 2.1.2. Categorical

Description
```{r}
for (var in varCat) {
  tablaAbs <- data.frame(table(datos[, var]))
  tablaFreq <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  m <- match(tablaAbs$Var1, tablaFreq$Var1)
  tablaAbs[, "FreqRel"] <- tablaFreq[m, "Freq"]
  colnames(tablaAbs) <- c("Categoria", "FreqAbs", "FreqRel")
  cat("===============", var, "===================================/n")
  print(tablaAbs)
  cat("==================================================/n")
}
```
Las frecuencias absolutas de las variables categóricas, muestran, en general, una distribución equitativa de las modalidades. Sin embargo, la variable $time_signature$ posee un mayor número de casos en la cuarta categoría.

Graphics (base)
```{r}
# Configurar la ventana gráfica para mostrar 2 filas y 3 columnas
par(mfrow = c(2, 3))

# Crear gráficos de barras con títulos personalizados
for (var in varCat) {
  if (var == "time_signature") {
    barplot(table(datos[, var]),
            main = "Distribución de Time Signature",
            col = "skyblue")
  } else if (var == "key") {
    barplot(table(datos[, var]),
            main = "Distribución de Key",
            col = "lightgreen")
  } else if (var == "audio_mode") {
    barplot(table(datos[, var]),
            main = "Distribución de Audio Mode",
            col = "lightcoral")
  } else {
    barplot(table(datos[, var]),
            main = var,  # título genérico si hay más variables
            col = "gray80")
  }
}

# Restaurar la configuración original
par(mfrow = c(1, 1))
```

-   **time_signature**:

la categoría 4 es la más frecuente, con casi 8000 canciones. Esto corresponde al compás 4/4, el más común en música popular. El resto de niveles, 0, 1, 3 y 5 son mucho menos frecuentes, lo que indica que compases alternativos o inusuales son raros en esta colección de canciones.

-   **key**:

La distribución es bastante uniforme, con cada tonalidad apareciendo entre 1000 y 2000 veces. No hay una tonalidad claramente dominante, lo que sugiere diversidad armónica en el conjunto de datos.

-   **audio_mode**:

Predominan las canciones en modo mayor, lo que puede reflejar una tendencia hacia música más optimista o comercial.

Graphics (ggplot2 + gridExtra)
```{r}
library(gridExtra)

plots <- list()  # lista vacía
i <- 1           # índice

for (var in varCat) {
  tabla <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  p <- ggplot(data = tabla, aes(x = Var1, y = Freq)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(round(Freq * 100, 2), "%")),
              vjust = 1.6, color = "white", size = 3.5) +
    theme_minimal() +
    labs(title = paste("Distribución de", var), x = var, y = "Proporción")

  plots[[i]] <- p
  i <- i + 1
}
# Mostrar todos los gráficos en un grid (ejemplo con 2 columnas)
# grid.arrange(grobs = plots, ncol = 2)
```

## 2.2. Bivariant Analysis

#### 2.2.1 Numerical vs. numerical

Description

```{r}
cor(na.omit(datos[, varNum]))
```

Las correlaciones con valores más extremos se encuentran entre las variables **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Graphics (base / PerformanceAnalytics)

```{r}
library(PerformanceAnalytics)
chart.Correlation(as.matrix(datos[, varNum]), histogram = TRUE, pch = 12)
```

Graphics (ggplot2 / ggcorrplot)

```{r, include=FALSE, eval=FALSE}
#install.packages("ggcorrplot")
library(ggcorrplot)
corr <- round(cor(datos[, varNum]), 1)
ggcorrplot(corr, lab = TRUE)
```

#### 2.2.2 Numerical vs. categorical

Description

```{r}
for (varN in varNum) {
  for (varC in varCat) {
   print(psych::describeBy(datos[, varN], group = datos[, varC]))
  }
}
```

Graphics (ggplot2)

```{r}
library(ggplot2)
library(gridExtra)

plots <- list()
i <- 1

for (varC in varCat) {
  for (varN in varNum) {
    grafico <- ggplot(datos, aes(x = .data[[varN]], fill = .data[[varC]])) +
      geom_histogram(colour = "black",
                     lwd = 0.75,
                     linetype = 1,
                     position = "identity",
                     alpha = 0.5) +
      labs(title = paste("Histograma de", varN, "por", varC),
           x = varN, y = "Frecuencia", fill = varC) +
      theme_minimal()
    plots[[i]] <- grafico
    i <- i + 1
  }
}
# Mostrar todos en un grid (2 columnas)
grid.arrange(grobs = plots, ncol = 2)
```

#### 2.2.3 Categorical vs. categorical

Description

```{r}
for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      prop_table <- prop.table(table(datos[[varc1]], datos[[varc2]]))
      cat("=============", varc1, " vs. ", varc2, "=========================\n")
      print(prop_table)
    }
  }
}

```

Graphics (base)

```{r}
par(mfrow = c(3, 3))
par(mar = c(3, 3, 3, 1))  

for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      tab <- table(datos[[varc1]], datos[[varc2]], useNA = "no")
      prop_tab <- prop.table(tab, margin = 2)

      barplot(prop_tab,
              beside = TRUE,
              ylim = c(0, 1),
              main  = paste(varc1, "vs", varc2),
              xlab  = varc2,
              ylab  = paste("Proporción dentro de", varc2))
      legend("topright", legend = rownames(prop_tab), cex = 0.7, bty = "n")
    }
  }
}

par(mfrow = c(1, 1))

```

# 3. EDA (Automatic Descriptive Analysis)

## 3.1. Skim

```{r}
library(skimr)
library(tidyverse)

# Podem visualitzar un descriptiu de les dades
skim(datos)
```

```{r}
# Visualitzem exclusivament les variables numériques
skim(datos) %>% yank("numeric")
```

```{r}
skim(datos) %>% yank("character")
```

## 3.2 Vis

```{r}
library(visdat)
## Busquem per a variables numériques o categóriques si hi ha NA's
vis_dat(datos)
```

Este gráfico representa que la mayoría de las variables son numéricas, con menos cantidad de variables categóricas. También se osberva que hay un gran número de valores missings en los dos tipos de variables.

```{r}
## Visualitzem percentatges de NA's en les variables
vis_miss(datos)
```

Concretamente, hay un total de 26% de valores perdidos.

```{r}
## Generem la matriu de correlacions
datos %>% dplyr::select(where(is.numeric)) %>% vis_cor()
```

Mediante la visualización de este gráfico de correlaciones, se refuerzan las relaciones lineales entre los diferentes pares de variables.

```{r}
## Podem visualitzar condicionants de les dades. En aquest cas, mirem si tenim mes de
## 2 clases
vis_expect(datos, ~ .x > 2)
```

## 3.3. Inspect df

```{r}
library(inspectdf)

## Tipus de dades
inspect_types(datos) %>% show_plot()
```

La base de datos contiene 3 variables factor, y 11 cuantitativas.

```{r}
## Utilització de la memoria
inspect_mem(datos) %>% show_plot()
```

```{r}
# Paquetes
library(dplyr)
library(inspectdf)
library(rlang)

# --- 1) Crear una categórica desde una numérica y comparar (High/Low) ---
# Usamos 'song_popularity' como ejemplo con umbral 50.
umbral <- 50
num_target <- "song_popularity"

stopifnot(num_target %in% c(
  "liveness","loudness","danceability","song_duration_ms","audio_valence",
  "energy","tempo","acousticness","speechiness","instrumentalness","song_popularity"
))

# Convertir a numérico por si viniera como texto
datos[[num_target]] <- suppressWarnings(as.numeric(datos[[num_target]]))

data_price_dummy <- datos %>%
  mutate(price_dummy = if_else(.data[[num_target]] > umbral, "High", "Low") %>% factor())

# Comparativa de NA entre High y Low
inspect_na(
  data_price_dummy %>% filter(price_dummy == "High"),
  data_price_dummy %>% filter(price_dummy == "Low")
) %>% show_plot()

```

```{r}
## Comprovem la distribució de les variables
inspect_num(datos) %>% show_plot()
```

```{r}
## check categorical variable distribution
inspect_imb(datos) %>% show_plot()
```

```{r}
## check two categorical
inspect_imb(data_price_dummy %>% dplyr::filter(price_dummy == "High"),
            data_price_dummy %>% dplyr::filter(price_dummy == "Low")) %>%
  show_plot() + theme(legend.position = "none")
```

```{r}
## similiar to inspect_imb, but for all levels
inspect_cat(datos) %>% show_plot()
```

```{r, eval=FALSE, include=FALSE}
inspect_cor(datos) %>% show_plot()
```

## 3.4 DataExplorer

```{r}
library(DataExplorer)
plot_str(datos)
introduce(datos)
```

```{r}
plot_intro(datos)
```

```{r}
plot_missing(datos)
```

Tal y como se muestra en esta ilustración, se ha asignado un 30% de valores missings en cada variable.

```{r}
plot_bar(datos)
```

Cabe destacar que la variable $key$ posee una gran cantidad de valores faltantes, algo que deberá de tratarse en el preprocesamiento de los datos para evitar el sesgo y que este hecho pueda cambiar significativamente los resultados. Asimismo, $time_signature$ también presenta bastantes valores perdidos.

```{r}
plot_histogram(datos)
```

Muchas de las distribuciones de las variables parecen tener asimetría, bien sea por la izquierda o por la derecha. Esto induce a pensar que variables como $acousticness$, $instrumentalness$, $speechiness$ o $loudness$ tienden a tomar valores más extremos.

```{r}
library(DataExplorer)

# Opción 1: omitir filas con NA
plot_correlation(na.omit(datos), type = "all", maxcat = 5L)
```

## 3.5. SmartEDA
```{r}
library("SmartEDA")
## Overview of the data
ExpData(data = datos,type = 1)

## structure of the data    
ExpData(data = datos,type = 2)
```

```{r}
SmartEDA::ExpCTable(datos,Target=NULL,margin=1,clim=10,nlim=5,round=2,bin=NULL,per=T)
```

# 4. Outliers treatment

```{r}
options(scipen = 999)
diagnose_numeric(datos)
```

Vemos que nos identifica muchisimos outliers (mas de 300 en 5 de las variables numericas) lo que inidca que el criterio de selección de anomalias debe ser mas estricto.

```{r}
diagnose_category(datos)
```

## 4.1. Univariate

### 4.1.1. Max and Min

```{r}
mapply(function(x, name) {
  cat("var. ", name, ": \n\t min: ", min(x, na.rm = TRUE),
      "\n\t max: ", max(x, na.rm = TRUE), "\n")
  invisible(NULL)
}, datos[, varNum], colnames(datos[, varNum]))
```

Vemos que existen outliers en todas la variables en tanto que el maximo o el minimo de estas se encuentra altamente alejado del valor del tercer o el primer quantil respectivamente. És una primer hipòtesis que nos lleva a ejecutar la identificación de estos outliers.

### 4.1.2. IQR

$$[Q1-1.5*IQR,Q3+1.5*IQR]\\IQR=[Q3-Q1]$$

```{r}
IQROutlier <- function(variable, rmnas = TRUE) {
  IQ <- IQR(variable, na.rm = rmnas)
  intInf <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[1]] - 1.5 * IQ
  intSup <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[2]] + 1.5 * IQ
  posicions <- which(variable >= intSup | variable <= intInf)
  if (length(posicions) > 0) {
    cat("Existeixen outliers en les posicions:", paste0(posicions, collapse = ", "), "\n")
  } else {
    cat("No existeixen outliers\n")
  }
  return(posicions)
}
```

Aplicamos para todas las variables numéricas

```{r}
mapply(function(x, name) {
  cat("\n\nVariable:", name, "\n")
  IQROutlier(x)
}, datos[, varNum], varNum)
```

Tenemos muchisimos outliers por variable y esta visualización es poco usable para tratar con ellos. Vemos que el metodo IQR detecta como outliers muchas observaciones en aquellas variables que presentan asimetria en sus distribuciones. Recordemos los histogramas de estas variables y observamos como a mayor es la asimetria del grafico en una variable mas outliers detecta el metodo IQR:

```{r}
plot_histogram(datos)
```

Conluimos para este apartados que podriamos considerar realmente outliers aquellos detectados por IQR en distribuciones gaussianas como en danceability por ejemplo.

### 4.1.3. Boxplot

```{r, warning=FALSE}
library(patchwork)

plots <- lapply(varNum, function(v) {
  ggplot(datos, aes(y = datos[[v]])) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = v, y = v) +
    theme_minimal()
})

wrap_plots(plots)
```

Graficando los boxplots vemos de forma más evidente esta relación entre la asimetria y la detección sobrepasada de valores anomalos. Concretamente, nos fijamos en las variables instrumentalness, speechiness, liveness, song_duration i loudness, que en este orden son las variables que más outliers presentan de más a menos y, por otro lado, estan ordenadas segun la asimetria presentada en sus boxplots y histogramas.

El problema está en que hay una cantidad abudante de valores que se encuentran fuera del intervalo [Q1,Q3], reflejado en los boxplots como puntos, y estos són detectados como outliers incorrectamene. Una posible solución seria escalar estas variables.

### 4.1.4. Z-Score

```{r}
# Función para un histograma z-score con líneas ±3
hist_z3 <- function(vec, var_name, binwidth = NULL) {
  # Filtrar no finitos
  x <- vec[is.finite(vec)]
  # Si no hay datos válidos o sd = 0, devolver un panel informativo
  if (length(x) == 0 || sd(x, na.rm = TRUE) == 0) {
    return(
      ggplot() +
        annotate("text", x = 0, y = 0, label = paste0(var_name, "\nSin variación o sin datos válidos")) +
        theme_void() + labs(title = var_name)
    )
  }
  # z-scores (scale devuelve matriz -> convertir a numérico)
  z <- as.numeric(scale(x))

  # Binwidth automático si no se especifica
  if (is.null(binwidth)) {
    r <- diff(range(z))
    binwidth <- if (is.finite(r) && r > 0) r / 30 else 0.5
  }

  ggplot(data.frame(z = z), aes(x = z)) +
    geom_histogram(binwidth = binwidth, fill = "skyblue", color = "black", boundary = 0) +
    geom_vline(xintercept = c(-3, 3), linetype = "dashed", color = "red", linewidth = 0.8) +
    theme_minimal() +
    labs(title = var_name, x = "z-score", y = "Frecuencia")
}

# Construir todos los plots
plots <- lapply(varNum, function(v) hist_z3(datos[[v]], v))

# Mostrar en cuadrícula (ajusta ncol según prefieras)
wrap_plots(plots, ncol = 4) + plot_annotation(title = "Histogramas (z-score)")

```


```{r}
library(tidyr)

# Función auxiliar: métricas y outliers z>|3| por variable 
calc_z_outliers <- function(x, var_name) {
  # Mantener solo valores finitos
  xf <- x[is.finite(x)]
  n_total   <- length(x)
  n_finite  <- sum(is.finite(x))
  mu <- mean(xf, na.rm = TRUE)
  s  <- sd(xf, na.rm = TRUE)

  if (!is.finite(s) || s == 0 || n_finite == 0) {
    return(list(
      summary = tibble(
        variable = var_name, n = n_total, n_finite = n_finite,
        mean = mu, sd = s,
        thr_low = NA_real_, thr_high = NA_real_,
        n_out_z3 = 0, pct_out_z3 = 0
      ),
      detail = tibble(
        variable = character(0), row_id = integer(0),
        value = numeric(0), z = numeric(0)
      )
    ))
  }

  # Umbrales en escala ORIGINAL equivalentes a |z|>3
  thr_low  <- mu - 3*s
  thr_high <- mu + 3*s

  # Índices (en el data.frame original) de outliers por z>|3|
  idx_finite <- which(is.finite(x))
  z_vals <- (xf - mu) / s
  out_mask <- abs(z_vals) > 3
  out_idx  <- idx_finite[out_mask]

  summary_tbl <- tibble(
    variable = var_name,
    n = n_total,
    n_finite = n_finite,
    mean = mu, sd = s,
    thr_low = thr_low, thr_high = thr_high,   # μ±3σ en escala original
    n_out_z3 = length(out_idx),
    pct_out_z3 = ifelse(n_finite > 0, 100*length(out_idx)/n_finite, 0)
  )

  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value  = x[out_idx],
    z      = (x[out_idx] - mu) / s
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas 
res_list <- map(varNum, ~ calc_z_outliers(datos[[.x]], .x))

tabla_resumen_z3 <- bind_rows(map(res_list, "summary")) %>%
  arrange(desc(n_out_z3))

tabla_detalle_z3 <- bind_rows(map(res_list, "detail")) %>%
  arrange(variable, desc(abs(z)))

#  Resultados:
tabla_resumen_z3
# head(tabla_detalle_z3)  # si solo quieres una vista
```

Vemos que con este escalado parece que hemos solucionado el problema que teniamos con la detección de outliers via IQR. Ahora tenemos outliers en 7 de las 10 variables y en todos suponen menos del 6% de las observaciones y menos de 2% en todos menos en los 2 con mas valores anómalos.

### 4.1.5. Hampel Identifier

Utilizamos la mediana y la desviación absoluta meidana en vez de la media.

```{r}
# Función para detectar outliers con Hampel
hampel_outliers <- function(x, var_name) {
  # Filtrar valores finitos
  x_finite <- x[is.finite(x)]
  if (length(x_finite) == 0) {
    return(list(
      summary = tibble(variable = var_name, median = NA_real_, mad = NA_real_,
                       lower_bound = NA_real_, upper_bound = NA_real_,
                       n_outliers = 0, pct_outliers = 0),
      detail = tibble(variable = character(0), row_id = integer(0), value = numeric(0))
    ))
  }

  med <- median(x_finite, na.rm = TRUE)
  madv <- mad(x_finite, constant = 1, na.rm = TRUE)
  lower <- med - 3 * madv
  upper <- med + 3 * madv

  idx_finite <- which(is.finite(x))
  out_idx <- idx_finite[which(x_finite < lower | x_finite > upper)]

  # Tabla resumen
  summary_tbl <- tibble(
    variable = var_name,
    median = med,
    mad = madv,
    lower_bound = lower,
    upper_bound = upper,
    n_outliers = length(out_idx),
    pct_outliers = 100 * length(out_idx) / length(x_finite)
  )

  # Tabla detallada
  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value = x[out_idx]
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas
res_hampel <- map(varNum, ~ hampel_outliers(datos[[.x]], .x))

# Combinar resultados en tablas 
tabla_resumen_hampel <- bind_rows(map(res_hampel, "summary")) %>%
  arrange(desc(n_outliers))

tabla_detalle_hampel <- bind_rows(map(res_hampel, "detail")) %>%
  arrange(variable, row_id)

#  Mostrar resultados
tabla_resumen_hampel        # resumen por variable
# head(tabla_detalle_hampel)  # detalle de los outliers (índices y valores)

```
Este método queda descartado para nuestra base de datos ya que se detecta como outlier más del 20% de las observaciones en cuatro variables y más del 5 en 8 de ellas. 

Ademmás, antes de su aplicación ya intuiamos que no iba a resultar productivo este metodo ya que operar con la mediana y la desvación absoluta mediana en distribuciones asimétricas no es compatible.

### 4.1.6. Statistics Tests

#### 4.1.6.1. Grubb's Test

Detección de valores extremos en distribuciones Gaussianas.

Por lo tanto, primero estudiamos que variables admiten este test.
```{r}
library(nortest)  

# Anderson–Darling para cada variable
ad_por_variable <- function(x, var_name) {
  x <- x[is.finite(x)]
  if (length(x) < 8) {
    return(tibble(variable = var_name, n = length(x), p_ad = NA_real_, decision = "No test (<8)"))
  }
  p <- tryCatch(nortest::ad.test(x)$p.value, error = function(e) NA_real_)
  tibble(
    variable = var_name,
    n = length(x),
    p_ad = p,
    decision = ifelse(!is.na(p) && p >= 0.05, "≈ Normal (apto Grubbs)", "No normal (No apto Grubbs)")
  )
}

resultado_AD <- map_dfr(varNum, ~ ad_por_variable(datos[[.x]], .x)) %>%
  arrange(decision, desc(p_ad))

resultado_AD

```

No hay ninguna variable que acepte este test, ya que en todas no se puede asumir normalidad, por lo tanto no será usado en nuestro estudio.

#### 4.1.6.2. Dixon's Test

Este test también queda descartado ya que solo es posible aplicarlo en bbdd pequeñas de entre 3 y 30 observaciones. Teniendo 13186 observaciones el uso de esta prueba no es viable.

#### 4.1.6.3. Rosner's Test

Este test parece ser aplicable ya que es aplicable para muestras grandes y además cuenta con la ventaja de la detección múltiple de valores atípicos. No obstante, este asume normalidad en los datos y ya hemos comprobado que ninguna variable la cumple en el primer subapartado de esta sección. 

Por lo tanto, no hemos podido aplicar ninguno de los tests en nuestra bbdd pero hemos concluido de este apartado que contamos con distribuciones no normales.

### 4.1.7. Conclusiones

Después de haber aplicado todos los métodos de tratamiento de valores anomalos en nuestra base de datos hemos visto que con las tres primeras secciones detectabamos muchos outliers incorrectamente debido a problmas de asimetria y de escala.

Ha sido al escalar las variables (Z-Score) cuando hemos podido recoger un conjunto de posibles valores atípicos más coherente en el que poder imputar aquellos que concluyamos como outliers definitivos. Estos estan guardados con su respectivos ID y variable correspondiente en tabla_detalle_z3.

```{r}
head(tabla_detalle_z3,6)
table(tabla_detalle_z3$variable)
```

## 4.2. Multivariate

Scatter plot de los trios de variable más interesantes (correlacionados):
```{r}
library(scatterplot3d)

num <- datos[, varNum, drop = FALSE]

# Matriz de correlaciones (por pares, ignorando NA)
C <- cor(num, use = "pairwise.complete.obs")

# Puntuar todos los tríos por media de |cor| en sus 3 pares
comb3  <- combn(varNum, 3, simplify = FALSE)
scores <- vapply(comb3, function(v) {
  mean(abs(c(C[v[1], v[2]], C[v[1], v[3]], C[v[2], v[3]])), na.rm = TRUE)
}, numeric(1))

# Seleccionar los N tríos “más interesantes”
N   <- 6 # cambia si quieres más/menos
ord <- order(scores, decreasing = TRUE)
top_trios <- comb3[ord][seq_len(min(N, length(comb3)))]

#  Mostrar ranking
ranking <- data.frame(
  rank = seq_along(top_trios),
  trio = sapply(top_trios, paste, collapse = " · "),
  score = round(scores[ord][seq_along(top_trios)], 3),
  row.names = NULL
)
print(ranking)

# Graficar (con escalado para comparabilidad)
scale_axes <- TRUE
old_par <- par(mfrow = c(ceiling(length(top_trios) / 3), min(3, length(top_trios))))
on.exit(par(old_par), add = TRUE)

for (tri in top_trios) {
  x <- num[[tri[1]]]; y <- num[[tri[2]]]; z <- num[[tri[3]]]
  if (scale_axes) { x <- scale(x)[,1]; y <- scale(y)[,1]; z <- scale(z)[,1] }
  scatterplot3d(
    x, y, z,
    main = paste(tri, collapse = " | "),
    xlab = tri[1], ylab = tri[2], zlab = tri[3],
    pch = 16
  )
}

```
Hacemos el gafico interactivo 3D para el unico trio que presenta una correlación conjunta superior a 0.5 (loudness, energy, acousticness)
```{r, warning=FALSE}
library(plotly)

(fig <- plotly::plot_ly(datos, x = ~loudness, y = ~energy, z = ~acousticness, size = 1) %>% 
       add_markers())
```
### 4.2.1. Caso general

Como contamos con una matriz de covarianza singular con colinealidad fuerte entre algunas de nuestras variables, debemos aplicar fallback a PCA en vez de la forma común dada en la teoria:
```{r}
library(mvoutlier)

# Subset y saneo de columnas 
X <- datos[, varNum, drop = FALSE]
# Asegurar numéricas
X <- X[, sapply(X, is.numeric), drop = FALSE]

# Quitar columnas con varianza 0 o todo NA
sd_cols <- sapply(X, function(x) sd(x, na.rm = TRUE))
X <- X[, sd_cols > 0 & !is.na(sd_cols), drop = FALSE]

# Quitar colinealidad casi perfecta (|r| >= 0.999)
if (ncol(X) > 1) {
  repeat {
    C <- suppressWarnings(cor(X, use = "pairwise.complete.obs"))
    up <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
    if (!nrow(up)) break
    # elimina la de menor varianza del par
    to_drop <- unique(colnames(X)[apply(up, 1, function(idx){
      v1 <- var(X[[idx[1]]], na.rm = TRUE)
      v2 <- var(X[[idx[2]]], na.rm = TRUE)
      if (v1 <= v2) idx[1] else idx[2]
    })])
    X <- X[, setdiff(colnames(X), to_drop), drop = FALSE]
  }
}

# Filas completas
Xc <- X[complete.cases(X), , drop = FALSE]

# Garantizar n > p
if (nrow(Xc) <= ncol(Xc)) {
  ord <- order(apply(Xc, 2, var, na.rm = TRUE), decreasing = TRUE)
  p_new <- max(1, nrow(Xc) - 1)
  Xc <- Xc[, ord[seq_len(p_new)], drop = FALSE]
}

##  DD-Plot con fallback robusto
dd_with_fallback <- function(M, quan = 1/2, alpha = 0.025) {
  M <- as.matrix(M)
  tryCatch(
    dd.plot(M, quan = quan, alpha = alpha),
    error = function(e) {
      message("Covarianza singular: hago fallback a PCA (scores). Detalle: ", e$message)
      pcs <- prcomp(M, center = TRUE, scale. = TRUE)
      # nº de componentes: hasta 95% var o n-1, lo que ocurra antes
      var_exp <- cumsum(pcs$sdev^2) / sum(pcs$sdev^2)
      k <- min(which(var_exp >= 0.95))
      k <- min(k, nrow(M) - 1, ncol(M))
      dd.plot(pcs$x[, seq_len(k), drop = FALSE], quan = quan, alpha = alpha)
    }
  )
}

distances <- dd_with_fallback(Xc, quan = 1/2, alpha = 0.025)

```

Las observaciones distribuidas en la rectangulo inferior izquierdo del dd plot són aquellas observaciónes con valores bajos en ambas distancias (no outliers).

Los que se encuentran en la parte superior izquierda, al contar con un valor de distancia robusta alto, són outliers robustos con una fuerte influencia.

Todos aquellos que se encuentran en el lado derecho de la linea vertica central són aquellos llamados "leverage points", observaciones que influyen mucho en la covarianza clásica.

Ahora, obtenemos los índices de aquellos puntos considerados outliers según las anteriores características.
```{r}
outliers<-distances$outliers
indOut<-which(outliers==TRUE)
indOut # 141 outliers
```

Visualizamos todos los outliers detectados como true
```{r}
head(distances$md.cla)
```
```{r}
head(distances$md.rob)
```

```{r}
head(outliers)
```
```{r}
table(outliers)
```
```{r}
distances_df<-as.data.frame(distances)
distances_df<-distances_df[order(-distances_df$md.rob),]
head(distances_df,10)
```
```{r}
p <- 10   
alpha <- 0.025
umbral2 <- qchisq(1 - alpha, df = p) # sobre D^2
umbral  <- sqrt(umbral2)             # sobre D (md.rob)

plot(distances_df$md.rob, 
     type = "h", 
     ylab = "Distancia de Mahalanobis robusta", 
     xlab = "Índice de observación",
     main = sprintf("Outliers multivariantes (p=%d)", p))
abline(h = umbral, col = "red", lwd = 2, lty = 2)
```

PROBLEMA: Con este metodo solo me deja calcular las distancias de observaciones sin NA's (376). El resto las excluye y solo puedo estudiar valores anomalos dentro de este subconjunto muy pequeño excluyendo las otras 12mil y pico obs.

### 4.2.2. PCA

Hay metodos basados en correlaciones que nos permiten detectar outliers. De momento no los ejecutaremos pero que sepamos que exite esa posibilidad en caso de encontrar dificultades si concluimos que nuestras variables regressoras presentan problemas de multicol·linealidad. Hasta ahora no lo parece.

### 4.2.3. Distancia de Mahalanobis 

```{r}
distancia_mahalanobis <- mahalanobis(datos[,varNum], colMeans(datos[,varNum]), cov(datos[,varNum]))
summary(distancia_mahalanobis)
```
Como vemos tenemos problemas de calculo con la distancia de mahalanobis debido a la cantidad de NA's que tenemos en nuestra bbdd. Haremos este metodo más adelante cuando Dante nos guie en si usar algun tipo de metodo de imputacion de NA's previo para calcular estas distancias.

### 4.2.4. Regresió Lineal i residus 

Un punto con un residuo grande puede considerarse un valor anomalo.

### 4.2.5. Distancia de Cook

Podemos calcular las distancias de Cook para discriminar como outliers aquellas observaciones que cuenten con un valor superior a 1.

### 4.2.6. Local Outlier Factor (LOF)
```{r}
#library(Rlof)
#vars_lof<-c("loudness","danceability","energy")
#outliers.scores <- Rlof::lof(datos[,vars_lof], k=5)
```


### 4.2.7. K-Nearest Neighbors (KNN) Outlier Score

```{r outlier-detection, warning=FALSE, message=FALSE}
# --- Paquetes ---
library(adamethods)

# --- 1) Outliers multivariantes KNN ---
datos_knn<-datos
res_knn <- do_knno(datos_knn[, varNum], k = 1, top_n = 3000)

# Asegura vector de índices del dataset original
idx_targets <- sort(unique(as.integer(res_knn)))
idx_targets_orig <- idx_targets  # guardamos copia

# --- 2) NUMÉRICAS: detección y marcaje de outliers ---
X <- datos_knn[idx_targets_orig, varNum, drop = FALSE]
p <- ncol(X)

# Estandarización segura
std_col <- function(v) {
  mu <- mean(v, na.rm = TRUE)
  sdv <- sd(v, na.rm = TRUE)
  if (is.na(sdv) || sdv == 0) return(v - mu)
  (v - mu) / sdv
}

# Función para detectar outliers (por IQR o Z-score)
is_outlier <- function(X, method = c("zscore", "iqr"), threshold = 3) {
  method <- match.arg(method)
  if (all(is.na(X))) return(rep(FALSE, length(X)))
  if (method == "zscore") {
    z <- (X - mean(X, na.rm = TRUE)) / sd(X, na.rm = TRUE)
    res <- abs(z) > threshold
  } else {
    q1 <- quantile(X, 0.25, na.rm = TRUE)
    q3 <- quantile(X, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    res <- (X < lower) | (X > upper)
  }
  res[is.na(res)] <- FALSE
  res
}

# Aplicar sobre el subset
X_no_outliers <- X
for (v in names(X)) {
  idx_rows <- which(is_outlier(X[[v]], method = "iqr"))
  if (length(idx_rows)) X_no_outliers[idx_rows, v] <- NA
}

# Reescalado posterior
Xs <- as.data.frame(lapply(X_no_outliers, std_col))
Xs <- as.matrix(Xs)

# --- 3) CATEGÓRICAS: detectar niveles raros y marcarlos NA ---
min_prop  <- 0.01   # <1% de los casos
min_count <- 20     # o menos de 20 observaciones

# Asegurar tipo factor
for (v in varCat) {
  if (is.character(datos_knn[[v]])) datos_knn[[v]] <- factor(datos_knn[[v]])
}

# Calcular niveles raros
rare_levels_list <- lapply(varCat, function(v) {
  Xv <- datos_knn[[v]]
  if (!is.factor(Xv)) Xv <- factor(Xv)
  if (nlevels(Xv) <= 1) return(character(0))
  tbl <- table(Xv, useNA = "no")
  n   <- sum(tbl)
  prop <- tbl / n
  names(tbl)[(prop < min_prop) | (tbl < min_count)]
})
names(rare_levels_list) <- varCat

# Marcar NA los niveles raros
for (v in varCat) {
  rare_lvls <- rare_levels_list[[v]]
  if (length(rare_lvls) == 0) next
  datos_knn[[v]][datos_knn[[v]] %in% rare_lvls] <- NA
}

# --- 4) Escribir de vuelta las numéricas sin outliers ---
stopifnot(nrow(X_no_outliers) == length(idx_targets_orig))
datos_knn[idx_targets_orig, varNum] <- X_no_outliers

cat("\n✅ Outliers numéricos (IQR) y categóricos (niveles raros) marcados como NA en `datos`.\n")
cat("Total de NA's ahora:", sum(is.na(datos_knn)), "\n")

```
```{r}
plot_missing(datos_knn)
```


# 5.NA's
Imputamos la base de datos original con outliers. Posteriormente se hara el tratamiento de outliers, ya que antes de esta imputación no es posible debido a que todos los metodos requieren de una base de datos completa (sin NA's).
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
datos<-complete(tempData,1)
```

Comprobamos
```{r}
plot_missing(datos)
```

## 5.1. Tratamiento de outliers posterior

### 5.1.1. LOF

Trataremos los outliers en las variables en las que detectamos anteriormente valores atípicos univariantemente mediante z-score, el metodo que mejor resultado dió ya que estamos delante de distribuciones asimetricas que hace falta escalar.

```{r}
tabla_resumen_z3[,c("variable", "n_out_z3")]
```

```{r}
# Construir todos los plots
plots_lof <- lapply(varNum[c(10,1,9,2,4,3,7)], function(v) hist_z3(datos[[v]], v))

# Mostrar en cuadrícula (ajusta ncol según prefieras)
wrap_plots(plots_lof, ncol = 3) + plot_annotation(title = "Histogramas (z-score)")
```
Excluiremos instrumentalness ya que su varianza es pratcicamente nula centrandose su distribución en un intervalo super reducido. 
```{r}
library(Rlof)
datos_scaled<-scale(datos[,varNum])
outliers_scores_lof<-Rlof::lof(datos_scaled[,varNum[c(1,9,2,4,3,7)]], k=5)
plot(density(outliers_scores_lof))
```
Trataremos como outliers segun lof aquellos que esten fuera del precentil 95% segun los scores.
```{r}
umbral_lof<-quantile(outliers_scores_lof,0.95)
lof<-as.integer(outliers_scores_lof>umbral_lof)
table(lof)
```
### 5.1.2. Mahalanobis

Seguimos excluyendo instrumntalness del conjunto de variables ya que su baja varianza, de modo que la matriz se converiria en singular (no invretible).
```{r}
datos_maha<-scale(datos[,varNum[-10]])
distancia_mahalanobis<-mahalanobis(datos_maha,colMeans(datos_maha),cov(datos_maha))
plot(density(distancia_mahalanobis))
```
```{r}
umbral_maha<-qchisq(p=0.99, df=ncol(datos_maha))
mahalanobis<-as.integer(distancia_mahalanobis>umbral_maha)
table(mahalanobis)
528/13186
```
Tenemos que hacer uso del mahalanobis robusto ya que en nuestra bbdd no podemos aproximar la ditribución de las distancias a una chiquadrado debido a que no todas las variables són normales. Para el vector de mahalanobis cogeremos aquellas observaciones fuera del quantil del 99% como outliers.
```{r}
umbral_maha<-quantile(distancia_mahalanobis, 0.99)
mahalanobis<-as.integer(distancia_mahalanobis>umbral_maha)
table(mahalanobis)
```
### 5.1.3. Mahalanobis Robusto
```{r}
library(chemometrics)
dis<-chemometrics::Moutlier(datos_maha, quantile=0.99, plot=TRUE)
```
```{r}
umbral_mcd<-quantile(dis$rd,0.99)
mcd<-as.integer(dis$rd>umbral_mcd)
table(mcd)
```
### 5.1.4. Isolation Forest

```{r}
library(solitude)
library(tidyverse)
#datos escalados con solo numericas y excluyendo instrentalness
datos_iso<-scale(datos[,varNum[-10]])
isoforest<-isolationForest$new(
  sample_size = as.integer(nrow(datos_iso)/2),
  num_trees = 500,
  replace = TRUE,
  seed=123
)
isoforest$fit(dataset=datos_iso)
pred_iso<-isoforest$predict(data=datos_iso)
head(pred_iso)
```
```{r}
umbral_iso <- quantile(pred_iso$anomaly_score, 0.95)
iso<-as.integer(pred_iso$anomaly_score>umbral_iso)
table(iso)
```
### 5.1.5. Ensbamble de metodos

Para evitar la dependencia de un único criterio y aumentar la robustez del análisis, se utilizaron tres métodos complementarios: LOF (densidad local), Mahalanobis clásico (distancia global), Mahalanobis robusto MCD (distancia robusta al núcleo multivariante) y Isolation Forest (aislamiento en particiones aleatorias). La clasificación final de outliers se obtiene mediante una regla de consenso (‘ensembling’), asignando una observación como outlier únicamente cuando al menos dos métodos la detectaron como tal.

```{r}
mat_out <- cbind(lof, mahalanobis, mcd, iso)
outlier<-as.integer(rowSums(mat_out)>=2)
colSums(cbind(mat_out, outlier))
table(outlier)
```
```{r}
library(plotly)

# Seleccionamos las variables
vars_plot <- c("liveness", "speechiness", "loudness")

# Usamos los datos escalados SOLO para estas 3 variables
datos_plot <- as.data.frame(scale(datos[, vars_plot]))

# Añadimos el vector outlier final
datos_plot$outlier <- factor(outlier, labels = c("Normal", "Outlier"))

# Gráfico 3D
plot_ly(
  data = datos_plot,
  x = ~liveness,
  y = ~speechiness,
  z = ~loudness,
  color = ~outlier,
  colors = c("Normal" = "black", "Outlier" = "red"),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 3)
) %>%
  layout(
    title = "Visualización 3D de Outliers (Ensamble)",
    scene = list(
      xaxis = list(title = "liveness (scaled)"),
      yaxis = list(title = "speechiness (scaled)"),
      zaxis = list(title = "loudness (scaled)")
    )
  )
```
Vemos que la detección la tomamos como buena, ya que graficamente los puntos detectados como outliers son aquellos que estan fuera de la nube de puntos con alta densidad. Los que esten en rojo cerca de la nube tal vez no presentan mucha lejania respecto a estas tres variables pero si que la presenten en alguna d elas otras variables con valores atipicos.

Por lo tanto añadimos esta variable artificial "outlier" a la base de datos original.

```{r}
datos$outlier<-outlier
```

```{r}
summary(datos)
```


# 6. Feature engineering

## 6.1 Feature selection

### 6.1.1 Null variance

Se identifican aquellas variables cuya varianza es prácticamente o completamente nula, ya que, al tener valores muy similares entre ellos, se consideran constantes y pueden estar altamente correlacionadas con el término independiente del modelo, lo que solo introduciría ruido.  

```{r, warning=FALSE}
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)

numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
```


Esta salida muestra que no hay ninguna variable con varianza exactamente igual a cero ni cercana a 0, lo cual indica que no hay necesidad de eliminar ninguna variable por falta de información. Sin embargo, las variables referentes a $liveness$, $danceability$, $audio_valence$, $energy$, $speechiness$ y, especialmente, $time_signature$, $key$ y $audio_mode$, tienen un *percentUnique* relativamente bajo, dando a entender que tienen pocos valores distintos. 

### 6.1.2 Feature correlation

Una manera alternativa de mirar si hay variables redundantes es calculando la correlación entre las variables predictoras. Un coeficiente alto de correlación puede generar un problema de multicolinealidad, que puede afectar negativamente a las predicciones del modelo. Anteriormente, mediante un gráfico den correlaciones entre las variables numéricas, se observó una elevada correlación entre **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Se pretende encontrar correlaciones que superen un coeficiente de 0.6:
```{r, warning=FALSE}
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
```

A través de la visualización del gráfico de correlaciones deducimos qué variables presentan una elevada correlación. 

```{r, warning=FALSE}
library("corrplot")

matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
```

La representación gráfica muestra una evidencia clara de correlación entre las variables previamente mencionadas. Este hecho nos impulsa a eliminar alguna de estas variables que ya están explicadas por otras. 

### 6.1.3 Linear combinations

Identificamos si existen combinaciones lineales entre las variables predictoras, otra medida que nos informa acerca de la posible correlación existente entre ellas. 

```{r, warning=FALSE}
#datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num[,-c(1,12)]))
```

Mediante este método, no se detectan combinaciones lineales entre las variables. Esto conlleva a deducir que no hay ninguna variable que contenga esencialmente la misma información que otra desde una perspectiva lineal.

### 6.1.4 Wrapper

Se aplica el método **Wrapper** para seleccionar las variables más relevantes del modelo.

```{r, warning=FALSE}
# install.packages("caret")
library(caret)

# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
#datos_num <- na.omit(datos_num)

# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]

# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")

if(length(variables_redundantes) > 0) {
  input <- input[, -variables_redundantes]
  cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}

# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
                      method = "cv",        # Validación cruzada
                      number = 5)           # 5-fold CV

# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
                      sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
                      rfeControl = control)

# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
```

La técnica de Eliminación Recursiva de Características (*RFE: Recursive Feature Elimination*) es un tipo de método **Wrapper** que selecciona las variables más relevantes para un modelo entrenándolo varias veces mediante un proceso de validación cruzada y eliminando en cada iteración las variables menos importantes. El algoritmo termina hasta encontrar el subconjunto que ofrece el mejor rendimiento. Cabe destacar que dicho método ha tenido en cuenta la correlación entre las variables predictoras. De esta manera, se ha obtenido un conjunto de variables explicativas que no superan un coeficiente de correlación de 0,6. 
Estas variables son: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$.



El análisis exhaustivo de selección de variables reveló un **conjunto óptimo** de **7 predictores** para el modelo de popularidad musical. Tras eliminar variables redundantes por alta correlación (> 0.6), el método **Wrapper RFE** identificó las características más relevantes: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Este conjunto representa una reducción del 46% en dimensionalidad mientras mantiene el poder predictivo, asegurando la estabilidad del modelo mediante la eliminación de multicolinealidad.


## 6.2 Feature transformation

El objetivo de este apartado es mejorar la distribución de las variables, reducir el sesgo y preparar los datos para los algoritmos de modelado.

### 6.2.1 Análisis inicial de distribuciones

```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
  skewness_values <- sapply(datos, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                          ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                            "liveness", "instrumentalness", "acousticness", "loudness")

datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
```

Mientras que las variables correspondientes a $danceability$ y $audio_valence$ presentan un sesgo bajo, el resto de variables obtienen un valor de sesgo potencialmente alto, a excepción de la variable $acousticness$, donde el sesgo es moderado. 

### 6.2.2 Visualización de distribuciones originales

```{r, warning=FALSE}
cat("\n VISUALIZACIÓN DE DISTRIBUCIONES ORIGINALES:\n")

# Función para graficar distribuciones
graficar_distribuciones <- function(datos, titulo) {
  datos_long <- datos %>%
    pivot_longer(everything(), names_to = "Variable", values_to = "Valor")
  
  ggplot(datos_long, aes(x = Valor)) +
    geom_histogram(aes(y = ..density..), fill = "steelblue", alpha = 0.7, bins = 30) +
    geom_density(color = "red", linewidth = 1) +
    facet_wrap(~ Variable, scales = "free") +
    labs(title = titulo,
         x = "Valor", y = "Densidad") +
    theme_minimal()
}

# Graficar distribuciones originales
graficar_distribuciones(datos_transformar, "Distribuciones Originales - Antes de Transformación")
```

### 6.2.3 Aplicación de las transformaciones

```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(datos_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(datos_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
      datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(datos_transformados[[var]])
        datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
```

Las variables con sesgo alto (mayor que 0.5 en valor absoluto), son: $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Se aplican 3 transformaciones a cada una:

  **1.** `log`: Permite reducir el sesgo positivo fuerte y funciona mejor cuando hay valores extremos positivos.
  
  **2.** `sqrt`: Se usa especialmente cuando el sesgo es moderado, y es menos agresiva que log, pero buena para datos con ceros.

  **3.** `box-cox`: Encuentra la transformación óptima automáticamente, con el requisito de que los valores sean estrictamente positivos. 

```{r}
any(datos_transformados$instrumentalness==0)
```

Dado que $instrumentalness$ contiene algún valor igual a 0, se han aplicado solo las dos primeras transformaciones. 

### 6.2.4 Evaluación de Transformaciones

A continuación, se realiza una evaluativa 

```{r evaluacion_transformaciones, warning=FALSE}
# Función para evaluar efectividad de transformaciones
evaluar_transformaciones <- function(var_original, vars_transformadas, nombre_original) {
  resultados <- data.frame()
  
  sesgo_original <- e1071::skewness(var_original, na.rm = TRUE)
  
  for(transform_name in names(vars_transformadas)) {
    if(transform_name != nombre_original) {
      var_transformada <- vars_transformadas[[transform_name]]
      sesgo_transformado <- e1071::skewness(var_transformada, na.rm = TRUE)
      
      reduccion_sesgo <- abs(sesgo_original) - abs(sesgo_transformado)
      
      resultados <- rbind(resultados, data.frame(
        Variable = nombre_original,
        Transformacion = transform_name,
        Sesgo_Original = round(sesgo_original, 3),
        Sesgo_Transformado = round(sesgo_transformado, 3),
        Reduccion_Sesgo = round(reduccion_sesgo, 3),
        Efectiva = ifelse(abs(sesgo_transformado) < abs(sesgo_original) & 
                          abs(sesgo_transformado) < 1, "SÍ", "NO")
      ))
    }
  }
  
  return(resultados)
}

# Evaluar todas las transformaciones
resultados_evaluacion <- data.frame()

for(var in variables_a_transformar) {
  # Obtener todas las versiones de la variable
  vars_relacionadas <- datos_transformados %>% 
    select(starts_with(var))
  
  eval_var <- evaluar_transformaciones(
    datos_transformar[[var]], 
    vars_relacionadas,
    var
  )
  
  resultados_evaluacion <- rbind(resultados_evaluacion, eval_var)
}

cat("EVALUACIÓN DE TRANSFORMACIONES:\n")
print(resultados_evaluacion)

# Seleccionar la mejor transformación para cada variable
mejores_transformaciones <- resultados_evaluacion %>%
  filter(Efectiva == "SÍ") %>%
  group_by(Variable) %>%
  filter(abs(Sesgo_Transformado) == min(abs(Sesgo_Transformado))) %>%
  ungroup()

cat("\n MEJORES TRANSFORMACIONES SELECCIONADAS:\n")
print(mejores_transformaciones)
```


El análisis de transformaciones reveló que **Box-Cox** fue **óptima** para **variables** con **alto sesgo** ($speechiness$, $liveness$), logrando reducciones superiores al 90%, mientras que la **raíz cuadrada** resultó más **efectiva** para **sesgos moderados** ($acousticness$). En cambio, para la variable $instrumentalness$ no se salió efectiva ninguna de las dos modificaciones aplicadas. 
Las transformaciones seleccionadas aseguran que todas las variables cumplan con $|sesgo| < 0.5$, mejorando significativamente los supuestos de normalidad para el modelado posterior.


## 6.3 Feature extraction

### 6.3.1 Preparar datos para PCA

```{r extraccion_variables, warning=FALSE}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)

variables_para_analisis <- c("danceability", "speechiness", "liveness_boxcox", "instrumentalness", "acousticness_boxcox", "loudness", "audio_valence")

# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
```

Se estudian las 7 variables que el método **Wrapper** había proporcionado como conjunto óptimo, pero teniendo en cuenta la transformación de las 3 variables anteriores. 

### 6.3.2 Análisis de Componentes Principales (PCA)

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
```

Dado que se disponen de 7 variables, existen 7 componentes principales. 

### 6.3.3 Determinar Componentes Principales

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
  cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
```

El **análisis** de **componentes principales** sobre las **7 variables óptimas** revela una estructura dimensional bien definida. Los primeros **5 componentes** explican individualmente **más del 10% de la varianza**, acumulando **84.9%** de la **información total**.

PC1 (26.8%) emerge como la dimensión principal, capturando casi un cuarto de la variabilidad del conjunto de datos. La **distribución gradual** de la **varianza** entre los componentes indica que **ninguna** **variable** **domina** excesivamente el **análisis**, sino que **todas** **contribuyen** de manera **balanceada** a las diferentes dimensiones latentes.

Este resultado confirma que las **7 variables seleccionadas** por **RFE** contienen información complementaria y valiosa para el modelo, justificando la retención de múltiples componentes que capturan distintas facetas de las características musicales relevantes para predecir la popularidad.

### 6.3.4 Crear nuevas variables (componentes)

Se crean las nuevas variables, las cuales corresponden a las 5 primeras componentes principales, que no son más que combinaciones lineales de las variables originales. De este modo, se consigue explicar más de un 85% de la variabilidad de $song_popularity$.

```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
  
  # Dar nombres descriptivos
  nombres_descriptivos <- c()
  for(i in 1:length(componentes_importantes)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos <- c(nombres_descriptivos, nombre)
  }
  colnames(nuevos_componentes) <- nombres_descriptivos
  
  # Añadir al dataset original
  datos_final <- cbind(datos_transformados, nuevos_componentes)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
  
} else {
  datos_final <- datos_final_transformado
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```


```{r}
dat<-datos_transformados[,c("speechiness", "liveness_boxcox", "acousticness_boxcox", "danceability", "audio_valence", "loudness", "instrumentalness")]

library("corrplot")

matriz_corr <- cor(dat)
corrplot(matriz_corr, method = "circle")
```

Dado que en el Análisis de Componentes Principales ha dado como resultado que con **5 componentes principales** se explica más de un 83% de la variabilidad de la variable de respuesta, solo es necesario considerar 5 variables como predictoras. En este gráfico de correlaciones se observa que entre $loudness$ y $acousticness_boxcox$ la correlación sigue siendo algo elevada. Esto significa que una variable ya está explicada por otra, así que se decide suprimir **loudness**. 

Por otra parte, como la variable $instrumentalness$ no reduce significativamente el sesgo mediante ninguna transformación, para evitar que este efecto pueda repercutir negativamente en la predicción del modelo, se decide eliminarla. 

Por lo tanto, las 5 variables seleccionadas son: $\text{speechiness, liveness_boxcox, acousticness_boxcox, danceability, audio_valence}$

```{r}
datos_final$outlier<-datos$outlier
```

# 7. KNN

Base de datos con las variables seleccionadas y sus transformaciones:

1) Modelo con las variables seleccionadas
```{r}
datos_prepro<-datos_final[,c("speechiness","liveness_boxcox", "acousticness_boxcox", "danceability", "audio_valence","outlier")]
datos_prepro$song_popularity<-datos_num$song_popularity
# View(datos_prepro)
```

```{r}
matriz_corr <- cor(datos_prepro)
corrplot(matriz_corr, method = "circle")
```


2) Modelo con las 5 componentes
```{r}
datos_prepro_comp<-datos_final[,c("Componente_1", "Componente_2", "Componente_3", "Componente_4", "Componente_5" )]
datos_prepro_comp$song_popularity<-datos_num$song_popularity
```

## 7.1 Elección del valor óptimo de k

Para discutir cuál es el valor óptimo de k, se toman por referencia los cálculos del *RMSE* (**Raíz del Error Cuadrático Medio**) y del *MAPE* (**Error Percentual Absoluto Medio**). Las fórmulas de ambas expresiones se describen a continuación:

\[
\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n}}
\]

\[
\text{MAPE} = 100 \times \frac{1}{n} \sum_{t=1}^{n} \left| \frac{A_t - F_t}{A_t} \right|
\]

El **Error Percentual Absoluto Medio** (**MAPE**: *Mean Absolute Percentual Error*) mide el error porcentual medio entre los valores reales  ($A_t$) y los valores pronosticados ($F_t$). Una ventaja que tiene esta medida frente a la **Raíz del Error Cuadrático Medio**, es que, al ser porcentual, es adimensional y, por tanto, no depende de las unidades. De esta manera facilita su interpretación. 

Se usan distintos valores impares para *k* y se calcula el **RMSE** y el **MAPE** en cada uno de ellos para escoger aquél que proporciona un error mínimo.

1) Modelo con las 5 variables seleccionadas
```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro

train <- datos[default_idx, ]
testK  <- datos[-default_idx, ]

X_train <- train[, -6]
X_test  <- testK[, -6]
y_train <- train[, 6]
y_test  <- testK[, 6]

# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))

# Función para calcular RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Función para calcular MAPE
mape <- function(actual, predicted) {
  non_zero <- actual != 0  # filtrar valores distintos de 0
  100 * mean(abs((actual[non_zero] - predicted[non_zero]) / (actual[non_zero])))
}

# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)

# Calcular RMSE y MAPE para cada k
results <- data.frame(
  k = k_values,
  RMSE = numeric(length(k_values)),
  MAPE = numeric(length(k_values))
)

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
  
  results$RMSE[i] <- rmse(y_test, pred)
  results$MAPE[i] <- mape(y_test, pred)
}

# Mostrar los resultados
print(results)

# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)

# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)

cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
```

2) Modelo con las 5 componentes
```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
datos_comp <- datos_prepro_comp

train_comp <- datos_comp[default_idx, ]
testK_comp  <- datos_comp[-default_idx, ]

X_train_comp <- train_comp[, -6]
X_test_comp  <- testK_comp[, -6]
y_train_comp <- train_comp[, 6]
y_test_comp  <- testK_comp[, 6]

# Convertimos todo a numérico
X_train_comp <- data.frame(lapply(X_train_comp, as.numeric))
X_test_comp  <- data.frame(lapply(X_test_comp, as.numeric))

# Calcular RMSE y MAPE para cada k
results_comp <- data.frame(
  k = k_values,
  RMSE = numeric(length(k_values)),
  MAPE = numeric(length(k_values))
)

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  pred_comp <- knn.reg(train = X_train_comp, test = X_test_comp, y = y_train_comp, k = k)$pred
  
  results_comp$RMSE[i] <- rmse(y_test_comp, pred_comp)
  results_comp$MAPE[i] <- mape(y_test_comp, pred_comp)
}

# Mostrar los resultados
print(results_comp)

# Encontrar el k con menor RMSE
best_rmse_k_comp <- results_comp$k[which.min(results_comp$RMSE)]
best_rmse_comp <- min(results_comp$RMSE)

# Encontrar el k con menor MAPE
best_mape_k_comp <- results_comp$k[which.min(results_comp$MAPE)]
best_mape_comp <- min(results_comp$MAPE)

cat("Mejor k según RMSE:", best_rmse_k_comp, " (RMSE =", best_rmse_comp, ")\n")
cat("Mejor k según MAPE:", best_mape_k_comp, " (MAPE =", best_mape_comp, ")\n")
```

Dado que el valor de k que minimiza el **RMSE** es 19, y el que minimiza el **MAPE** es 17, se procede a calcular un criterio combinado para elegir el que minimice ambos errores balanceados:

1) Modelo con las 5 variables seleccionadas
```{r}
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
```

Por lo tanto, el valor óptimo de k es **k=19**. 

2) Modelo con las 5 componentes
```{r}
results_comp$score <- scale(results_comp$RMSE) + scale(results_comp$MAPE)
best_combined_k_comp <- results_comp$k[which.min(results_comp$score)]
cat("Mejor k combinado:", best_combined_k_comp, "\n")
```
## 7.2 Modelo con k óptimo 

1) Modelo con las 5 variables seleccionadas

Se usa **k=19** como valor óptimo para realizar el *KNN*.

```{r}
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 17)
yp <- modelo_knn$pred
y  <- y_test

# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)

# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
```

2) Modelo con las 5 componentes

Se usa **k=19** como valor óptimo para realizar el *KNN*.

```{r}
modelo_knn_comp <- knn.reg(train = X_train_comp, test = X_test_comp, y = y_train_comp, k = 19)
yp_comp <- modelo_knn_comp$pred
y_comp  <- y_test_comp

# --- Cálculo de errores ---
e1_comp <- (y_comp - yp_comp)^2
e2_comp <- abs(y_comp - yp_comp)

# --- RMSE final ---
rmse_comp <- sqrt(mean(e1_comp))
cat("RMSE del modelo con k =", 19, ":", rmse_comp, "\n")
```

# 8. Association Rules

```{r, include=FALSE, warning=FALSE, message=FALSE}
# INSTALLING AND LOADING THE ARULES PACKAGE

# install.packages("arules")
# install.packages("arulesViz")
# install.packages("tidyverse")
# install.packages("arulesViz")
library(arules)
library(arulesViz)
library(FactoMineR)
library(tidyverse)

# Configurar opciones para mejor visualización
options(digits = 3)
```

```{r}
# Visión general de la base de datos preprocesada
names(datos_prepro) ## 5 predictoras
head(datos_prepro)
summary(datos_prepro)
str(datos_prepro)
```
## 8.1 Preparación y exploración de datos

Transformamos la base de datos en un archivo de transacciones. Para ello, todas las variables deben ser categóricas. 

```{r}
library(arules)
library(arulesViz)
library(dplyr)

datos_disc <- datos_prepro

for (col in names(datos_disc)) {
  
  if (is.numeric(datos_disc[[col]])) {
    
    # Intentar con 3 breaks (percentiles)
    tmp <- try(
      discretize(datos_disc[[col]],
                 method = "frequency",
                 breaks = 3),
      silent = TRUE
    )
    
    # Si falla o produce intervalos degenerados → usar 2 breaks
    if (inherits(tmp, "try-error") || length(levels(tmp)) < 2) {
      tmp <- discretize(datos_disc[[col]],
                        method = "frequency",
                        breaks = 2)
    }
    
    datos_disc[[col]] <- as.factor(tmp)
  }
}

str(datos_disc)
```

## 8.2 Conversión a transacciones

```{r}
datos_transacciones <- as(datos_disc, "transactions")
summary(datos_transacciones)
```

```{r}
# Cantidad de transacciones e ítems
datos_transacciones
summary(datos_transacciones)
class(datos_transacciones)

# FUNCTION inspect
inspect(datos_transacciones[1:6])
transactionInfo(datos_transacciones[1:10])

# Distribución del tamaño de transacciones
SIZE <- size(datos_transacciones)
summary(SIZE)
quantile(SIZE, probs = seq(0,1,0.1))
```

## 8.3 Análisis de itemsets frecuentes

```{r}
cat("=== ITEMSETS FRECUENTES ===\n")

# Probar con diferentes valores de support
supports <- c(0.01, 0.02, 0.03, 0.05)
itemsets_encontrados <- FALSE

for (sup in supports) {
  itemsets_frecuentes <- apriori(datos_transacciones,
                                parameter = list(support = sup,
                                               minlen = 2,
                                               maxlen = 3,  # Reducir longitud máxima
                                               target = "frequent itemsets"))
  
  cat("Support =", sup, "-> Itemsets encontrados:", length(itemsets_frecuentes), "\n")
  
  if (length(itemsets_frecuentes) > 0) {
    itemsets_encontrados <- TRUE
    break
  }
}

if (itemsets_encontrados) {
  cat("\n=== TOP ITEMSETS POR SUPPORT ===\n")
  # Mostrar todos los itemsets encontrados (hasta 10)
  n_mostrar <- min(10, length(itemsets_frecuentes))
  inspect(sort(itemsets_frecuentes, by = "support")[1:n_mostrar])
} else {
  cat("\n*** No se encontraron itemsets frecuentes. Probando con parámetros más flexibles... ***\n")
  
  # Intentar con parámetros más flexibles
  itemsets_frecuentes <- apriori(datos_transacciones,
                                parameter = list(support = 0.005,  # Más bajo
                                               minlen = 1,        # Permitir itemsets de 1
                                               maxlen = 2,
                                               target = "frequent itemsets"))
  
  cat("Con support = 0.005 -> Itemsets encontrados:", length(itemsets_frecuentes), "\n")
  
  if (length(itemsets_frecuentes) > 0) {
    n_mostrar <- min(10, length(itemsets_frecuentes))
    inspect(sort(itemsets_frecuentes, by = "support")[1:n_mostrar])
  } else {
    cat("*** No se pudieron encontrar itemsets frecuentes con los parámetros actuales ***\n")
    cat("*** Revisar la estructura de los datos transaccionales ***\n")
    print(summary(datos_transacciones))
  }
}
```

## 8.4 Generación de Reglas de asociación

```{r, eval=F, include=F}
cat("=== GENERACIÓN DE REGLAS DE ASOCIACIÓN ===\n")

# Probar diferentes combinaciones de parámetros
parametros_list <- list(
  list(support = 0.01, confidence = 0.3),
  list(support = 0.005, confidence = 0.2),
  list(support = 0.02, confidence = 0.4)
)

reglas_encontradas <- FALSE

for (params in parametros_list) {
  reglas <- apriori(datos_transacciones,
                   parameter = list(support = params[[1]],
                                  confidence = params[[2]],
                                  minlen = 1,  # Permitir reglas más simples
                                  maxlen = 3))
  
  cat("Support =", params[[1]], "Confidence =", params[[2]], 
      "-> Reglas encontradas:", length(reglas), "\n")
  
  if (length(reglas) > 0) {
    reglas_encontradas <- TRUE
    break
  }
}

if (reglas_encontradas) {
  cat("Total de reglas generadas:", length(reglas), "\n")
  
  # Filtrar reglas interesantes (parámetros más flexibles)
  if (length(reglas) > 0) {
    calidad_reglas <- quality(reglas)
    reglas_interesantes <- reglas[
      calidad_reglas$lift > 1.0 &  # Más flexible
      calidad_reglas$confidence > 0.3  # Más flexible
    ]
    
    cat("Reglas interesantes (lift > 1.0, confidence > 0.3):", length(reglas_interesantes), "\n")
    
    if (length(reglas_interesantes) > 0) {
      # Ordenar y mostrar
      reglas_ordenadas <- sort(reglas_interesantes, by = "lift", decreasing = TRUE)
      n_mostrar <- min(15, length(reglas_ordenadas))
      cat("\n=== TOP", n_mostrar, "REGLAS POR LIFT ===\n")
      inspect(reglas_ordenadas[1:n_mostrar])
    }
  }
  
} else {
  cat("*** No se pudieron generar reglas con los parámetros estándar ***\n")
  reglas_interesantes <- reglas  # Vacío pero evita error
}
```

**Código modificado**

```{r}
reglas <- apriori(
  trans,
  parameter = list(
    support = 0.03,
    confidence = 0.5,
    minlen = 2,
    maxlen = 4
  )
)

inspect(head(sort(reglas, by = "lift"), 10))
```

## 8.5 Análisis de Popularidad

```{r}
cat("=== 8.5 ANÁLISIS DE POPULARIDAD ===\n")

# Detectar automáticamente el ítem asociado a la popularidad "alta"
labels_trans <- itemLabels(trans)

# Buscamos cualquier etiqueta que incluya el nombre de la variable de popularidad
pop_items <- grep("pop", labels_trans, ignore.case = TRUE, value = TRUE)

cat("Items que se relacionan con la popularidad encontrados:\n")
print(pop_items)

# Elegimos el item que representa popularidad ALTA si existe
patron_popularidad <- grep("alto", pop_items, ignore.case = TRUE, value = TRUE)[1]

# Si no existe "alto", tomamos el primer item de popularidad
if (is.na(patron_popularidad)) {
  patron_popularidad <- pop_items[1]
}

cat("\nItem objetivo para popularidad:", patron_popularidad, "\n\n")

# Generación de reglas que predicen popularidad
reglas_popularidad <- apriori(
  trans,
  parameter = list(
    support = 0.02,      # 2% → suficientemente flexible
    confidence = 0.5,    # confianza media-alta
    minlen = 2,
    maxlen = 4
  ),
  appearance = list(rhs = patron_popularidad, default = "lhs")
)

cat("Total de reglas generadas hacia la popularidad:", length(reglas_popularidad), "\n")

# Filtrado por calidad (lift>1 indica asociación positiva)
reglas_pop_filtradas <- reglas_popularidad[
  quality(reglas_popularidad)$lift > 1
]

cat("Reglas con lift > 1:", length(reglas_pop_filtradas), "\n")

# Seleccionar TOP 10 por lift
TOP_pop <- head(sort(reglas_pop_filtradas, by = "lift", decreasing = TRUE), 10)

cat("\n=== TOP 10 REGLAS QUE PREDICEN POPULARIDAD ===\n")
inspect(TOP_pop)
```
```{r}
appearance = list(rhs = patron_popularidad, default = "lhs")
appearance
```

```{r}
summary(train$song_popularity)
```

## 8.6 Exportación de resultados

```{r}
cat("=== EXPORTACIÓN DE RESULTADOS ===\n")

# Crear dataframe con las reglas
resultados_df <- as(TOP_pop, "data.frame")

# Exportar a CSV
write.csv(resultados_df, "reglas_asociacion_musica.csv", row.names = FALSE)
cat("Resultados exportados a 'reglas_asociacion_musica.csv'\n")

# Mostrar resumen ejecutivo
cat("\n=== RESUMEN EJECUTIVO ===\n")
cat("Total reglas generadas:", length(reglas), "\n")
cat("Reglas filtradas interesantes:", length(reglas_interesantes), "\n")
cat("Lift máximo:", max(quality(TOP_pop)$lift), "\n")
cat("Confidence máximo:", max(quality(TOP_pop)$confidence), "\n")
```

**Resultados**

```{r}
cat("=== INTERPRETACIÓN ===\n")
cat("Las reglas muestran patrones entre características musicales y popularidad.\n")
cat("Lift > 1 indica asociación positiva significativa.\n")
cat("Confidence > 0.6 indica reglas confiables.\n")
cat("Support indica la frecuencia del patrón en las transacciones.\n")
```
# 0'S en Song_Popularity

**Procedimiento**:
Procedimiento mixto clasificación + regresión, muy típico cuando la variable respuesta contiene ceros que distorsionan métricas como el MAPE. Pipeline en R:
  - Separa la base en dos subconjuntos (song_popularity = 0 y song_popularity > 0).

  - Entrena un clasificador para predecir la probabilidad de que una observación tenga popularidad 0.

  - Entrena un modelo de regresión solo con los casos con popularidad > 0.

  - En el test, combina ambos modelos: Si la probabilidad de ser 0 es alta --> predicción = 0
                                       Si no --> predicción = predicción del modelo de regresión



A partir de la base de datos **datos_final**

```{r}
# Cargar datos
datos <- datos_prepro  # sustituye si tu objeto tiene otro nombre

# Crear variable binaria para el modelo de clasificación
datos$is_zero <- ifelse(datos$song_popularity == 0, 1, 0)

# Separar en train y test (mantener proporciones)
set.seed(123)
library(caret)

train_index <- createDataPartition(datos$song_popularity, p = 0.8, list = FALSE)

train <- datos[train_index, ]
test  <- datos[-train_index, ]

# Entrenar un modelo clasificador para predecir ceros
library(randomForest)

modelo_clasif <- randomForest(
  as.factor(is_zero) ~ . -song_popularity,
  data = train,
  ntree = 300
)

# Obtener probabilidades en test
test$prob_zero <- predict(modelo_clasif, newdata = test, type = "prob")[,2]

# Entrenar el modelo de regresión solo con casos > 0
train_reg <- subset(train, song_popularity > 0)

modelo_reg <- randomForest(
  song_popularity ~ . -is_zero,
  data = train_reg,
  ntree = 300
)

# Predicciones en test
test$pred_reg <- predict(modelo_reg, newdata = test)

# Combinar ambos modelos (reglas de imputación)
## umbral, de 0.5
umbral <- 0.5

test$pred_final <- ifelse(test$prob_zero > umbral,
                          0,          # si es muy probable que sea 0
                          test$pred_reg)  # si no, usar el modelo de regresión

# Calcular el MAPE seguro (evita dividir por cero)
# library(Metrics)

# Evitar división entre cero
# mape_seguro <- mean(abs(test$song_popularity - test$pred_final) /
#                    pmax(test$song_popularity, 1))

# mape_seguro

# Guardamos los datasets
datos0<-datos
test0<-test

```


```{r}
# Función para calcular RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Función para calcular MAPE
mape <- function(actual, predicted) {
  non_zero <- actual != 0  # filtrar valores distintos de 0
  100 * mean(abs((actual[non_zero] - predicted[non_zero]) / (actual[non_zero])))
}


rmse(test$song_popularity, test$pred_final)
mape(test$song_popularity,test$pred_final)
```

# 9. CART 
```{r}
library(MASS)
library(dplyr)
library(tidyr)
library(skimr)
library(ggplot2)
library(ggpubr)
library(tree)
```
## 9.1. Arbol inicial
```{r}
set.seed(123)
train       <- sample(1:nrow(datos_prepro), size = nrow(datos_prepro)/2)
datos_train <- datos_prepro[train, ]
datos_test  <- datos_prepro[-train, ]

# Control del árbol: hojas grandes para que no haya mucha profundidad
control <- tree::tree.control(
  nobs    = nrow(datos_train),
  mincut  = 250,   # mínimo obs en cada rama antes de cortar
  minsize = 500,   # mínimo obs en cada hoja
  mindev  = 0      # permitimos todos los splits posibles dado el tamaño
)

arbol_regresion <- tree::tree(
  song_popularity ~ .,
  data    = datos_train,
  split   = "deviance",
  control = control
)

summary(arbol_regresion)

```
## 9.2. Optimización 
```{r}
# tamaños de árbol que quieres probar (≤ número de nodos terminales del árbol grande)
sizes <- c(2, 4, 6, 8, 10, 12, 15, 18)

rmse_por_size <- sapply(sizes, function(s) {
  arbol_podado_1 <- prune.tree(arbol_regresion, best = s)
  pred <- predict(arbol_podado_1, newdata = datos_test)
  sqrt(mean((pred - datos_test$song_popularity)^2))
})

data.frame(size = sizes, RMSE = rmse_por_size)

size_optimo <- sizes[which.min(rmse_por_size)]
size_optimo

```
## 9.3. Árbol óptimo
```{r}
# Árbol podado óptimo
arbol_final_1 <- prune.tree(arbol_regresion, best = size_optimo)

# RMSE del árbol grande
pred_ini <- predict(arbol_regresion, newdata = datos_test)
rmse_ini <- sqrt(mean((pred_ini - datos_test$song_popularity)^2))

# RMSE del árbol podado
pred_fin <- predict(arbol_final_1, newdata = datos_test)
rmse_fin <- sqrt(mean((pred_fin - datos_test$song_popularity)^2))

rmse_ini
rmse_fin

```
### 9.3.1. Evaluación
```{r}
caret::postResample(pred_fin, datos_test$song_popularity)
##Another KPI to evaluate Accuracy in Regression --> MAPE
error<-abs(pred_fin-datos_test$song_popularity)
error<-error/datos_test$song_popularity
average<-mean(error)*100
acc<-100-average
acc
```

```{r}
### Complete Function for Accuracy KPIs
accuracy <- function(pred_fin, obs, na.rm = FALSE, 
                     tol = sqrt(.Machine$double.eps)) {
  err <- obs - pred_fin     # Errors
  if(na.rm) {
    is.a <- !is.na(err)
    err <- err[is.a]
    obs <- obs[is.a]
  }  
  perr <- 100*err/pmax(obs, tol)  # % errors
  return(c(
    me = mean(err),           # Mean error
    rmse = sqrt(mean(err^2)), # sqrt mean squared error
    mae = mean(abs(err)),     # mean absolute error
    mpe = mean(perr),         # mean percentage error
    mape = mean(abs(perr)),   # mean absolute percentage error
    r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2)
  ))
}
accuracy(pred_fin, datos_test$song_popularity)
```
```{r}
# Árbol grande
plot(arbol_regresion)
text(arbol_regresion, pretty = 0)   # añade etiquetas con los splits

```

```{r}
# Árbol podado
plot(arbol_final_1)
text(arbol_final_1, pretty = 0)

```
```{r}
par(mfrow = c(1, 2))   # 1 fila, 2 columnas de gráficos

plot(arbol_regresion)
text(arbol_regresion, pretty = 0)
title("Árbol completo")

plot(arbol_final_1)
text(arbol_final_1, pretty = 0)
title("Árbol podado (size_optimo)")
par(mfrow = c(1, 1))   # volver a configuración normal

```

## 9.4. Método class para regresión
```{r}
### MAIN PACKAGES **** DECISION TREES --> REGRESSION
library(dplyr)
library(rpart)
library(rpart.plot)
library(caret)
library(tree)

### Tus datos
# Suponemos que ya tienes creado 'datos_prepro'
str(datos_prepro)
mydata <- datos_prepro

###### Train & Test Partitions
set.seed(1234)
ind   <- sample(1:nrow(mydata), 0.7*nrow(mydata))
train <- mydata[ind, ]
test  <- mydata[-ind, ]

#############################################
### 1) Árbol de REGRESIÓN con rpart (básico)
#############################################

# method = "anova" --> regresión
tree_rpart <- rpart(song_popularity ~ ., data = train, method = "anova")
summary(tree_rpart)

# Ver el árbol en una ventana nueva
windows()
rpart.plot(tree_rpart, main = "Árbol rpart (inicial)")

#############################################
### 2) Árbol grande + poda con cp (estrategia profesor)
#############################################

# Árbol grande (cp = 0 deja crecer al máximo y luego podamos)
tree_big <- rpart(
  song_popularity ~ .,
  data    = train,
  method  = "anova",
  control = rpart.control(cp = 0)
)

printcp(tree_big)   # tabla de complejidad
plotcp(tree_big, main = "Curva de complejidad (cp vs xerror)")

# Elegimos el cp que minimiza el error de CV (xerror)
xerror <- tree_big$cptable[, "xerror"]
imin   <- which.min(xerror)
cp_opt <- tree_big$cptable[imin, "CP"]
cp_opt

# Podamos el árbol grande con ese cp óptimo
tree_opt <- prune(tree_big, cp = cp_opt)

rpart.plot(tree_opt, main = "Árbol rpart podado (cp óptimo)")

#############################################
### 3) Importancia de variables
#############################################

importance <- tree_opt$variable.importance   # importancia cruda
importance <- round(100 * importance / sum(importance), 1)
importance

#############################################
### 4) Evaluación: RMSE, MAE, R2 (train y test)
#############################################

# Predicciones
pred_train <- predict(tree_opt, newdata = train)
pred_test  <- predict(tree_opt, newdata = test)

# Métricas con caret
cat("Métricas TRAIN:\n")
caret::postResample(pred_train, train$song_popularity)

cat("Métricas TEST:\n")
caret::postResample(pred_test,  test$song_popularity)

#############################################
### 5) MAPE (con cuidado con ceros)
#############################################

y_test <- test$song_popularity

error_rel <- abs(pred_test - y_test) / ifelse(y_test == 0, NA, y_test)
mape <- mean(error_rel, na.rm = TRUE) * 100   # en %
acc  <- 100 - mape

mape
acc

```
```{r}
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)

datos <- datos_prepro %>%
  dplyr::select(
    song_popularity,
    liveness_boxcox,
    acousticness_boxcox,
    danceability,
    audio_valence,
    outlier
  )


set.seed(123)

train_idx <- sample(1:nrow(datos), 0.7 * nrow(datos))
train <- datos[train_idx, ]
test  <- datos[-train_idx, ]

ctrl <- trainControl(
  method = "cv",
  number = 10
)

modelo_rpart <- train(
  song_popularity ~ .,
  data = train,
  method = "rpart",
  trControl = ctrl,
  tuneLength = 20,
  metric = "RMSE"
)

modelo_rpart$bestTune   # mejor cp
modelo_rpart$finalModel

# Predicciones
pred_train <- predict(modelo_rpart, newdata = train)
pred_test  <- predict(modelo_rpart, newdata = test)

# Métricas
caret::postResample(pred_train, train$song_popularity)
caret::postResample(pred_test,  test$song_popularity)

# Importancia de variables
var.imp <- varImp(modelo_rpart)
var.imp
plot(var.imp, main = "Importancia de variables (árbol rpart)")

mape_safe <- function(pred, obs) {
  rel_err <- abs(pred - obs) / ifelse(obs == 0, NA, obs)
  mean(rel_err, na.rm = TRUE) * 100
}

accuracy_reg <- function(pred, obs) {
  err  <- obs - pred
  mse  <- mean(err^2)
  rmse <- sqrt(mse)
  mae  <- mean(abs(err))
  r2   <- 1 - sum(err^2) / sum((obs - mean(obs))^2)
  mape <- mape_safe(pred, obs)
  c(RMSE = rmse, MAE = mae, R2 = r2, MAPE = mape)
}

accuracy_reg(pred_test, test$song_popularity)


```


```{r arbol-podado, fig.width=8, fig.height=6}
# Gráfico árbol final
rpart.plot(modelo_rpart$finalModel, type = 2, extra = 1)
```
```{r}
plot(arbol_final_1)
text(arbol_final_1, pretty = 0)

```
# 10. RF & Boosting

## 10.1. Random Forest

Librerias
```{r}
library(DAAG)
library(mlbench)
library(caret)
library(pROC)
library(printr)
library(randomForest)
library(ranger)
```

```{r}
set.seed(1234)
datos <- datos0
#datos_no0<- datos[datos$song_popularity != 0, ]
#default_idx <- sample(nrow(datos), nrow(datos) * 0.7)

train <- train_reg
test  <- test0
```



### 10.1.1. Optimización de parámetros
Creación de la grid search
```{r}
library(ranger)
require(utils)
param_grid = expand.grid(num_trees = c(50, 100, 500, 1000, 5000),
             mtry= c(2,3,4,5),max_depth = c(1, 3, 10, 20))
head(param_grid)
```

Ajustamos un modelo con cada combinación de la grid
```{r}
oob_error = rep(NA, nrow(param_grid))

for(i in 1:nrow(param_grid)){
  
  modelo <- ranger(
    formula   = song_popularity ~ .,
    data      = train, 
    num.trees = param_grid$num_trees[i],
    mtry      = param_grid$mtry[i],
    max.depth = param_grid$max_depth[i],
    seed      = 123
  )
  
  oob_error[i] <- sqrt(modelo$prediction.error)
}
```

Resultados
```{r}
library(dplyr)
library(tidyr)

resultados <- param_grid
resultados$oob_error <- oob_error
resultados <- resultados %>% arrange(oob_error)
resultados
head(resultados[,4])
p<-which(resultados[,4]==min(resultados[,4]))
resultados[p,]
```
### 10.1.2. Bosque óptimo
```{r}
set.seed(123)
modelo  <- ranger(
  formula   = song_popularity ~ .,
  data      = train,
  num.trees = 5000,
  mtry=3,
  max.depth = 20,
  importance= "impurity",
  seed      = 123
)
print(modelo)
predicciones <- predict(modelo,data = test)
predicciones
importancia_pred <- modelo$variable.importance
sort(importancia_pred,decreasing=TRUE)
```
KPI's
```{r}
test$pred_reg<-predicciones$predictions
test$RF <- ifelse(test$prob_zero > umbral,
                          0,          
                          test$pred_reg) 
caret::postResample(test$RF, test$song_popularity)
```
```{r}
error<-abs(test$RF-test$song_popularity)
error<-error/test$song_popularity
average<-mean(error)*100
acc<-100-average
acc
```
Función para KPI's
```{r}
accuracy <- function(pred, obs, na.rm = FALSE, 
                     tol = sqrt(.Machine$double.eps)) {
  err <- obs - pred     # Errors
  if(na.rm) {
    is.a <- !is.na(err)
    err <- err[is.a]
    obs <- obs[is.a]
  }  
  perr <- 100*err/pmax(obs, tol)  # % errors
  return(c(
    me = mean(err),           # Mean error
    rmse = sqrt(mean(err^2)), # sqrt mean squared error
    mae = mean(abs(err)),     # mean absolute error
    mpe = mean(perr),         # mean percentage error
    mape = mean(abs(perr)),   # mean absolute percentage error
    r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2)
  ))
}
accuracy(test$RF, test$song_popularity)
```

## 10.2. Boosting

### 10.2.1. GBM
```{r}
library(gbm)
set.seed(1234)
datos <- datos_prepro
#datos_no0<- datos[datos$song_popularity != 0, ]
default_idx <- sample(nrow(datos), nrow(datos) * 0.7)

train <- datos[default_idx, ]
test  <- datos[-default_idx, ]
```
Modelo
```{r}
gbm.fit<-gbm(song_popularity~.,data=train)
gbm.fit
summary(gbm.fit)
```
Plots
```{r}
p1<-plot(gbm.fit, i="danceability")
p2<-plot(gbm.fit, i="acousticness_boxcox")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```
Predicciones
```{r}
pred_boost<-predict(gbm.fit, newdata=test)
obs<-test$song_popularity
accuracy(pred_boost,obs)
```

### 10.2.2. Caret
```{r}
library(caret)
set.seed(1234)
trControl<-trainControl(method="cv", number=5)
caret.gbm0 <- train(song_popularity ~ ., method = "gbm", data = train,
                    trControl = trControl, verbose = FALSE)
caret.gbm0
```
```{r}
tuneGrid <- data.frame(n.trees =  100, interaction.depth = 3, 
                       n.minobsinnode = 10, shrinkage = c(0.3, 0.1, 0.05, 0.01, 0.005))
caret.gbm1 <- train(song_popularity ~ ., method = "gbm", data = train,
                    tuneGrid = tuneGrid, trControl = trControl, verbose = FALSE)
caret.gbm1
```
Predicciones
```{r}
pred_boost_caret <- predict(caret.gbm1, newdata = test)
accuracy(pred_boost_caret, obs)
```
### 10.2.3. Xboost
```{r}
library(caret)
caret.xgb <- train(song_popularity ~ ., method = "xgbTree", 
                   data = train, trControl = trControl, verbosity = 0)
caret.xgb
caret.xgb$bestTune
```
Predicciones
```{r}
pred_xgboost<-predict(caret.xgb, newdata=test)
accuracy(pred_xgboost, obs)
```

# 11. SVM


# 12. Submission

## 12.1. Preliminares
```{r}
test<-read_csv("test.csv")
test$key <- factor(test$key)
test$audio_mode <- factor(test$audio_mode) # 0 = menor, 1 = mayor
test$time_signature <- factor(test$time_signature)

clases <- sapply(test, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```

## 12.2. Na's
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(test,m=5,maxit=50,meth='pmm',seed=500)
test<-complete(tempData,1)
```

## 12.3. Outliers

Lof
```{r}
library(Rlof)
test_scaled<-scale(test[,varNum])
test_scores_lof<-Rlof::lof(test_scaled[,varNum[c(1,9,2,4,3,7)]], k=5)
plot(density(test_scores_lof))
```
```{r}
umbral_test_lof<-quantile(test_scores_lof,0.95)
lof_test<-as.integer(test_scores_lof>umbral_test_lof)
table(lof_test)
```

Mahalanobis
```{r}
test_maha<-scale(test[,varNum[-10]])
distancia_test_mahalanobis<-mahalanobis(test_maha,colMeans(test_maha),cov(test_maha))
plot(density(distancia_test_mahalanobis))
```
```{r}
umbral_test_maha<-quantile(distancia_test_mahalanobis, 0.99)
mahalanobis_test<-as.integer(distancia_test_mahalanobis>umbral_test_maha)
table(mahalanobis_test)
```

Mahalanobis Robusto (MCD)
```{r}
library(chemometrics)
dis_test<-chemometrics::Moutlier(test_maha, quantile=0.99, plot=TRUE)
```
```{r}
umbral_test_mcd<-quantile(dis_test$rd,0.99)
mcd_test<-as.integer(dis_test$rd>umbral_test_mcd)
table(mcd_test)
```

Isolation Forest
```{r}
library(solitude)
library(tidyverse)
#datos escalados con solo numericas y excluyendo instrentalness
test_iso<-scale(test[,varNum[-10]])
isoforest_test<-isolationForest$new(
  sample_size = as.integer(nrow(test_iso)/2),
  num_trees = 500,
  replace = TRUE,
  seed=123
)
isoforest_test$fit(dataset=test_iso)
pred_test_iso<-isoforest_test$predict(data=test_iso)
head(pred_test_iso)
```
```{r}
umbral_test_iso <- quantile(pred_test_iso$anomaly_score, 0.95)
iso_test<-as.integer(pred_test_iso$anomaly_score>umbral_test_iso)
table(iso_test)
```

Ensamble de metodos
```{r}
mat_out_test <- cbind(lof_test, mahalanobis_test, mcd_test, iso_test)
outlier_test<-as.integer(rowSums(mat_out_test)>=2)
colSums(cbind(mat_out_test, outlier_test))
table(outlier_test)
```
```{r}
library(plotly)

# Seleccionamos las variables
vars_plot <- c("liveness", "speechiness", "loudness")

# Usamos los datos escalados SOLO para estas 3 variables
test_plot <- as.data.frame(scale(test[, vars_plot]))

# Añadimos el vector outlier final
test_plot$outlier <- factor(outlier_test, labels = c("Normal", "Outlier"))

# Gráfico 3D
plot_ly(
  data = test_plot,
  x = ~liveness,
  y = ~speechiness,
  z = ~loudness,
  color = ~outlier_test,
  colors = c("Normal" = "black", "Outlier" = "red"),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 3)
) %>%
  layout(
    title = "Visualización 3D de Outliers (Ensamble)",
    scene = list(
      xaxis = list(title = "liveness (scaled)"),
      yaxis = list(title = "speechiness (scaled)"),
      zaxis = list(title = "loudness (scaled)")
    )
  )
```
```{r}
test$outlier<-outlier_test
```

## 12.4. Ingenieria de variables

Analisis
```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

numeric_cols <- sapply(test, is.numeric)
test_num <- test[, numeric_cols]
# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(test) {
  skewness_values <- sapply(test, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                           ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                             "liveness", "instrumentalness", "acousticness", "loudness")

test_transformar <- test_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(test_transformar)
print(resultados_sesgo)
```

Aplicacion transformacion
```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
test_transformados <- test_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(test_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(test_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      test_transformados[[paste0(var, "_log")]] <- log1p(test_transformados[[var]])
      test_transformados[[paste0(var, "_sqrt")]] <- sqrt(test_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(test_transformados[[var]])
        test_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, test_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(test_transformar), "\n")
```
PCA
```{r extraccion_variables, warning=FALSE}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)

variables_para_analisis <- c("danceability", "speechiness", "liveness_boxcox", "instrumentalness", "acousticness_boxcox", "loudness", "audio_valence")

# Crear dataset solo con estas variables (excluir objetivo)
test_analisis <- test_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
test_estandarizados <- scale(test_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(test_estandarizados), "\n\n")
```

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado_test <- prcomp(test_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado_test$sdev), "\n\n")
```

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada_test <- pca_resultado_test$sdev^2 / sum(pca_resultado_test$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada_test)) {
  cat("   • PC", i, ": ", round(varianza_explicada_test[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes_test <- which(varianza_explicada_test > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes_test, collapse = ", PC"), "\n", sep = "")
```

Creacion variables finales
```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes_test) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes_test <- as.data.frame(pca_resultado_test$x[, componentes_importantes_test])
  
  # Dar nombres descriptivos
  nombres_descriptivos_test <- c()
  for(i in 1:length(componentes_importantes_test)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos_test <- c(nombres_descriptivos_test, nombre)
  }
  colnames(nuevos_componentes_test) <- nombres_descriptivos_test
  
  # Añadir al dataset original
  test_final <- cbind(test_transformados, nuevos_componentes_test)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos_test, collapse = ", "), "\n")
  
} else {
  test_final <- test_transformados
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```
```{r}
test_final$outlier<-outlier_test
```


## 12.5. Modelos 

### 10.5.1. KNN

1) Modelo con las 4 variables seleccionadas
```{r}
test_prepro<-test_final[,c("speechiness","liveness_boxcox", "acousticness_boxcox", "danceability", "audio_valence","outlier")]
```

2) Modelo con las 5 componentes
```{r}
test_prepro_comp<-test_final[,c("Componente_1", "Componente_2", "Componente_3", "Componente_4", "Componente_5" )]
```
Modelado

1) Modelo con las 5 variables seleccionadas
```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
train <- datos_prepro
test  <- test_prepro

X_train <- train[, -7]
X_test  <- test
y_train <- train[, 7]

# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))
```

```{r}
modelo_knn_test <- knn.reg(train = X_train, test = X_test, y = y_train, k = 19)
yp <- modelo_knn_test$pred
```

2) Modelo con las 5 componentes
```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
train_comp <- datos_prepro_comp
test_comp  <- test_prepro_comp

X_train_comp <- train_comp[, -6]
X_test_comp  <- test_comp
y_train_comp <- train_comp[, 6]

# Convertimos todo a numérico
X_train_comp <- data.frame(lapply(X_train_comp, as.numeric))
X_test_comp  <- data.frame(lapply(X_test_comp, as.numeric))
```

```{r}
modelo_knn_test_comp <- knn.reg(train = X_train_comp, test = X_test_comp, y = y_train_comp, k = 19)
yp_comp <- modelo_knn_test_comp$pred
```

```{r}
# Crear dataframe de submission
submission4 <- data.frame(id = test_num$ID, "song_popularity" = yp)

# Exportar a CSV (sin comillas ni índice)
write.table(submission4, 
            file = "submission4.csv", 
            sep = ",",              # separador por comas
            row.names = FALSE, 
            col.names = TRUE, 
            quote = FALSE, 
            dec = "."               # punto como separador decimal
)
```

## 0's en el output
```{r}
set.seed(123)
library(caret)
library(dplyr)
# Crear variable binaria para el modelo de clasificación
train$is_zero <- ifelse(train$song_popularity == 0, 1, 0)

# Entrenar un modelo clasificador para predecir ceros
library(randomForest)
modelo_clasif <- randomForest(
  as.factor(is_zero) ~ . ,
  data = dplyr::select(train, -song_popularity),
  ntree = 300
)

# Obtener probabilidades en test
test$prob_zero <- predict(modelo_clasif, newdata = test, type = "prob")[,2]

# Entrenar el modelo de regresión solo con casos > 0
train_reg <- subset(train, song_popularity > 0)
train_reg <- dplyr::select(train_reg, -is_zero)
modelo_reg <- randomForest(
  song_popularity ~ .,
  data = train_reg,
  ntree = 300
)

# Predicciones en test
test$pred_reg <- predict(modelo_reg, newdata = test)

# Combinar ambos modelos (reglas de imputación)
## umbral, de 0.5
umbral <- 0.5

test$pred_final <- ifelse(test$prob_zero > umbral,
                          0,          # si es muy probable que sea 0
                          test$pred_reg)  # si no, usar el modelo de regresión
```
### 12.5.2. CART

### 12.5.3. RF

Creación de la grid search
```{r}
library(ranger)
require(utils)
param_grid = expand.grid(num_trees = c(50, 100, 500, 1000, 5000),
             mtry= c(2,3,4,5),max_depth = c(1, 3, 10, 20))
head(param_grid)
```
Ajustamos un modelo con cada combinación de la grid
```{r}
oob_error = rep(NA, nrow(param_grid))

for(i in 1:nrow(param_grid)){
  
  modelo <- ranger(
    formula   = song_popularity ~ .,
    data      = train_reg, 
    num.trees = param_grid$num_trees[i],
    mtry      = param_grid$mtry[i],
    max.depth = param_grid$max_depth[i],
    seed      = 123
  )
  
  oob_error[i] <- sqrt(modelo$prediction.error)
}
```
Resultados
```{r}
library(dplyr)
library(tidyr)

resultados <- param_grid
resultados$oob_error <- oob_error
resultados <- resultados %>% arrange(oob_error)
resultados
head(resultados[,4])
p<-which(resultados[,4]==min(resultados[,4]))
resultados[p,]
```
Bosque óptimo
```{r}
set.seed(123)
modelo  <- ranger(
  formula   = song_popularity ~ .,
  data      = train_reg,
  num.trees = 5000,
  mtry=2,
  max.depth = 20,
  importance= "impurity",
  seed      = 123
)
print(modelo)
predicciones <- predict(modelo,data = test)
predicciones
importancia_pred <- modelo$variable.importance
sort(importancia_pred,decreasing=TRUE)
```

```{r}
test$pred_reg<-predicciones$predictions
test$RF <- ifelse(test$prob_zero > umbral,
                          0,          
                          test$pred_reg) 
```

Submission rf 
```{r}
yp_rf<-test$RF
# Crear dataframe de submission
submission7 <- data.frame(ID = test_num$ID, "song_popularity" = yp_rf)

# Exportar a CSV (sin comillas ni índice)
write.table(submission7, 
            file = "submission7.csv", 
            sep = ",",              # separador por comas
            row.names = FALSE, 
            col.names = TRUE, 
            quote = FALSE, 
            dec = "."               # punto como separador decimal
)
```

### 12.5.4. Boosting
Con caret
```{r}
train <- datos_prepro
test  <- test_prepro
```

```{r}
library(caret)
set.seed(1234)
trControl<-trainControl(method="cv", number=5)
caret.gbm0 <- train(song_popularity ~ ., method = "gbm", data = train,
                    trControl = trControl, verbose = FALSE)
caret.gbm0
```
```{r}
tuneGrid <- data.frame(n.trees =  100, interaction.depth = 3, 
                       n.minobsinnode = 10, shrinkage = c(0.3, 0.1, 0.05, 0.01, 0.005))
caret.gbm1 <- train(song_popularity ~ ., method = "gbm", data = train,
                    tuneGrid = tuneGrid, trControl = trControl, verbose = FALSE)
caret.gbm1
```
Predicciones
```{r}
pred_boost <- predict(caret.gbm1, newdata = test)
```

```{r}
yp_boost<-round(pred_boost,0)
# Crear dataframe de submission
submission6 <- data.frame(ID = test_num$ID, "song_popularity" = yp_boost)

# Exportar a CSV (sin comillas ni índice)
write.table(submission6, 
            file = "submission6.csv", 
            sep = ",",              # separador por comas
            row.names = FALSE, 
            col.names = TRUE, 
            quote = FALSE, 
            dec = "."               # punto como separador decimal
)
```

### 12.5.5. SVM
