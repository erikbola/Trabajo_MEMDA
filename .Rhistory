for (j in 1:n) {
if (j == i) next
xj <- Xs[j, ]
mask <- mask_i & !is.na(xj)
m <- sum(mask)
if (m == 0) {
d[j] <- NA_real_
} else {
diff2 <- xi[mask] - xj[mask]
d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
}
}
d
}
## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
d <- pairwise_dist_row(i, Xs)
# Ordenar ignorando NAs y excluyendo la propia observación
vec <- sort(d[!is.na(d)], partial = k)
if (length(vec) < k) return(NA_real_)
vec[k]
}
## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))
resultado_res_knn <- data.frame(
fila = idx_targets,
score_knn = scores_res
)
## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
d <- pairwise_dist_row(i, Xs)
vec <- sort(d[!is.na(d)], partial = k)
if (length(vec) < k) return(NA_real_)
mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))
## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)
medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv
## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]
## Ver top
head(resultado_res_knn, 10)
plot(resultado_res_knn$score_knn)
abline(h=0.3)
#library(DMwR2)
#ibrary(dplyr)
#outlier.scores <- lofactor(datos[, varNum], k = 5)
#par(mfrow=c(1,1))
#plot(density(outlier.scores))
#outlier.scores
#outliers <- order(outlier.scores, decreasing=T)
#outliers <- order(outlier.scores, decreasing=T)[1:5]
datos$fila_original <- seq_len(nrow(datos))
datos <- datos |>
dplyr::mutate(
key            = factor(key),
audio_mode     = factor(audio_mode),
time_signature = factor(time_signature)
)
na_var_sum  <- naniar::miss_var_summary(datos)
na_case_sum <- naniar::miss_case_summary(datos)
print(head(na_var_sum, 15))
print(head(na_case_sum, 10))
preds <- setdiff(names(datos), c("ID","song_popularity"))
bloque_na_filas <- sum(rowSums(is.na(datos[, preds])) == length(preds))
completas_pred  <- sum(stats::complete.cases(datos[, preds]))
cat("\n[1.2] Filas con TODOS los predictores en NA:", bloque_na_filas, "\n")
cat("[1.2] Filas con predictores completos:", completas_pred, "\n")
datos_filtrado <- datos[rowSums(is.na(datos[, preds])) < length(preds), ]
cat("\n[1.3] Dimensión tras excluir bloque:", nrow(datos_filtrado), "x", ncol(datos_filtrado), "\n")
excluir_base <- c("ID", "song_popularity")
X <- datos[ , setdiff(names(datos), excluir_base), drop = FALSE]
nzv_like <- function(x){
x_noNA <- x[!is.na(x)]
n <- length(x_noNA)
if (n == 0) return(list(nzv = TRUE, reason = "all_NA"))
# % de valores únicos (como caret)
pct_unique <- 100 * (length(unique(x_noNA)) / n)
# ratio entre la categoría/valor más frecuente y el segundo
tb <- sort(table(x_noNA), decreasing = TRUE)
top <- as.numeric(tb[1])
second <- as.numeric(if (length(tb) >= 2) tb[2] else 0)
freq_ratio <- if (second == 0) Inf else top / second
# constante (varianza cero) para numéricos
const_num <- is.numeric(x_noNA) && sd(x_noNA) < .Machine$double.eps
# near-zero variance estilo caret
nzv <- (pct_unique <= 10) && (freq_ratio > 19)
reason <- c()
if (const_num) reason <- c(reason, "constante")
if (nzv)       reason <- c(reason, "near-zero")
if (length(reason) == 0) reason <- "ok"
list(nzv = (const_num || nzv), reason = paste(reason, collapse = "+"),
pct_unique = pct_unique, freq_ratio = freq_ratio)
}
res <- lapply(X, nzv_like)
tab <- data.frame(
variable   = names(res),
nzv        = vapply(res, `[[`, logical(1), "nzv"),
reason     = vapply(res, `[[`, character(1), "reason"),
pct_unique = round(vapply(res, `[[`, numeric(1), "pct_unique"), 3),
freq_ratio = round(vapply(res, `[[`, numeric(1), "freq_ratio"), 3),
row.names = NULL
)
# Resultado
cat("\nVariables que vamos a eliminar):\n")
print(tab[tab$nzv, c("variable","reason","pct_unique","freq_ratio")], row.names = FALSE)
cat("\nVariables que conservamos:\n")
print(tab[!tab$nzv, c("variable","pct_unique","freq_ratio")], row.names = FALSE)
vars_a_quitar <- tab$variable[tab$nzv]
datos_nzv <- datos[, setdiff(names(datos), vars_a_quitar), drop = FALSE]
## 5.6.1) Correlaciones con el target (solo numéricas, excluyendo ID)
is_num  <- sapply(datos_nzv, is.numeric)
nums    <- setdiff(names(datos_nzv)[is_num], c("ID","song_popularity","fila_original"))
cors    <- sapply(nums, function(v) cor(datos_nzv[[v]], datos_nzv[["song_popularity"]], use = "complete.obs"))
cat("\n[3.1] Correlaciones con song_popularity:\n")
print(round(cors, 4))
## 5.6.2) Selección por umbral |r| >= 0.3
umbral <- 0.3
sel_num <- names(which(abs(cors) >= umbral))
cat("\n[3.2] Numéricas seleccionadas con |r| >= ", umbral, ":\n", sep = "")
print(sel_num)
## 5.6.3) Añadir categóricas
cat_vars <- intersect(c("key","audio_mode"), names(datos_nzv))
predictores_finales <- unique(c(sel_num, cat_vars))
cat("\n[3.3] Predictores finales (numéricas por score + categóricas):\n")
print(predictores_finales)
stopifnot(exists("resultado_res_knn"))
stopifnot(all(c("fila") %in% names(resultado_res_knn)))
stopifnot("ID" %in% names(datos))        # para mapear fila -> ID
stopifnot("ID" %in% names(datos_nzv))    # para filtrar en el dataset final
# Elegimos score normalizado en [0,1]
if ("score_minmax_0_1" %in% names(resultado_res_knn)) {
score_norm <- resultado_res_knn$score_minmax_0_1
} else {
s <- resultado_res_knn$score_knn
s_min <- min(s, na.rm = TRUE); s_max <- max(s, na.rm = TRUE)
score_norm <- (s - s_min) / (s_max - s_min)
}
# Filas que superan el umbral 0.3
umbral <- 0.3
filas_out <- resultado_res_knn$fila[!is.na(score_norm) & score_norm >= umbral]
# Convertimos filas -> IDs del dataset original
filas_out <- filas_out[filas_out >= 1 & filas_out <= nrow(datos)]
ids_out   <- unique(as.character(datos$ID[filas_out]))
ids_out   <- ids_out[!is.na(ids_out)]
cat("\n[4] Outliers por score >= ", umbral,
"  | filas=", length(filas_out),
"  | IDs únicos=", length(ids_out), "\n", sep = "")
# Filtramos en el dataset ya depurado (datos_nzv)
n0 <- nrow(datos_nzv)
en_nzv <- sum(datos_nzv$ID %in% ids_out)
if (en_nzv > 0) {
datos_nzv <- datos_nzv[!(datos_nzv$ID %in% ids_out), , drop = FALSE]
cat("Filtrado aplicado en datos_nzv: ", n0, " -> ", nrow(datos_nzv),
" (eliminadas ", en_nzv, " filas)\n", sep = "")
} else {
cat("AVISO: 0 IDs intersectan con datos_nzv (puede que esas filas ya se eliminasen por NA/NZV o por partición).\n")
}
set.seed(123)
idx <- sample.int(nrow(datos_nzv), size = floor(0.7 * nrow(datos_nzv)))
train <- datos_nzv[idx, , drop = FALSE]
test  <- datos_nzv[-idx, , drop = FALSE]
y_train <- train[["song_popularity"]]
y_test  <- test[["song_popularity"]]
cat("\n[5] Split -> train:", nrow(train), " | test:", nrow(test), "\n")
# Identificación numérica/factor en TRAIN
is_num_train  <- sapply(train, is.numeric)
is_fact_train <- sapply(train, is.factor)
# Medianas (num) y modas (factor) en TRAIN
medianas <- vapply(train[, is_num_train, drop = FALSE],
function(v) stats::median(v, na.rm = TRUE), numeric(1))
moda <- function(v){
v2 <- v[!is.na(v)]
if (!length(v2)) return(NA)
names(sort(table(v2), decreasing = TRUE))[1]
}
modas <- vapply(train[, is_fact_train, drop = FALSE], moda, character(1))
# Función de imputación
imputa_df <- function(df){
# Numéricas
for (nm in names(medianas)) if (nm %in% names(df)) {
v <- df[[nm]]; v[is.na(v)] <- medianas[[nm]]; df[[nm]] <- v
}
# Factores
for (nm in names(modas)) if (nm %in% names(df)) {
v <- df[[nm]]; repl <- modas[[nm]]
v[is.na(v)] <- repl
if (is.factor(v) && !is.na(repl) && !(repl %in% levels(v))) {
levels(v) <- c(levels(v), repl)
}
df[[nm]] <- v
}
df
}
train_imp <- imputa_df(train)
test_imp  <- imputa_df(test)
cat("[6] Imputación realizada con mediana (num) y moda (factor) usando TRAIN.\n")
# Conjunto de predictores para modelado
# =====================================================================
preds_final <- setdiff(names(train_imp), c("ID", "song_popularity"))
cat("[7] Predictores finales:", length(preds_final), "\n")
library(mice)
tempData <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
# install.packages("caret")
library(caret)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")
if(length(variables_redundantes) > 0) {
input <- input[, -variables_redundantes]
cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}
# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
method = "cv",        # Validación cruzada
number = 5)           # 5-fold CV
# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
library(caret)
library(e1071)
library(recipes)
library(tidyverse)
# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas
cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")
# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
skewness_values <- sapply(datos, function(x) {
if(is.numeric(x)) {
sesgo <- e1071::skewness(x, na.rm = TRUE)
tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
} else {
return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
}
})
t(skewness_values) %>% as.data.frame() %>%
rownames_to_column("Variable")
}
# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness",
"liveness", "instrumentalness", "acousticness", "loudness")
datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
cat("\n VISUALIZACIÓN DE DISTRIBUCIONES ORIGINALES:\n")
# Función para graficar distribuciones
graficar_distribuciones <- function(datos, titulo) {
datos_long <- datos %>%
pivot_longer(everything(), names_to = "Variable", values_to = "Valor")
ggplot(datos_long, aes(x = Valor)) +
geom_histogram(aes(y = ..density..), fill = "steelblue", alpha = 0.7, bins = 30) +
geom_density(color = "red", linewidth = 1) +
facet_wrap(~ Variable, scales = "free") +
labs(title = titulo,
x = "Valor", y = "Densidad") +
theme_minimal()
}
# Graficar distribuciones originales
graficar_distribuciones(datos_transformar, "Distribuciones Originales - Antes de Transformación")
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
pull(Variable)
cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)
# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar
for(var in variables_a_transformar) {
if(var %in% colnames(datos_transformados)) {
# Obtener valores mínimos para ajustar transformaciones
min_val <- min(datos_transformados[[var]], na.rm = TRUE)
# Aplicar transformaciones según el tipo de variable
if(min_val >= 0) {
# Para variables con valores positivos
datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
# Transformación Box-Cox (requiere valores estrictamente positivos)
if(min_val > 0) {
bc_transform <- BoxCoxTrans(datos_transformados[[var]])
datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
}
}
}
}
cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
any(datos_transformados$instrumentalness==0)
# Función para evaluar efectividad de transformaciones
evaluar_transformaciones <- function(var_original, vars_transformadas, nombre_original) {
resultados <- data.frame()
sesgo_original <- e1071::skewness(var_original, na.rm = TRUE)
for(transform_name in names(vars_transformadas)) {
if(transform_name != nombre_original) {
var_transformada <- vars_transformadas[[transform_name]]
sesgo_transformado <- e1071::skewness(var_transformada, na.rm = TRUE)
reduccion_sesgo <- abs(sesgo_original) - abs(sesgo_transformado)
resultados <- rbind(resultados, data.frame(
Variable = nombre_original,
Transformacion = transform_name,
Sesgo_Original = round(sesgo_original, 3),
Sesgo_Transformado = round(sesgo_transformado, 3),
Reduccion_Sesgo = round(reduccion_sesgo, 3),
Efectiva = ifelse(abs(sesgo_transformado) < abs(sesgo_original) &
abs(sesgo_transformado) < 1, "SÍ", "NO")
))
}
}
return(resultados)
}
# Evaluar todas las transformaciones
resultados_evaluacion <- data.frame()
for(var in variables_a_transformar) {
# Obtener todas las versiones de la variable
vars_relacionadas <- datos_transformados %>%
select(starts_with(var))
eval_var <- evaluar_transformaciones(
datos_transformar[[var]],
vars_relacionadas,
var
)
resultados_evaluacion <- rbind(resultados_evaluacion, eval_var)
}
cat("EVALUACIÓN DE TRANSFORMACIONES:\n")
print(resultados_evaluacion)
# Seleccionar la mejor transformación para cada variable
mejores_transformaciones <- resultados_evaluacion %>%
filter(Efectiva == "SÍ") %>%
group_by(Variable) %>%
filter(abs(Sesgo_Transformado) == min(abs(Sesgo_Transformado))) %>%
ungroup()
cat("\n MEJORES TRANSFORMACIONES SELECCIONADAS:\n")
print(mejores_transformaciones)
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)
cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")
cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")
# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)
variables_para_analisis <- c("danceability", "speechiness_boxcox", "liveness_boxcox", "instrumentalness", "acousticness_sqrt", "loudness", "audio_valence")
# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]
# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)
cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")
pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)
# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")
# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100
# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}
# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC",
paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
cat("\n4. CREANDO NUEVAS VARIABLES...\n")
if(length(componentes_importantes) > 0) {
# Extraer scores de componentes importantes
nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
# Dar nombres descriptivos
nombres_descriptivos <- c()
for(i in 1:length(componentes_importantes)) {
nombre <- paste0("Componente_", i)
nombres_descriptivos <- c(nombres_descriptivos, nombre)
}
colnames(nuevos_componentes) <- nombres_descriptivos
# Añadir al dataset original
datos_final <- cbind(datos_transformados, nuevos_componentes)
cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
} else {
datos_final <- datos_final_transformado
cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
dat<-datos_transformados[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence", "loudness", "instrumentalness")]
library("corrplot")
matriz_corr <- cor(dat)
corrplot(matriz_corr, method = "circle")
datos_prepro<-datos_final[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence")]
datos_prepro$song_popularity<-datos_num$song_popularity
# View(datos_prepro)
library(FNN)
set.seed(1994)
# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro
train <- datos[default_idx, ]
test  <- datos[-default_idx, ]
X_train <- train[, -6]
X_test  <- test[, -6]
y_train <- train[, 6]
y_test  <- test[, 6]
# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))
# Función para calcular RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
# Función para calcular MAPE
mape <- function(actual, predicted) {
non_zero <- actual != 0  # filtrar valores distintos de 0
mean(abs((actual[non_zero] - predicted[non_zero]) / actual[non_zero])) * 100
}
# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)
# Calcular RMSE y MAPE para cada k
results <- data.frame(
k = k_values,
RMSE = numeric(length(k_values)),
MAPE = numeric(length(k_values))
)
for (i in seq_along(k_values)) {
k <- k_values[i]
pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
results$RMSE[i] <- rmse(y_test, pred)
results$MAPE[i] <- mape(y_test, pred)
}
# Mostrar los resultados
print(results)
# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)
# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)
cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test
# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)
# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)
# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
install.packages("arules")
#install.packages("arulesViz"
#install.packages("tidyverse
# install.packages("arules")
# install.packages("arulesViz")
# install.packages("tidyverse")
install.packages("arulesViz")
library(arules)
library(arulesViz)
library(FactoMineR)
library(tidyverse)
