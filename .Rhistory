}
# Función para calcular MAPE
mape <- function(actual, predicted) {
non_zero <- actual != 0  # filtrar valores distintos de 0
mean(abs((actual[non_zero] - predicted[non_zero]) / actual[non_zero])) * 100
}
# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)
# Calcular RMSE y MAPE para cada k
results <- data.frame(
k = k_values,
RMSE = numeric(length(k_values)),
MAPE = numeric(length(k_values))
)
for (i in seq_along(k_values)) {
k <- k_values[i]
pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
results$RMSE[i] <- rmse(y_test, pred)
results$MAPE[i] <- mape(y_test, pred)
}
# Mostrar los resultados
print(results)
# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)
# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)
cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test
# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)
# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 5, ":", rmse, "\n")
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test
# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)
# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
library(FNN)
set.seed(1994)
# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro
train <- datos[default_idx, ]
test  <- datos[-default_idx, ]
X_train <- train[, -6]
X_test  <- test[, -6]
y_train <- train[, 6]
y_test  <- test[, 6]
# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))
# Función para calcular RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
# Función para calcular MAPE
mape <- function(actual, predicted) {
non_zero <- actual != 0  # filtrar valores distintos de 0
mean(abs((actual[non_zero] - predicted[non_zero]) / actual[non_zero])) * 100
}
# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)
# Calcular RMSE y MAPE para cada k
results <- data.frame(
k = k_values,
RMSE = numeric(length(k_values)),
MAPE = numeric(length(k_values))
)
for (i in seq_along(k_values)) {
k <- k_values[i]
pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
results$RMSE[i] <- rmse(y_test, pred)
results$MAPE[i] <- mape(y_test, pred)
}
# Mostrar los resultados
print(results)
# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)
# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)
cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
# install.packages("caret")
library(caret)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")
if(length(variables_redundantes) > 0) {
input <- input[, -variables_redundantes]
cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}
# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
method = "cv",        # Validación cruzada
number = 5)           # 5-fold CV
# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
library(caret)
library(e1071)
library(recipes)
library(tidyverse)
# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas
cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")
# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
skewness_values <- sapply(datos, function(x) {
if(is.numeric(x)) {
sesgo <- e1071::skewness(x, na.rm = TRUE)
tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
} else {
return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
}
})
t(skewness_values) %>% as.data.frame() %>%
rownames_to_column("Variable")
}
# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness",
"liveness", "instrumentalness", "acousticness", "loudness")
datos_transformar <- datos_num[, variables_seleccionadas]
datos_transformar
datos_num
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
# install.packages("caret")
library(caret)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")
if(length(variables_redundantes) > 0) {
input <- input[, -variables_redundantes]
cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}
# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
method = "cv",        # Validación cruzada
number = 5)           # 5-fold CV
# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
library(caret)
library(e1071)
library(recipes)
library(tidyverse)
# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas
cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")
# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
skewness_values <- sapply(datos, function(x) {
if(is.numeric(x)) {
sesgo <- e1071::skewness(x, na.rm = TRUE)
tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
} else {
return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
}
})
t(skewness_values) %>% as.data.frame() %>%
rownames_to_column("Variable")
}
# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness",
"liveness", "instrumentalness", "acousticness", "loudness")
datos_transformar <- datos_num[, variables_seleccionadas]
datos_num
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_num
datos
names(datos)
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)
# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
str(datos)
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)
clases <- sapply(datos, class)
varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
str(datos)
library(psych)
psych::describe(datos[, varNum])
for (var in varNum[-1]) {
# Acceder a la columna por nombre y asegurarse de que es numérica
column_data <- as.numeric(datos[[var]])
# Comprobar si la columna es numérica
if (is.numeric(column_data)) {
# Eliminar NAs
column_data <- na.omit(column_data)
# Comprobar si quedan datos después de eliminar NAs
if (length(column_data) > 0) {
hist(column_data, main = paste0("Histograma variable ", var))
boxplot(column_data, main = paste0("Boxplot variable ", var))
} else {
warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
}
} else {
warning(paste("La variable", var, "no es numérica y será ignorada"))
}
}
par(mfrow = c(1, 1))
for (var in varCat) {
tablaAbs <- data.frame(table(datos[, var]))
tablaFreq <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
m <- match(tablaAbs$Var1, tablaFreq$Var1)
tablaAbs[, "FreqRel"] <- tablaFreq[m, "Freq"]
colnames(tablaAbs) <- c("Categoria", "FreqAbs", "FreqRel")
cat("===============", var, "===================================/n")
print(tablaAbs)
cat("==================================================/n")
}
# Configurar la ventana gráfica para mostrar 2 filas y 3 columnas
par(mfrow = c(2, 3))
# Crear gráficos de barras con títulos personalizados
for (var in varCat) {
if (var == "time_signature") {
barplot(table(datos[, var]),
main = "Distribución de Time Signature",
col = "skyblue")
} else if (var == "key") {
barplot(table(datos[, var]),
main = "Distribución de Key",
col = "lightgreen")
} else if (var == "audio_mode") {
barplot(table(datos[, var]),
main = "Distribución de Audio Mode",
col = "lightcoral")
} else {
barplot(table(datos[, var]),
main = var,  # título genérico si hay más variables
install.packages("inspectdf")
col = "gray80")
install.packages("DataExplorer")
install.packages("skimr")
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)
# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
install.packages("skimr")
install.packages("inspectdf")
install.packages("DataExplorer")
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)
clases <- sapply(datos, class)
varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
str(datos)
library(psych)
psych::describe(datos[, varNum])
for (var in varNum[-1]) {
# Acceder a la columna por nombre y asegurarse de que es numérica
column_data <- as.numeric(datos[[var]])
# Comprobar si la columna es numérica
if (is.numeric(column_data)) {
# Eliminar NAs
column_data <- na.omit(column_data)
# Comprobar si quedan datos después de eliminar NAs
if (length(column_data) > 0) {
hist(column_data, main = paste0("Histograma variable ", var))
boxplot(column_data, main = paste0("Boxplot variable ", var))
} else {
warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
}
} else {
warning(paste("La variable", var, "no es numérica y será ignorada"))
}
}
par(mfrow = c(1, 1))
library(ggplot2)
library(patchwork)
plots <- list()
for (var in varNum) {
histo <- ggplot(datos, aes(x = .data[[var]])) +
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
geom_density(alpha = .2, fill = "#FF6666") +
geom_vline(aes(xintercept = mean(.data[[var]], na.rm = TRUE)),
color = "blue", linetype = "dashed", linewidth = 1) +
ggtitle(paste("Histograma de", var))
boxp <- ggplot(datos, aes(x = .data[[var]])) +
geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) +
ggtitle(paste("Boxplot de", var))
plots <- append(plots, list(histo, boxp))
print(plots)
}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)
# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
str(datos)
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)
clases <- sapply(datos, class)
varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
str(datos)
library(psych)
psych::describe(datos[, varNum])
for (var in varNum[-1]) {
# Acceder a la columna por nombre y asegurarse de que es numérica
column_data <- as.numeric(datos[[var]])
# Comprobar si la columna es numérica
if (is.numeric(column_data)) {
# Eliminar NAs
column_data <- na.omit(column_data)
# Comprobar si quedan datos después de eliminar NAs
if (length(column_data) > 0) {
install.packages("DataExplorer")
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)
# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
names(datos_num)
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
names(datos_num)
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num<-datos_num[, -1]
names(datos_num)
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
datos_num<-datos_num[, -1]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
