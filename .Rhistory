mask_i <- !is.na(xi)
d <- rep(NA_real_, n)
for (j in 1:n) {
if (j == i) next
xj <- Xs[j, ]
mask <- mask_i & !is.na(xj)
m <- sum(mask)
if (m == 0) {
d[j] <- NA_real_
} else {
diff2 <- xi[mask] - xj[mask]
d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
}
}
d
}
## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
d <- pairwise_dist_row(i, Xs)
# Ordenar ignorando NAs y excluyendo la propia observación
vec <- sort(d[!is.na(d)], partial = k)
if (length(vec) < k) return(NA_real_)
vec[k]
}
## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))
resultado_res_knn <- data.frame(
fila = idx_targets,
score_knn = scores_res
)
## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
d <- pairwise_dist_row(i, Xs)
vec <- sort(d[!is.na(d)], partial = k)
if (length(vec) < k) return(NA_real_)
mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))
## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)
medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv
## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]
## Ver top
head(resultado_res_knn, 10)
plot(resultado_res_knn$score_knn)
abline(h=0.3)
#library(DMwR2)
#ibrary(dplyr)
#outlier.scores <- lofactor(datos[, varNum], k = 5)
#par(mfrow=c(1,1))
#plot(density(outlier.scores))
#outlier.scores
#outliers <- order(outlier.scores, decreasing=T)
#outliers <- order(outlier.scores, decreasing=T)[1:5]
datos$fila_original <- seq_len(nrow(datos))
datos <- datos |>
dplyr::mutate(
key            = factor(key),
audio_mode     = factor(audio_mode),
time_signature = factor(time_signature)
)
na_var_sum  <- naniar::miss_var_summary(datos)
na_case_sum <- naniar::miss_case_summary(datos)
print(head(na_var_sum, 15))
print(head(na_case_sum, 10))
preds <- setdiff(names(datos), c("ID","song_popularity"))
bloque_na_filas <- sum(rowSums(is.na(datos[, preds])) == length(preds))
completas_pred  <- sum(stats::complete.cases(datos[, preds]))
cat("\n[1.2] Filas con TODOS los predictores en NA:", bloque_na_filas, "\n")
cat("[1.2] Filas con predictores completos:", completas_pred, "\n")
datos_filtrado <- datos[rowSums(is.na(datos[, preds])) < length(preds), ]
cat("\n[1.3] Dimensión tras excluir bloque:", nrow(datos_filtrado), "x", ncol(datos_filtrado), "\n")
excluir_base <- c("ID", "song_popularity")
X <- datos[ , setdiff(names(datos), excluir_base), drop = FALSE]
nzv_like <- function(x){
x_noNA <- x[!is.na(x)]
n <- length(x_noNA)
if (n == 0) return(list(nzv = TRUE, reason = "all_NA"))
# % de valores únicos (como caret)
pct_unique <- 100 * (length(unique(x_noNA)) / n)
# ratio entre la categoría/valor más frecuente y el segundo
tb <- sort(table(x_noNA), decreasing = TRUE)
top <- as.numeric(tb[1])
second <- as.numeric(if (length(tb) >= 2) tb[2] else 0)
freq_ratio <- if (second == 0) Inf else top / second
# constante (varianza cero) para numéricos
const_num <- is.numeric(x_noNA) && sd(x_noNA) < .Machine$double.eps
# near-zero variance estilo caret
nzv <- (pct_unique <= 10) && (freq_ratio > 19)
reason <- c()
if (const_num) reason <- c(reason, "constante")
if (nzv)       reason <- c(reason, "near-zero")
if (length(reason) == 0) reason <- "ok"
list(nzv = (const_num || nzv), reason = paste(reason, collapse = "+"),
pct_unique = pct_unique, freq_ratio = freq_ratio)
}
res <- lapply(X, nzv_like)
tab <- data.frame(
variable   = names(res),
nzv        = vapply(res, `[[`, logical(1), "nzv"),
reason     = vapply(res, `[[`, character(1), "reason"),
pct_unique = round(vapply(res, `[[`, numeric(1), "pct_unique"), 3),
freq_ratio = round(vapply(res, `[[`, numeric(1), "freq_ratio"), 3),
row.names = NULL
)
# Resultado
cat("\nVariables que vamos a eliminar):\n")
print(tab[tab$nzv, c("variable","reason","pct_unique","freq_ratio")], row.names = FALSE)
cat("\nVariables que conservamos:\n")
print(tab[!tab$nzv, c("variable","pct_unique","freq_ratio")], row.names = FALSE)
vars_a_quitar <- tab$variable[tab$nzv]
datos_nzv <- datos[, setdiff(names(datos), vars_a_quitar), drop = FALSE]
## 5.6.1) Correlaciones con el target (solo numéricas, excluyendo ID)
is_num  <- sapply(datos_nzv, is.numeric)
nums    <- setdiff(names(datos_nzv)[is_num], c("ID","song_popularity","fila_original"))
cors    <- sapply(nums, function(v) cor(datos_nzv[[v]], datos_nzv[["song_popularity"]], use = "complete.obs"))
cat("\n[3.1] Correlaciones con song_popularity:\n")
print(round(cors, 4))
## 5.6.2) Selección por umbral |r| >= 0.3
umbral <- 0.3
sel_num <- names(which(abs(cors) >= umbral))
cat("\n[3.2] Numéricas seleccionadas con |r| >= ", umbral, ":\n", sep = "")
print(sel_num)
## 5.6.3) Añadir categóricas
cat_vars <- intersect(c("key","audio_mode"), names(datos_nzv))
predictores_finales <- unique(c(sel_num, cat_vars))
cat("\n[3.3] Predictores finales (numéricas por score + categóricas):\n")
print(predictores_finales)
stopifnot(exists("resultado_res_knn"))
stopifnot(all(c("fila") %in% names(resultado_res_knn)))
stopifnot("ID" %in% names(datos))        # para mapear fila -> ID
stopifnot("ID" %in% names(datos_nzv))    # para filtrar en el dataset final
# Elegimos score normalizado en [0,1]
if ("score_minmax_0_1" %in% names(resultado_res_knn)) {
score_norm <- resultado_res_knn$score_minmax_0_1
} else {
s <- resultado_res_knn$score_knn
s_min <- min(s, na.rm = TRUE); s_max <- max(s, na.rm = TRUE)
score_norm <- (s - s_min) / (s_max - s_min)
}
# Filas que superan el umbral 0.3
umbral <- 0.3
filas_out <- resultado_res_knn$fila[!is.na(score_norm) & score_norm >= umbral]
# Convertimos filas -> IDs del dataset original
filas_out <- filas_out[filas_out >= 1 & filas_out <= nrow(datos)]
ids_out   <- unique(as.character(datos$ID[filas_out]))
ids_out   <- ids_out[!is.na(ids_out)]
cat("\n[4] Outliers por score >= ", umbral,
"  | filas=", length(filas_out),
"  | IDs únicos=", length(ids_out), "\n", sep = "")
# Filtramos en el dataset ya depurado (datos_nzv)
n0 <- nrow(datos_nzv)
en_nzv <- sum(datos_nzv$ID %in% ids_out)
if (en_nzv > 0) {
datos_nzv <- datos_nzv[!(datos_nzv$ID %in% ids_out), , drop = FALSE]
cat("Filtrado aplicado en datos_nzv: ", n0, " -> ", nrow(datos_nzv),
" (eliminadas ", en_nzv, " filas)\n", sep = "")
} else {
cat("AVISO: 0 IDs intersectan con datos_nzv (puede que esas filas ya se eliminasen por NA/NZV o por partición).\n")
}
set.seed(123)
idx <- sample.int(nrow(datos_nzv), size = floor(0.7 * nrow(datos_nzv)))
train <- datos_nzv[idx, , drop = FALSE]
test  <- datos_nzv[-idx, , drop = FALSE]
y_train <- train[["song_popularity"]]
y_test  <- test[["song_popularity"]]
cat("\n[5] Split -> train:", nrow(train), " | test:", nrow(test), "\n")
# Identificación numérica/factor en TRAIN
is_num_train  <- sapply(train, is.numeric)
is_fact_train <- sapply(train, is.factor)
# Medianas (num) y modas (factor) en TRAIN
medianas <- vapply(train[, is_num_train, drop = FALSE],
function(v) stats::median(v, na.rm = TRUE), numeric(1))
moda <- function(v){
v2 <- v[!is.na(v)]
if (!length(v2)) return(NA)
names(sort(table(v2), decreasing = TRUE))[1]
}
modas <- vapply(train[, is_fact_train, drop = FALSE], moda, character(1))
# Función de imputación
imputa_df <- function(df){
# Numéricas
for (nm in names(medianas)) if (nm %in% names(df)) {
v <- df[[nm]]; v[is.na(v)] <- medianas[[nm]]; df[[nm]] <- v
}
# Factores
for (nm in names(modas)) if (nm %in% names(df)) {
v <- df[[nm]]; repl <- modas[[nm]]
v[is.na(v)] <- repl
if (is.factor(v) && !is.na(repl) && !(repl %in% levels(v))) {
levels(v) <- c(levels(v), repl)
}
df[[nm]] <- v
}
df
}
train_imp <- imputa_df(train)
test_imp  <- imputa_df(test)
cat("[6] Imputación realizada con mediana (num) y moda (factor) usando TRAIN.\n")
# Conjunto de predictores para modelado
# =====================================================================
preds_final <- setdiff(names(train_imp), c("ID", "song_popularity"))
cat("[7] Predictores finales:", length(preds_final), "\n")
library(caret)
library(idealista18)
library(tidyverse)
install.packages("idealista18")
# install.packages("idealista18")
library(idealista18)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
View(bd)
View(datos)
numeric_cols <- sapply(datos, is.numeric)
numeric_cols
datos_num <- Madrid_Sale[, numeric_cols]
datos_num <- datos[, numeric_cols]
datos_num
ncol(datos_num)
ncol(datos_num); ncol(datos)
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
head(varianza, 2)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
head(varianza, 2)
dim(varianza)
View(varianza)
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
varnum<-sapply(datos, var)
varnum<-apply(datos, var)
varianza
datos_num
ncol(datos_num)
(alta_corr <- findCorrelation(datos_num, cutoff = .9))
datos_cor <- cor(datos_num[, 1:13])
(alta_corr <- findCorrelation(datos_num, cutoff = .9))
datos_cor <- cor(datos_num)
(alta_corr <- findCorrelation(datos_num, cutoff = .9))
datos_cor <- cor(datos_num)
datos_cor
datos_num
library(mice)
datos <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
numeric_cols
class(numeric_cols)
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
datos_cor <- cor(datos_num)
(alta_corr <- findCorrelation(datos_num, cutoff = .8))
datos_cor <- cor(datos_num[1:20])
ncol(datos_num)
datos_cor <- cor(datos_num[1:15])
(alta_corr <- findCorrelation(datos_num, cutoff = .8))
is.numeric(datos_num)
str(datos_num)
(alta_corr <- findCorrelation(na.omit(datos_num), cutoff = .8))
is.logical(datos_num)
class(datos_num)
type(datos_num)
is.character(datos_num)
datos_cor <- cor(datos_num[1:15])
(alta_corr <- findCorrelation(datos_num, cutoff = .8))
numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
is.numeric(datos_num)
datos_cor <- cor(as.numeric(datos_num[1:15]))
(alta_corr <- findCorrelation(datos_num, cutoff = .8))
str(datos_num)
datos_num_clean <- na.omit(datos_num)
datos_cor <- cor(datos_num_clean, use = "pairwise.complete.obs")
alta_corr <- findCorrelation(datos_cor, cutoff = 0.8)
colnames(datos_num_clean)[alta_corr]
alta_corr <- findCorrelation(datos_cor, cutoff = 0.7)
colnames(datos_num_clean)[alta_corr]
colnames(datos_num_clean)[alta_corr]
datos_num_clean <- na.omit(datos_num)
datos_cor <- cor(datos_num_clean, use = "pairwise.complete.obs")
alta_corr <- findCorrelation(datos_cor, cutoff = 0.7)
colnames(datos_num_clean)[alta_corr]
datos_num_clean <- na.omit(datos_num)
datos_cor <- cor(datos_num_clean, use = "pairwise.complete.obs")
alta_corr <- findCorrelation(datos_cor, cutoff = 0.7)
colnames(datos_num_clean)[alta_corr]
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
cor(datos_cor)
datos_num_clean <- na.omit(datos_num)
datos_cor <- cor(datos_num_clean, use = "pairwise.complete.obs")
alta_corr <- findCorrelation(datos_cor, cutoff = 0.6)
colnames(datos_num_clean)[alta_corr]
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
colnames(datos_num_clean)[alta_corr]
is.vector(colnames(datos_num_clean)[alta_corr])
length(colnames(datos_num_clean)[alta_corr])
datos_cor <- cor(datos_num)
(alta_corr <- findCorrelation(madrid_cor, cutoff = .9))
(alta_corr <- findCorrelation(datos_cor, cutoff = .9))
(alta_corr <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_cor <- cor(datos_num)
(alta_corr <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_cor <- cor(na.omit(datos_num))
(alta_corr <- findCorrelation(datos_cor, cutoff = 0.6))
colnames(datos_num)[alta_corr]
colnames(alta_corr)
for (var in colnames(datos_num)[alta_corr]) {
cat("\n", var, ":\n")
print(sort(datos_cor[var, abs(datos_cor[var, ]) > 0.6 & names(datos_cor[var, ]) != var]))
}
datos_cor <- cor(na.omit(datos_num))
(alta_corr <- findCorrelation(datos_cor, cutoff = 0.6))
for (var in colnames(datos_num)[alta_corr]) {
cat("\n", var, ":\n")
print(sort(datos_cor[var, abs(datos_cor[var, ]) > 0.6 & names(datos_cor[var, ]) != var]))
}
cor(datos_cor)
(alta_corr <- findCorrelation(datos_cor, cutoff = 0.6))
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
for (var in colnames(datos_num)[alta_corr]) {
cat("\n", var, ":\n")
correladas <- datos_cor[var, ]
correladas <- correladas[abs(correladas) > 0.6 & names(correladas) != var]
print(sort(correladas, decreasing = TRUE))
}
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
which(datos_cor[alta_cor])
names(datos_cor[alta_cor])
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
library("corrplot")
matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na))
library("rsample")
# Se toma una muestra con el paquete rsample
set.seed(7)
# Se toma una muestra con el paquete rsample
set.seed(7)
datos_num_sample <- sample(1:nrow(datos_num), size = 5000, replace = FALSE)
datos_num_sample <- datos_num[datos_num_sample, ]
# Se realiza binning con cuatro bins
datos_num_sample_bin <- datos_num_sample |>
mutate(price_bin = cut(PRICE, breaks = c(0, 250000, 500000, 750000, 10000000), labels = c("primerQ", "segundoQ", "tercerQ", "c"), include.lowest = TRUE)) |>
select(price_bin, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER, HASTERRACE, HASLIFT)
# Se eliminan los registros con valores missing
datos_sample_na <- drop_na(datos_num_sample_bin)
# Se realiza binning con cuatro bins
datos_num_sample_bin <- datos_num_sample |>
mutate(price_bin = cut(PRICE, breaks = c(0, 250000, 500000, 750000, 10000000), labels = c("primerQ", "segundoQ", "tercerQ", "c"), include.lowest = TRUE)) |>
select(price_bin, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER, HASTERRACE, HASLIFT)
# Se fijan los parámetros
evaluador <- wrapperEvaluator("rpart1SE")
install.packages("FSinR")
# install.packages("FSinR")
library(FSinR)
# Se fijan los parámetros
evaluador <- wrapperEvaluator("rpart1SE")
buscador <- searchAlgorithm("sequentialForwardSelection")
# Se evalúan sobre Madrid_Sale_sample_na
results <- featureSelection(datos_num_na, "price_bin", buscador, evaluador)
resultados$bestFeatures
resultados$bestValue
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)  # Eliminar filas con NA
target <- datos$song_popularity[rownames(datos_num)]  # Variable objetivo (ajústala según tu caso)
# Paso 2: Crear función evaluadora con modelo de regresión
evaluator <- wrapperEvaluator('lm', datos_num, target)
# Paso 2: Crear función evaluadora con modelo de regresión
evaluator <- wrapperEvaluator('lm', datos_num, target)
# Paso 2: Crear función evaluadora con modelo de regresión
evaluator <- wrapperEvaluator(
model = 'lm',
input = datos_num[, colnames(datos_num) != 'song_popularity'],
output = target,
fittingParams = list(metric = 'RMSE')
)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Paso 2: Crear evaluador tipo wrapper
evaluator <- wrapperEvaluator('lm')
# Paso 3: Aplicar selección de variables
selected_features <- sequentialForwardSelection(input, target, evaluator)
# Paso 4: Ver resultados
print(selected_features)
# Paso 3: Aplicar selección de variables
selected_features <- sequentialForwardSelection(input, target, evaluator)
# Paso 3: Crear algoritmo de búsqueda
search_method <- sequentialForwardSelection()
# Paso 4: Aplicar selección de variables
resultado <- featureSelection(input, target, evaluator, search_method)
target <- as.numeric(datos_num$song_popularity)
input <- datos_num[, colnames(datos_num) != "song_popularity"]
evaluator <- wrapperEvaluator('lm')
search_method <- sequentialForwardSelection()
resultado <- featureSelection(input, target, evaluator, search_method)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
target <- as.numeric(datos_num$song_popularity)
input <- datos_num[, colnames(datos_num) != "song_popularity"]
input
# Paso 2: Crear evaluador tipo wrapper
evaluator <- wrapperEvaluator('lm')
# Paso 3: Crear algoritmo de búsqueda
search_method <- sequentialForwardSelection()
# Paso 4: Aplicar selección de variables
resultado <- featureSelection(input, target, evaluator, search_method)
names(target) <-NULL
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
target <- as.numeric(target)
names(target) <-NULL
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Paso 2: Crear evaluador tipo wrapper
evaluator <- wrapperEvaluator('lm')
# Paso 3: Crear algoritmo de búsqueda
search_method <- sequentialForwardSelection()
# Paso 4: Aplicar selección de variables
resultado <- featureSelection(input, target, evaluator, search_method)
evaluator <- wrapperEvaluator("rpart")
# Paso 3: Crear algoritmo de búsqueda
search_method <- sequentialForwardSelection()
# Paso 4: Aplicar selección de variables
resultado <- featureSelection(input, target, evaluator, search_method)
# Instalar si no lo tienes
install.packages("caret")
library(caret)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Paso 2: Configurar control de RFE
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
method = "cv",        # Validación cruzada
number = 5)           # 5-fold CV
# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
install.packages("caret")
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)
# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]
# Paso 2: Configurar control de RFE
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
method = "cv",        # Validación cruzada
number = 5)           # 5-fold CV
# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
resultados_rfe <- rfe(input, target,
sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
rfeControl = control)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
La transformación de las variables permite cambiar su dominio para facilitar el entendimiento de los datos. En base a la sección anterior ($\text{feature selection}$) se aplicarán las siguientes transformaciones a las siguientes variables:
