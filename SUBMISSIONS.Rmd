---
title: "Submission"
author: "Erik Bola Sanchez"
date: "2025-10-29"
output: html_document
---
# 0. Paquetes y Datos
```{r setup, include=FALSE}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
library(DataExplorer)
library(inspectdf)
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)

# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
```


```{r}
datos <- read_csv("train.csv") #13186
test <- read_csv("test.csv") #5649
test$song_popularity<-NA
datos <-rbind(datos,test)
```

# 1. Preliminares
```{r}
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)

clases <- sapply(datos, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```


# 2. Outliers
```{r}
library(adamethods)
res_knn<-do_knno(datos[,varNum], k=1, top_n=500)
```
Con este metodo conseguimos el identificador de las 3000 observaciones con un outliers scoore mas alto.

```{r}
## --- Parámetros ---
k <- 1              # usa el mismo k que en do_knno()
idx_targets <- res_knn  # tus 100 índices

## --- 1) Preparar matriz X con variables numéricas ---
X <- datos[, varNum, drop = FALSE]
p <- ncol(X)

## --- 2) Estandarizar por columna ignorando NAs ---
std_col <- function(v) {
  mu <- mean(v, na.rm = TRUE)
  sdv <- sd(v, na.rm = TRUE)
  if (is.na(sdv) || sdv == 0) return((v - mu))   # evita dividir por 0
  (v - mu) / sdv
}
Xs <- as.data.frame(lapply(X, std_col))
Xs <- as.matrix(Xs)  # para operaciones más rápidas

n <- nrow(Xs)

## --- 3) Función de distancia euclídea "pairwise" ajustada por missingness ---
## d_ij = sqrt( sum((xi-xj)^2 over m) * (p / m) ), con m vars observadas en el par
pairwise_dist_row <- function(i, Xs) {
  xi <- Xs[i, ]
  # Máscara NA por columnas respecto xi (TRUE si xi es no-NA)
  mask_i <- !is.na(xi)
  d <- rep(NA_real_, n)
  for (j in 1:n) {
    if (j == i) next
    xj <- Xs[j, ]
    mask <- mask_i & !is.na(xj)
    m <- sum(mask)
    if (m == 0) {
      d[j] <- NA_real_
    } else {
      diff2 <- xi[mask] - xj[mask]
      d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
    }
  }
  d
}

## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
  d <- pairwise_dist_row(i, Xs)
  # Ordenar ignorando NAs y excluyendo la propia observación
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  vec[k]
}

## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))

resultado_res_knn <- data.frame(
  fila = idx_targets,
  score_knn = scores_res
)

## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
  d <- pairwise_dist_row(i, Xs)
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))

## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)

medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv

## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]

## Ver top
head(resultado_res_knn, 10)
```

```{r}
plot(resultado_res_knn$score_robust_z)
abline(h=3)
```

```{r}
outliers <- resultado_res_knn$fila[resultado_res_knn$score_robust_z > 3]
length(outliers)
length(outliers)/13186
```

```{r}
datos<-datos[-outliers,]
```

# 3. Missings / NA's
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
```

```{r}
plot_missing(tempData$data[,-15])
```

# 4. Featuring Engineering

## 4.1. Feature selection
```{r}
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)

numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
```

```{r, warning=FALSE}
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
```

```{r, warning=FALSE}
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
```

```{r, warning=FALSE}
# install.packages("caret")
library(caret)

# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)

# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]

# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")

if(length(variables_redundantes) > 0) {
  input <- input[, -variables_redundantes]
  cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}

# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
                      method = "cv",        # Validación cruzada
                      number = 5)           # 5-fold CV

# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
                      sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
                      rfeControl = control)

# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
```

## 4.2. Feature transformation
```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
  skewness_values <- sapply(datos, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                          ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                            "liveness", "instrumentalness", "acousticness", "loudness")

datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
```

```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(datos_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(datos_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
      datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(datos_transformados[[var]])
        datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
```

```{r evaluacion_transformaciones, warning=FALSE}
# Función para evaluar efectividad de transformaciones
evaluar_transformaciones <- function(var_original, vars_transformadas, nombre_original) {
  resultados <- data.frame()
  
  sesgo_original <- e1071::skewness(var_original, na.rm = TRUE)
  
  for(transform_name in names(vars_transformadas)) {
    if(transform_name != nombre_original) {
      var_transformada <- vars_transformadas[[transform_name]]
      sesgo_transformado <- e1071::skewness(var_transformada, na.rm = TRUE)
      
      reduccion_sesgo <- abs(sesgo_original) - abs(sesgo_transformado)
      
      resultados <- rbind(resultados, data.frame(
        Variable = nombre_original,
        Transformacion = transform_name,
        Sesgo_Original = round(sesgo_original, 3),
        Sesgo_Transformado = round(sesgo_transformado, 3),
        Reduccion_Sesgo = round(reduccion_sesgo, 3),
        Efectiva = ifelse(abs(sesgo_transformado) < abs(sesgo_original) & 
                          abs(sesgo_transformado) < 1, "SÍ", "NO")
      ))
    }
  }
  
  return(resultados)
}

# Evaluar todas las transformaciones
resultados_evaluacion <- data.frame()

for(var in variables_a_transformar) {
  # Obtener todas las versiones de la variable
  vars_relacionadas <- datos_transformados %>% 
    select(starts_with(var))
  
  eval_var <- evaluar_transformaciones(
    datos_transformar[[var]], 
    vars_relacionadas,
    var
  )
  
  resultados_evaluacion <- rbind(resultados_evaluacion, eval_var)
}

cat("EVALUACIÓN DE TRANSFORMACIONES:\n")
print(resultados_evaluacion)

# Seleccionar la mejor transformación para cada variable
mejores_transformaciones <- resultados_evaluacion %>%
  filter(Efectiva == "SÍ") %>%
  group_by(Variable) %>%
  filter(abs(Sesgo_Transformado) == min(abs(Sesgo_Transformado))) %>%
  ungroup()

cat("\n MEJORES TRANSFORMACIONES SELECCIONADAS:\n")
print(mejores_transformaciones)
```

## 4.3. Feature extraction
```{r}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)

variables_para_analisis <- c("danceability", "speechiness_boxcox", "liveness_boxcox", "instrumentalness", "acousticness_sqrt", "loudness", "audio_valence")

# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
```

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
```

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
  cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
```

```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
  
  # Dar nombres descriptivos
  nombres_descriptivos <- c()
  for(i in 1:length(componentes_importantes)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos <- c(nombres_descriptivos, nombre)
  }
  colnames(nuevos_componentes) <- nombres_descriptivos
  
  # Añadir al dataset original
  datos_final <- cbind(datos_transformados, nuevos_componentes)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
  
} else {
  datos_final <- datos_final_transformado
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```

```{r}
dat<-datos_transformados[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence", "loudness", "instrumentalness")]

library("corrplot")

matriz_corr <- cor(dat)
corrplot(matriz_corr, method = "circle")
```

# 5. Modelo 

Base de datos con las variables seleccionadas y sus transformaciones
```{r}
datos_prepro<-datos_final[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence")]
datos_prepro$song_popularity<-datos_num$song_popularity
# View(datos_prepro)
```

## 5.1. KNN

### 5.1.1. Elección valor óptimo de k
```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro

train <- datos[default_idx, ]
test  <- datos[-default_idx, ]

X_train <- train[, -6]
X_test  <- test[, -6]
y_train <- train[, 6]
y_test  <- test[, 6]

# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))

# Función para calcular RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Función para calcular MAPE
mape <- function(actual, predicted) {
  non_zero <- actual != 0  # filtrar valores distintos de 0
  mean(abs((actual[non_zero] - predicted[non_zero]) / actual[non_zero])) * 100
}

# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)

# Calcular RMSE y MAPE para cada k
results <- data.frame(
  k = k_values,
  RMSE = numeric(length(k_values)),
  MAPE = numeric(length(k_values))
)

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
  
  results$RMSE[i] <- rmse(y_test, pred)
  results$MAPE[i] <- mape(y_test, pred)
}

# Mostrar los resultados
print(results)

# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)

# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)

cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
```

```{r}
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
```

### 5.1.2. Modelo con k óptimo
```{r}
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test

# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)

# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
```