---
title: "proyecto regression"
author: "Erik Bola, Vega Carmona, Emma Carretero, Sandra González y Raquel Purroy"
date: 
output:
  html_document: default
  pdf_document: default
---
# 0. Paquetes y Datos
```{r setup, include=FALSE}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
install.packages("DataExplorer")
library(DataExplorer)
install.packages("inspectdf")
library(inspectdf)
install.packages("skimr")
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)

# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
```

```{r}
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# 1. Descripción del problema
```{r, warning=FALSE}
str(datos)
```
Preliminares
```{r}
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)

clases <- sapply(datos, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```

Comprobación
```{r}
str(datos)
```

# 2. Análisis Exploratorio

## 2.1. Univariant Analysis

### 2.1.1. Numerical

Descrpition
```{r}
library(psych)
psych::describe(datos[, varNum])
```

En base a este resumen estadístico se puede afirmar que la mayoría de las canciones no son en vivo, dado que el promedio de la variable $liveness$ es bastante bajo. Respecto a el volumen de las canciones, teniendo en cuenta que $loudness$ es una característica que oscila entre -36.73dB y 1.34dB, su valor medio parece indicar que es relativamente alto. 

La media de $danceability$ apunta que las canciones son bastante bailables, y el valor esperado de $audio_valence$ muestra que las canciones tienden a tener emociones más alegres y positivas, ya que es ligeramente superior a 0,5. 

Asimismo, haciendo referencia a cómo de energéticas o animadas son dichas canciones, $energy$ señala que, en promedio, puede decirse que lo son bastante. 

En cuanto al grado acústico, observando que el valor medio es bajo, se podría deducir que mayoritariamente las canciones son producidas electrónicamente. 

Por último, se podría añadir que gran parte de las canciones no son habladas, pues la variable $speechiness$ tiene un promedio bajo. Destaca la variabilidad del nivel de popularidad ($song_popularity$), con canciones desde poco conocidas hasta muy populares.

Graphics (base)
```{r}
for (var in varNum[-1]) {
  # Acceder a la columna por nombre y asegurarse de que es numérica
  column_data <- as.numeric(datos[[var]])
  
  # Comprobar si la columna es numérica
  if (is.numeric(column_data)) {
    # Eliminar NAs
    column_data <- na.omit(column_data)
    
    # Comprobar si quedan datos después de eliminar NAs
    if (length(column_data) > 0) {
      hist(column_data, main = paste0("Histograma variable ", var))
      boxplot(column_data, main = paste0("Boxplot variable ", var))
    } else {
      warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
    }
  } else {
    warning(paste("La variable", var, "no es numérica y será ignorada"))
  }
}

par(mfrow = c(1, 1))


```

Graphics (ggplot2 + patchwork)
```{r, eval=FALSE}
library(ggplot2)
library(patchwork)

plots <- list()

for (var in varNum) {
  histo <- ggplot(datos, aes(x = .data[[var]])) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
    geom_density(alpha = .2, fill = "#FF6666") +
    geom_vline(aes(xintercept = mean(.data[[var]], na.rm = TRUE)),
               color = "blue", linetype = "dashed", linewidth = 1) +
    ggtitle(paste("Histograma de", var))
  boxp <- ggplot(datos, aes(x = .data[[var]])) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) +
    ggtitle(paste("Boxplot de", var))

  plots <- append(plots, list(histo, boxp))
  print(plots)
}
```

Combinar en un grid automático con 2 columnas
```{r, eval=FALSE, include=FALSE}
final_plot <- Reduce(`+`, plots) + plot_layout(ncol = 2)
final_plot
```

### 2.1.2. Categorical

Description
```{r}
for (var in varCat) {
  tablaAbs <- data.frame(table(datos[, var]))
  tablaFreq <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  m <- match(tablaAbs$Var1, tablaFreq$Var1)
  tablaAbs[, "FreqRel"] <- tablaFreq[m, "Freq"]
  colnames(tablaAbs) <- c("Categoria", "FreqAbs", "FreqRel")
  cat("===============", var, "===================================/n")
  print(tablaAbs)
  cat("==================================================/n")
}
```
Las frecuencias absolutas de las variables categóricas, muestran, en general, una distribución equitativa de las modalidades. Sin embargo, la variable $time_signature$ posee un mayor número de casos en la cuarta categoría.

Graphics (base)
```{r}
# Configurar la ventana gráfica para mostrar 2 filas y 3 columnas
par(mfrow = c(2, 3))

# Crear gráficos de barras con títulos personalizados
for (var in varCat) {
  if (var == "time_signature") {
    barplot(table(datos[, var]),
            main = "Distribución de Time Signature",
            col = "skyblue")
  } else if (var == "key") {
    barplot(table(datos[, var]),
            main = "Distribución de Key",
            col = "lightgreen")
  } else if (var == "audio_mode") {
    barplot(table(datos[, var]),
            main = "Distribución de Audio Mode",
            col = "lightcoral")
  } else {
    barplot(table(datos[, var]),
            main = var,  # título genérico si hay más variables
            col = "gray80")
  }
}

# Restaurar la configuración original
par(mfrow = c(1, 1))
```

-   **time_signature**:

la categoría 4 es la más frecuente, con casi 8000 canciones. Esto corresponde al compás 4/4, el más común en música popular. El resto de niveles, 0, 1, 3 y 5 son mucho menos frecuentes, lo que indica que compases alternativos o inusuales son raros en esta colección de canciones.

-   **key**:

La distribución es bastante uniforme, con cada tonalidad apareciendo entre 1000 y 2000 veces. No hay una tonalidad claramente dominante, lo que sugiere diversidad armónica en el conjunto de datos.

-   **audio_mode**:

Predominan las canciones en modo mayor, lo que puede reflejar una tendencia hacia música más optimista o comercial.

Graphics (ggplot2 + gridExtra)
```{r}
library(gridExtra)

plots <- list()  # lista vacía
i <- 1           # índice

for (var in varCat) {
  tabla <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  p <- ggplot(data = tabla, aes(x = Var1, y = Freq)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(round(Freq * 100, 2), "%")),
              vjust = 1.6, color = "white", size = 3.5) +
    theme_minimal() +
    labs(title = paste("Distribución de", var), x = var, y = "Proporción")

  plots[[i]] <- p
  i <- i + 1
}
# Mostrar todos los gráficos en un grid (ejemplo con 2 columnas)
# grid.arrange(grobs = plots, ncol = 2)
```

## 2.2. Bivariant Analysis

#### 2.2.1 Numerical vs. numerical

Description

```{r}
cor(na.omit(datos[, varNum]))
```

Las correlaciones con valores más extremos se encuentran entre las variables **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Graphics (base / PerformanceAnalytics)

```{r}
library(PerformanceAnalytics)
chart.Correlation(as.matrix(datos[, varNum]), histogram = TRUE, pch = 12)
```

Graphics (ggplot2 / ggcorrplot)

```{r, include=FALSE, eval=FALSE}
#install.packages("ggcorrplot")
library(ggcorrplot)
corr <- round(cor(datos[, varNum]), 1)
ggcorrplot(corr, lab = TRUE)
```

#### 2.2.2 Numerical vs. categorical

Description

```{r}
for (varN in varNum) {
  for (varC in varCat) {
   print(psych::describeBy(datos[, varN], group = datos[, varC]))
  }
}
```

Graphics (ggplot2)

```{r}
library(ggplot2)
library(gridExtra)

plots <- list()
i <- 1

for (varC in varCat) {
  for (varN in varNum) {
    grafico <- ggplot(datos, aes(x = .data[[varN]], fill = .data[[varC]])) +
      geom_histogram(colour = "black",
                     lwd = 0.75,
                     linetype = 1,
                     position = "identity",
                     alpha = 0.5) +
      labs(title = paste("Histograma de", varN, "por", varC),
           x = varN, y = "Frecuencia", fill = varC) +
      theme_minimal()
    plots[[i]] <- grafico
    i <- i + 1
  }
}
# Mostrar todos en un grid (2 columnas)
grid.arrange(grobs = plots, ncol = 2)
```

#### 2.2.3 Categorical vs. categorical

Description

```{r}
for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      prop_table <- prop.table(table(datos[[varc1]], datos[[varc2]]))
      cat("=============", varc1, " vs. ", varc2, "=========================\n")
      print(prop_table)
    }
  }
}

```

Graphics (base)

```{r}
par(mfrow = c(3, 3))
par(mar = c(3, 3, 3, 1))  

for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      tab <- table(datos[[varc1]], datos[[varc2]], useNA = "no")
      prop_tab <- prop.table(tab, margin = 2)

      barplot(prop_tab,
              beside = TRUE,
              ylim = c(0, 1),
              main  = paste(varc1, "vs", varc2),
              xlab  = varc2,
              ylab  = paste("Proporción dentro de", varc2))
      legend("topright", legend = rownames(prop_tab), cex = 0.7, bty = "n")
    }
  }
}

par(mfrow = c(1, 1))

```

# 3. EDA (Automatic Descriptive Analysis)

## 3.1. Skim

```{r}
library(skimr)
library(tidyverse)

# Podem visualitzar un descriptiu de les dades
skim(datos)
```

```{r}
# Visualitzem exclusivament les variables numériques
skim(datos) %>% yank("numeric")
```

```{r}
skim(datos) %>% yank("character")
```

## 3.2 Vis

```{r}
library(visdat)
## Busquem per a variables numériques o categóriques si hi ha NA's
vis_dat(datos)
```

Este gráfico representa que la mayoría de las variables son numéricas, con menos cantidad de variables categóricas. También se osberva que hay un gran número de valores missings en los dos tipos de variables.

```{r}
## Visualitzem percentatges de NA's en les variables
vis_miss(datos)
```

Concretamente, hay un total de 26% de valores perdidos.

```{r}
## Generem la matriu de correlacions
datos %>% dplyr::select(where(is.numeric)) %>% vis_cor()
```

Mediante la visualización de este gráfico de correlaciones, se refuerzan las relaciones lineales entre los diferentes pares de variables.

```{r}
## Podem visualitzar condicionants de les dades. En aquest cas, mirem si tenim mes de
## 2 clases
vis_expect(datos, ~ .x > 2)
```

## 3.3. Inspect df

```{r}
library(inspectdf)

## Tipus de dades
inspect_types(datos) %>% show_plot()
```

La base de datos contiene 3 variables factor, y 11 cuantitativas.

```{r}
## Utilització de la memoria
inspect_mem(datos) %>% show_plot()
```

```{r}
# Paquetes
library(dplyr)
library(inspectdf)
library(rlang)

# --- 1) Crear una categórica desde una numérica y comparar (High/Low) ---
# Usamos 'song_popularity' como ejemplo con umbral 50.
umbral <- 50
num_target <- "song_popularity"

stopifnot(num_target %in% c(
  "liveness","loudness","danceability","song_duration_ms","audio_valence",
  "energy","tempo","acousticness","speechiness","instrumentalness","song_popularity"
))

# Convertir a numérico por si viniera como texto
datos[[num_target]] <- suppressWarnings(as.numeric(datos[[num_target]]))

data_price_dummy <- datos %>%
  mutate(price_dummy = if_else(.data[[num_target]] > umbral, "High", "Low") %>% factor())

# Comparativa de NA entre High y Low
inspect_na(
  data_price_dummy %>% filter(price_dummy == "High"),
  data_price_dummy %>% filter(price_dummy == "Low")
) %>% show_plot()

```

```{r}
## Comprovem la distribució de les variables
inspect_num(datos) %>% show_plot()
```

```{r}
## check categorical variable distribution
inspect_imb(datos) %>% show_plot()
```

```{r}
## check two categorical
inspect_imb(data_price_dummy %>% dplyr::filter(price_dummy == "High"),
            data_price_dummy %>% dplyr::filter(price_dummy == "Low")) %>%
  show_plot() + theme(legend.position = "none")
```

```{r}
## similiar to inspect_imb, but for all levels
inspect_cat(datos) %>% show_plot()
```

```{r, eval=FALSE, include=FALSE}
inspect_cor(datos) %>% show_plot()
```

## 3.4 DataExplorer

```{r}
library(DataExplorer)
plot_str(datos)
introduce(datos)
```

```{r}
plot_intro(datos)
```

```{r}
plot_missing(datos)
```

Tal y como se muestra en esta ilustración, se ha asignado un 30% de valores missings en cada variable.

```{r}
plot_bar(datos)
```

Cabe destacar que la variable $key$ posee una gran cantidad de valores faltantes, algo que deberá de tratarse en el preprocesamiento de los datos para evitar el sesgo y que este hecho pueda cambiar significativamente los resultados. Asimismo, $time_signature$ también presenta bastantes valores perdidos.

```{r}
plot_histogram(datos)
```

Muchas de las distribuciones de las variables parecen tener asimetría, bien sea por la izquierda o por la derecha. Esto induce a pensar que variables como $acousticness$, $instrumentalness$, $speechiness$ o $loudness$ tienden a tomar valores más extremos.

```{r}
library(DataExplorer)

# Opción 1: omitir filas con NA
plot_correlation(na.omit(datos), type = "all", maxcat = 5L)
```

## 3.5. SmartEDA
```{r}
library("SmartEDA")
## Overview of the data
ExpData(data = datos,type = 1)

## structure of the data    
ExpData(data = datos,type = 2)
```

```{r}
SmartEDA::ExpCTable(datos,Target=NULL,margin=1,clim=10,nlim=5,round=2,bin=NULL,per=T)
```

# 4. Outliers treatment

```{r}
options(scipen = 999)
diagnose_numeric(datos)
```

Vemos que nos identifica muchisimos outliers (mas de 300 en 5 de las variables numericas) lo que inidca que el criterio de selección de anomalias debe ser mas estricto.

```{r}
diagnose_category(datos)
```

## 4.1. Univariate

### 4.1.1. Max and Min

```{r}
mapply(function(x, name) {
  cat("var. ", name, ": \n\t min: ", min(x, na.rm = TRUE),
      "\n\t max: ", max(x, na.rm = TRUE), "\n")
  invisible(NULL)
}, datos[, varNum], colnames(datos[, varNum]))
```

Vemos que existen outliers en todas la variables en tanto que el maximo o el minimo de estas se encuentra altamente alejado del valor del tercer o el primer quantil respectivamente. És una primer hipòtesis que nos lleva a ejecutar la identificación de estos outliers.

### 4.1.2. IQR

$$[Q1-1.5*IQR,Q3+1.5*IQR]\\IQR=[Q3-Q1]$$

```{r}
IQROutlier <- function(variable, rmnas = TRUE) {
  IQ <- IQR(variable, na.rm = rmnas)
  intInf <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[1]] - 1.5 * IQ
  intSup <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[2]] + 1.5 * IQ
  posicions <- which(variable >= intSup | variable <= intInf)
  if (length(posicions) > 0) {
    cat("Existeixen outliers en les posicions:", paste0(posicions, collapse = ", "), "\n")
  } else {
    cat("No existeixen outliers\n")
  }
  return(posicions)
}
```

Aplicamos para todas las variables numéricas

```{r}
mapply(function(x, name) {
  cat("\n\nVariable:", name, "\n")
  IQROutlier(x)
}, datos[, varNum], varNum)
```

Tenemos muchisimos outliers por variable y esta visualización es poco usable para tratar con ellos. Vemos que el metodo IQR detecta como outliers muchas observaciones en aquellas variables que presentan asimetria en sus distribuciones. Recordemos los histogramas de estas variables y observamos como a mayor es la asimetria del grafico en una variable mas outliers detecta el metodo IQR:

```{r}
plot_histogram(datos)
```

Conluimos para este apartados que podriamos considerar realmente outliers aquellos detectados por IQR en distribuciones gaussianas como en danceability por ejemplo.

### 4.1.3. Boxplot

```{r, warning=FALSE}
library(patchwork)

plots <- lapply(varNum, function(v) {
  ggplot(datos, aes(y = datos[[v]])) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = v, y = v) +
    theme_minimal()
})

wrap_plots(plots)
```

Graficando los boxplots vemos de forma más evidente esta relación entre la asimetria y la detección sobrepasada de valores anomalos. Concretamente, nos fijamos en las variables instrumentalness, speechiness, liveness, song_duration i loudness, que en este orden son las variables que más outliers presentan de más a menos y, por otro lado, estan ordenadas segun la asimetria presentada en sus boxplots y histogramas.

El problema está en que hay una cantidad abudante de valores que se encuentran fuera del intervalo [Q1,Q3], reflejado en los boxplots como puntos, y estos són detectados como outliers incorrectamene. Una posible solución seria escalar estas variables.

### 4.1.4. Z-Score

```{r}
# Función para un histograma z-score con líneas ±3
hist_z3 <- function(vec, var_name, binwidth = NULL) {
  # Filtrar no finitos
  x <- vec[is.finite(vec)]
  # Si no hay datos válidos o sd = 0, devolver un panel informativo
  if (length(x) == 0 || sd(x, na.rm = TRUE) == 0) {
    return(
      ggplot() +
        annotate("text", x = 0, y = 0, label = paste0(var_name, "\nSin variación o sin datos válidos")) +
        theme_void() + labs(title = var_name)
    )
  }
  # z-scores (scale devuelve matriz -> convertir a numérico)
  z <- as.numeric(scale(x))

  # Binwidth automático si no se especifica
  if (is.null(binwidth)) {
    r <- diff(range(z))
    binwidth <- if (is.finite(r) && r > 0) r / 30 else 0.5
  }

  ggplot(data.frame(z = z), aes(x = z)) +
    geom_histogram(binwidth = binwidth, fill = "skyblue", color = "black", boundary = 0) +
    geom_vline(xintercept = c(-3, 3), linetype = "dashed", color = "red", linewidth = 0.8) +
    theme_minimal() +
    labs(title = var_name, x = "z-score", y = "Frecuencia")
}

# Construir todos los plots
plots <- lapply(varNum, function(v) hist_z3(datos[[v]], v))

# Mostrar en cuadrícula (ajusta ncol según prefieras)
wrap_plots(plots, ncol = 4) + plot_annotation(title = "Histogramas (z-score)")

```

```{r}
library(tidyr)

# Función auxiliar: métricas y outliers z>|3| por variable 
calc_z_outliers <- function(x, var_name) {
  # Mantener solo valores finitos
  xf <- x[is.finite(x)]
  n_total   <- length(x)
  n_finite  <- sum(is.finite(x))
  mu <- mean(xf, na.rm = TRUE)
  s  <- sd(xf, na.rm = TRUE)

  if (!is.finite(s) || s == 0 || n_finite == 0) {
    return(list(
      summary = tibble(
        variable = var_name, n = n_total, n_finite = n_finite,
        mean = mu, sd = s,
        thr_low = NA_real_, thr_high = NA_real_,
        n_out_z3 = 0, pct_out_z3 = 0
      ),
      detail = tibble(
        variable = character(0), row_id = integer(0),
        value = numeric(0), z = numeric(0)
      )
    ))
  }

  # Umbrales en escala ORIGINAL equivalentes a |z|>3
  thr_low  <- mu - 3*s
  thr_high <- mu + 3*s

  # Índices (en el data.frame original) de outliers por z>|3|
  idx_finite <- which(is.finite(x))
  z_vals <- (xf - mu) / s
  out_mask <- abs(z_vals) > 3
  out_idx  <- idx_finite[out_mask]

  summary_tbl <- tibble(
    variable = var_name,
    n = n_total,
    n_finite = n_finite,
    mean = mu, sd = s,
    thr_low = thr_low, thr_high = thr_high,   # μ±3σ en escala original
    n_out_z3 = length(out_idx),
    pct_out_z3 = ifelse(n_finite > 0, 100*length(out_idx)/n_finite, 0)
  )

  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value  = x[out_idx],
    z      = (x[out_idx] - mu) / s
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas 
res_list <- map(varNum, ~ calc_z_outliers(datos[[.x]], .x))

tabla_resumen_z3 <- bind_rows(map(res_list, "summary")) %>%
  arrange(desc(n_out_z3))

tabla_detalle_z3 <- bind_rows(map(res_list, "detail")) %>%
  arrange(variable, desc(abs(z)))

#  Resultados:
tabla_resumen_z3
# head(tabla_detalle_z3)  # si solo quieres una vista
```

Vemos que con este escalado parece que hemos solucionado el problema que teniamos con la detección de outliers via IQR. Ahora tenemos outliers en 7 de las 10 variables y en todos suponen menos del 6% de las observaciones y menos de 2% en todos menos en los 2 con mas valores anómalos.

### 4.1.5. Hampel Identifier

Utilizamos la mediana y la desviación absoluta meidana en vez de la media.

```{r}
# Función para detectar outliers con Hampel
hampel_outliers <- function(x, var_name) {
  # Filtrar valores finitos
  x_finite <- x[is.finite(x)]
  if (length(x_finite) == 0) {
    return(list(
      summary = tibble(variable = var_name, median = NA_real_, mad = NA_real_,
                       lower_bound = NA_real_, upper_bound = NA_real_,
                       n_outliers = 0, pct_outliers = 0),
      detail = tibble(variable = character(0), row_id = integer(0), value = numeric(0))
    ))
  }

  med <- median(x_finite, na.rm = TRUE)
  madv <- mad(x_finite, constant = 1, na.rm = TRUE)
  lower <- med - 3 * madv
  upper <- med + 3 * madv

  idx_finite <- which(is.finite(x))
  out_idx <- idx_finite[which(x_finite < lower | x_finite > upper)]

  # Tabla resumen
  summary_tbl <- tibble(
    variable = var_name,
    median = med,
    mad = madv,
    lower_bound = lower,
    upper_bound = upper,
    n_outliers = length(out_idx),
    pct_outliers = 100 * length(out_idx) / length(x_finite)
  )

  # Tabla detallada
  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value = x[out_idx]
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas
res_hampel <- map(varNum, ~ hampel_outliers(datos[[.x]], .x))

# Combinar resultados en tablas 
tabla_resumen_hampel <- bind_rows(map(res_hampel, "summary")) %>%
  arrange(desc(n_outliers))

tabla_detalle_hampel <- bind_rows(map(res_hampel, "detail")) %>%
  arrange(variable, row_id)

#  Mostrar resultados
tabla_resumen_hampel        # resumen por variable
# head(tabla_detalle_hampel)  # detalle de los outliers (índices y valores)

```
Este método queda descartado para nuestra base de datos ya que se detecta como outlier más del 20% de las observaciones en cuatro variables y más del 5 en 8 de ellas. 

Ademmás, antes de su aplicación ya intuiamos que no iba a resultar productivo este metodo ya que operar con la mediana y la desvación absoluta mediana en distribuciones asimétricas no es compatible.

### 4.1.6. Statistics Tests

#### 4.1.6.1. Grubb's Test

Detección de valores extremos en distribuciones Gaussianas.

Por lo tanto, primero estudiamos que variables admiten este test.
```{r}
library(nortest)  

# Anderson–Darling para cada variable
ad_por_variable <- function(x, var_name) {
  x <- x[is.finite(x)]
  if (length(x) < 8) {
    return(tibble(variable = var_name, n = length(x), p_ad = NA_real_, decision = "No test (<8)"))
  }
  p <- tryCatch(nortest::ad.test(x)$p.value, error = function(e) NA_real_)
  tibble(
    variable = var_name,
    n = length(x),
    p_ad = p,
    decision = ifelse(!is.na(p) && p >= 0.05, "≈ Normal (apto Grubbs)", "No normal (No apto Grubbs)")
  )
}

resultado_AD <- map_dfr(varNum, ~ ad_por_variable(datos[[.x]], .x)) %>%
  arrange(decision, desc(p_ad))

resultado_AD

```

No hay ninguna variable que acepte este test, ya que en todas no se puede asumir normalidad, por lo tanto no será usado en nuestro estudio.

#### 4.1.6.2. Dixon's Test

Este test también queda descartado ya que solo es posible aplicarlo en bbdd pequeñas de entre 3 y 30 observaciones. Teniendo 13186 observaciones el uso de esta prueba no es viable.

#### 4.1.6.3. Rosner's Test

Este test parece ser aplicable ya que es aplicable para muestras grandes y además cuenta con la ventaja de la detección múltiple de valores atípicos. No obstante, este asume normalidad en los datos y ya hemos comprobado que ninguna variable la cumple en el primer subapartado de esta sección. 

Por lo tanto, no hemos podido aplicar ninguno de los tests en nuestra bbdd pero hemos concluido de este apartado que contamos con distribuciones no normales.

### 4.1.7. Conclusiones

Después de haber aplicado todos los métodos de tratamiento de valores anomalos en nuestra base de datos hemos visto que con las tres primeras secciones detectabamos muchos outliers incorrectamente debido a problmas de asimetria y de escala.

Ha sido al escalar las variables (Z-Score) cuando hemos podido recoger un conjunto de posibles valores atípicos más coherente en el que poder imputar aquellos que concluyamos como outliers definitivos. Estos estan guardados con su respectivos ID y variable correspondiente en tabla_detalle_z3.

```{r}
head(tabla_detalle_z3,6)
table(tabla_detalle_z3$variable)
```

## 4.2. Multivariate

Scatter plot de los trios de variable más interesantes (correlacionados):
```{r}
library(scatterplot3d)

num <- datos[, varNum, drop = FALSE]

# Matriz de correlaciones (por pares, ignorando NA)
C <- cor(num, use = "pairwise.complete.obs")

# Puntuar todos los tríos por media de |cor| en sus 3 pares
comb3  <- combn(varNum, 3, simplify = FALSE)
scores <- vapply(comb3, function(v) {
  mean(abs(c(C[v[1], v[2]], C[v[1], v[3]], C[v[2], v[3]])), na.rm = TRUE)
}, numeric(1))

# Seleccionar los N tríos “más interesantes”
N   <- 6 # cambia si quieres más/menos
ord <- order(scores, decreasing = TRUE)
top_trios <- comb3[ord][seq_len(min(N, length(comb3)))]

#  Mostrar ranking
ranking <- data.frame(
  rank = seq_along(top_trios),
  trio = sapply(top_trios, paste, collapse = " · "),
  score = round(scores[ord][seq_along(top_trios)], 3),
  row.names = NULL
)
print(ranking)

# Graficar (con escalado para comparabilidad)
scale_axes <- TRUE
old_par <- par(mfrow = c(ceiling(length(top_trios) / 3), min(3, length(top_trios))))
on.exit(par(old_par), add = TRUE)

for (tri in top_trios) {
  x <- num[[tri[1]]]; y <- num[[tri[2]]]; z <- num[[tri[3]]]
  if (scale_axes) { x <- scale(x)[,1]; y <- scale(y)[,1]; z <- scale(z)[,1] }
  scatterplot3d(
    x, y, z,
    main = paste(tri, collapse = " | "),
    xlab = tri[1], ylab = tri[2], zlab = tri[3],
    pch = 16
  )
}

```
Hacemos el gafico interactivo 3D para el unico trio que presenta una correlación conjunta superior a 0.5 (loudness, energy, acousticness)
```{r, warning=FALSE}
library(plotly)

(fig <- plotly::plot_ly(datos, x = ~loudness, y = ~energy, z = ~acousticness, size = 1) %>% 
       add_markers())
```
### 4.2.1. Caso general

Como contamos con una matriz de covarianza singular con colinealidad fuerte entre algunas de nuestras variables, debemos aplicar fallback a PCA en vez de la forma común dada en la teoria:
```{r}
library(mvoutlier)

# Subset y saneo de columnas 
X <- datos[, varNum, drop = FALSE]
# Asegurar numéricas
X <- X[, sapply(X, is.numeric), drop = FALSE]

# Quitar columnas con varianza 0 o todo NA
sd_cols <- sapply(X, function(x) sd(x, na.rm = TRUE))
X <- X[, sd_cols > 0 & !is.na(sd_cols), drop = FALSE]

# Quitar colinealidad casi perfecta (|r| >= 0.999)
if (ncol(X) > 1) {
  repeat {
    C <- suppressWarnings(cor(X, use = "pairwise.complete.obs"))
    up <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
    if (!nrow(up)) break
    # elimina la de menor varianza del par
    to_drop <- unique(colnames(X)[apply(up, 1, function(idx){
      v1 <- var(X[[idx[1]]], na.rm = TRUE)
      v2 <- var(X[[idx[2]]], na.rm = TRUE)
      if (v1 <= v2) idx[1] else idx[2]
    })])
    X <- X[, setdiff(colnames(X), to_drop), drop = FALSE]
  }
}

# Filas completas
Xc <- X[complete.cases(X), , drop = FALSE]

# Garantizar n > p
if (nrow(Xc) <= ncol(Xc)) {
  ord <- order(apply(Xc, 2, var, na.rm = TRUE), decreasing = TRUE)
  p_new <- max(1, nrow(Xc) - 1)
  Xc <- Xc[, ord[seq_len(p_new)], drop = FALSE]
}

##  DD-Plot con fallback robusto
dd_with_fallback <- function(M, quan = 1/2, alpha = 0.025) {
  M <- as.matrix(M)
  tryCatch(
    dd.plot(M, quan = quan, alpha = alpha),
    error = function(e) {
      message("Covarianza singular: hago fallback a PCA (scores). Detalle: ", e$message)
      pcs <- prcomp(M, center = TRUE, scale. = TRUE)
      # nº de componentes: hasta 95% var o n-1, lo que ocurra antes
      var_exp <- cumsum(pcs$sdev^2) / sum(pcs$sdev^2)
      k <- min(which(var_exp >= 0.95))
      k <- min(k, nrow(M) - 1, ncol(M))
      dd.plot(pcs$x[, seq_len(k), drop = FALSE], quan = quan, alpha = alpha)
    }
  )
}

distances <- dd_with_fallback(Xc, quan = 1/2, alpha = 0.025)

```

Las observaciones distribuidas en la rectangulo inferior izquierdo del dd plot són aquellas observaciónes con valores bajos en ambas distancias (no outliers).

Los que se encuentran en la parte superior izquierda, al contar con un valor de distancia robusta alto, són outliers robustos con una fuerte influencia.

Todos aquellos que se encuentran en el lado derecho de la linea vertica central són aquellos llamados "leverage points", observaciones que influyen mucho en la covarianza clásica.

Ahora, obtenemos los índices de aquellos puntos considerados outliers según las anteriores características.
```{r}
outliers<-distances$outliers
indOut<-which(outliers==TRUE)
indOut # 141 outliers
```

Visualizamos todos los outliers detectados como true
```{r}
head(distances$md.cla)
```
```{r}
head(distances$md.rob)
```

```{r}
head(outliers)
```
```{r}
table(outliers)
```
```{r}
distances_df<-as.data.frame(distances)
distances_df<-distances_df[order(-distances_df$md.rob),]
head(distances_df,10)
```
```{r}
p <- 10   
alpha <- 0.025
umbral2 <- qchisq(1 - alpha, df = p) # sobre D^2
umbral  <- sqrt(umbral2)             # sobre D (md.rob)

plot(distances_df$md.rob, 
     type = "h", 
     ylab = "Distancia de Mahalanobis robusta", 
     xlab = "Índice de observación",
     main = sprintf("Outliers multivariantes (p=%d)", p))
abline(h = umbral, col = "red", lwd = 2, lty = 2)
```

PROBLEMA: Con este metodo solo me deja calcular las distancias de observaciones sin NA's (376). El resto las excluye y solo puedo estudiar valores anomalos dentro de este subconjunto muy pequeño excluyendo las otras 12mil y pico obs.

### 4.2.2. PCA

Hay metodos basados en correlaciones que nos permiten detectar outliers. De momento no los ejecutaremos pero que sepamos que exite esa posibilidad en caso de encontrar dificultades si concluimos que nuestras variables regressoras presentan problemas de multicol·linealidad. Hasta ahora no lo parece.

### 4.2.3. Distancia de Mahalanobis 

```{r}
distancia_mahalanobis <- mahalanobis(datos[,varNum], colMeans(datos[,varNum]), cov(datos[,varNum]))
summary(distancia_mahalanobis)
```
Como vemos tenemos problemas de calculo con la distancia de mahalanobis debido a la cantidad de NA's que tenemos en nuestra bbdd. Haremos este metodo más adelante cuando Dante nos guie en si usar algun tipo de metodo de imputacion de NA's previo para calcular estas distancias.

### 4.2.4. Regresió Lineal i residus 

Un punto con un residuo grande puede considerarse un valor anomalo.

### 4.2.5. Distancia de Cook

Podemos calcular las distancias de Cook para discriminar como outliers aquellas observaciones que cuenten con un valor superior a 1.

### 4.2.6. K-Nearest Neighbors (KNN) Outlier Score

```{r}
library(adamethods)
res_knn<-do_knno(datos[,varNum], k=1, top_n=3000)
```
Con este metodo conseguimos el identificador de las 3000 observaciones con un outliers scoore mas alto.

```{r}
## --- Parámetros ---
k <- 1              # usa el mismo k que en do_knno()
idx_targets <- res_knn  # tus 100 índices

## --- 1) Preparar matriz X con variables numéricas ---
X <- datos[, varNum, drop = FALSE]
p <- ncol(X)

## --- 2) Estandarizar por columna ignorando NAs ---
std_col <- function(v) {
  mu <- mean(v, na.rm = TRUE)
  sdv <- sd(v, na.rm = TRUE)
  if (is.na(sdv) || sdv == 0) return((v - mu))   # evita dividir por 0
  (v - mu) / sdv
}
Xs <- as.data.frame(lapply(X, std_col))
Xs <- as.matrix(Xs)  # para operaciones más rápidas

n <- nrow(Xs)

## --- 3) Función de distancia euclídea "pairwise" ajustada por missingness ---
## d_ij = sqrt( sum((xi-xj)^2 over m) * (p / m) ), con m vars observadas en el par
pairwise_dist_row <- function(i, Xs) {
  xi <- Xs[i, ]
  # Máscara NA por columnas respecto xi (TRUE si xi es no-NA)
  mask_i <- !is.na(xi)
  d <- rep(NA_real_, n)
  for (j in 1:n) {
    if (j == i) next
    xj <- Xs[j, ]
    mask <- mask_i & !is.na(xj)
    m <- sum(mask)
    if (m == 0) {
      d[j] <- NA_real_
    } else {
      diff2 <- xi[mask] - xj[mask]
      d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
    }
  }
  d
}

## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
  d <- pairwise_dist_row(i, Xs)
  # Ordenar ignorando NAs y excluyendo la propia observación
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  vec[k]
}

## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))

resultado_res_knn <- data.frame(
  fila = idx_targets,
  score_knn = scores_res
)

## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
  d <- pairwise_dist_row(i, Xs)
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))

## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)

medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv

## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]

## Ver top
head(resultado_res_knn, 10)
```
```{r}
plot(resultado_res_knn$score_knn)
abline(h=0.3)
```

```{r}

```


### 4.2.7. Local Outlier Factor (LOF)

```{r}
#library(DMwR2)
#ibrary(dplyr)

#outlier.scores <- lofactor(datos[, varNum], k = 5)
#par(mfrow=c(1,1))
#plot(density(outlier.scores))
#outlier.scores
#outliers <- order(outlier.scores, decreasing=T)
#outliers <- order(outlier.scores, decreasing=T)[1:5]
```


#### 4.2.7.1. Nueva versión de LOF

### 4.2.8. Isolation Forest


# 5.NA's

## 5.1 Resumen de NA (variables y casos)
```{r}
datos$fila_original <- seq_len(nrow(datos))

datos <- datos |>
  dplyr::mutate(
    key            = factor(key),
    audio_mode     = factor(audio_mode),
    time_signature = factor(time_signature)
  )
na_var_sum  <- naniar::miss_var_summary(datos)
na_case_sum <- naniar::miss_case_summary(datos)

print(head(na_var_sum, 15))
print(head(na_case_sum, 10))
```

## 5.2 “Bloque” de filas con todos los predictores en NA

```{r}
preds <- setdiff(names(datos), c("ID","song_popularity"))
bloque_na_filas <- sum(rowSums(is.na(datos[, preds])) == length(preds))
completas_pred  <- sum(stats::complete.cases(datos[, preds]))
cat("\n[1.2] Filas con TODOS los predictores en NA:", bloque_na_filas, "\n")
cat("[1.2] Filas con predictores completos:", completas_pred, "\n")
```

## 5.3 Exclusión del bloque sin información de predictores
```{r}

datos_filtrado <- datos[rowSums(is.na(datos[, preds])) < length(preds), ]
cat("\n[1.3] Dimensión tras excluir bloque:", nrow(datos_filtrado), "x", ncol(datos_filtrado), "\n")
```
## 5.4 Detección y exclusión de variables casi nulas
```{r}
excluir_base <- c("ID", "song_popularity")

X <- datos[ , setdiff(names(datos), excluir_base), drop = FALSE]

nzv_like <- function(x){
  x_noNA <- x[!is.na(x)]
  n <- length(x_noNA)
  if (n == 0) return(list(nzv = TRUE, reason = "all_NA"))
  # % de valores únicos (como caret)
  pct_unique <- 100 * (length(unique(x_noNA)) / n)
  # ratio entre la categoría/valor más frecuente y el segundo
  tb <- sort(table(x_noNA), decreasing = TRUE)
  top <- as.numeric(tb[1])
  second <- as.numeric(if (length(tb) >= 2) tb[2] else 0)
  freq_ratio <- if (second == 0) Inf else top / second
  # constante (varianza cero) para numéricos
  const_num <- is.numeric(x_noNA) && sd(x_noNA) < .Machine$double.eps
  # near-zero variance estilo caret
  nzv <- (pct_unique <= 10) && (freq_ratio > 19)
  reason <- c()
  if (const_num) reason <- c(reason, "constante")
  if (nzv)       reason <- c(reason, "near-zero")
  if (length(reason) == 0) reason <- "ok"
  list(nzv = (const_num || nzv), reason = paste(reason, collapse = "+"),
       pct_unique = pct_unique, freq_ratio = freq_ratio)
}

res <- lapply(X, nzv_like)
tab <- data.frame(
  variable   = names(res),
  nzv        = vapply(res, `[[`, logical(1), "nzv"),
  reason     = vapply(res, `[[`, character(1), "reason"),
  pct_unique = round(vapply(res, `[[`, numeric(1), "pct_unique"), 3),
  freq_ratio = round(vapply(res, `[[`, numeric(1), "freq_ratio"), 3),
  row.names = NULL
)

# Resultado
cat("\nVariables que vamos a eliminar):\n")
print(tab[tab$nzv, c("variable","reason","pct_unique","freq_ratio")], row.names = FALSE)

cat("\nVariables que conservamos:\n")
print(tab[!tab$nzv, c("variable","pct_unique","freq_ratio")], row.names = FALSE)


vars_a_quitar <- tab$variable[tab$nzv]
datos_nzv <- datos[, setdiff(names(datos), vars_a_quitar), drop = FALSE]
```

## 5.6 Selección de predictores 

```{r}

## 5.6.1) Correlaciones con el target (solo numéricas, excluyendo ID)
is_num  <- sapply(datos_nzv, is.numeric)
nums    <- setdiff(names(datos_nzv)[is_num], c("ID","song_popularity","fila_original"))
cors    <- sapply(nums, function(v) cor(datos_nzv[[v]], datos_nzv[["song_popularity"]], use = "complete.obs"))
cat("\n[3.1] Correlaciones con song_popularity:\n")
print(round(cors, 4))
## 5.6.2) Selección por umbral |r| >= 0.3
umbral <- 0.3
sel_num <- names(which(abs(cors) >= umbral))
cat("\n[3.2] Numéricas seleccionadas con |r| >= ", umbral, ":\n", sep = "")
print(sel_num)

## 5.6.3) Añadir categóricas 
cat_vars <- intersect(c("key","audio_mode"), names(datos_nzv))
predictores_finales <- unique(c(sel_num, cat_vars))
cat("\n[3.3] Predictores finales (numéricas por score + categóricas):\n")
print(predictores_finales)
```

## 5.7 Filtrado de outliers con el score KNN

```{r}

stopifnot(exists("resultado_res_knn"))
stopifnot(all(c("fila") %in% names(resultado_res_knn)))
stopifnot("ID" %in% names(datos))        # para mapear fila -> ID
stopifnot("ID" %in% names(datos_nzv))    # para filtrar en el dataset final

# Elegimos score normalizado en [0,1]
if ("score_minmax_0_1" %in% names(resultado_res_knn)) {
  score_norm <- resultado_res_knn$score_minmax_0_1
} else {
  
  s <- resultado_res_knn$score_knn
  s_min <- min(s, na.rm = TRUE); s_max <- max(s, na.rm = TRUE)
  score_norm <- (s - s_min) / (s_max - s_min)
}

# Filas que superan el umbral 0.3
umbral <- 0.3
filas_out <- resultado_res_knn$fila[!is.na(score_norm) & score_norm >= umbral]

# Convertimos filas -> IDs del dataset original
filas_out <- filas_out[filas_out >= 1 & filas_out <= nrow(datos)]
ids_out   <- unique(as.character(datos$ID[filas_out]))
ids_out   <- ids_out[!is.na(ids_out)]

cat("\n[4] Outliers por score >= ", umbral, 
    "  | filas=", length(filas_out), 
    "  | IDs únicos=", length(ids_out), "\n", sep = "")

# Filtramos en el dataset ya depurado (datos_nzv)
n0 <- nrow(datos_nzv)
en_nzv <- sum(datos_nzv$ID %in% ids_out)

if (en_nzv > 0) {
  datos_nzv <- datos_nzv[!(datos_nzv$ID %in% ids_out), , drop = FALSE]
  cat("Filtrado aplicado en datos_nzv: ", n0, " -> ", nrow(datos_nzv),
      " (eliminadas ", en_nzv, " filas)\n", sep = "")
} else {
  cat("AVISO: 0 IDs intersectan con datos_nzv (puede que esas filas ya se eliminasen por NA/NZV o por partición).\n")
}

```

```{r}
set.seed(123)
idx <- sample.int(nrow(datos_nzv), size = floor(0.7 * nrow(datos_nzv)))
train <- datos_nzv[idx, , drop = FALSE]
test  <- datos_nzv[-idx, , drop = FALSE]

y_train <- train[["song_popularity"]]
y_test  <- test[["song_popularity"]]
cat("\n[5] Split -> train:", nrow(train), " | test:", nrow(test), "\n")
```

```{r}
# Identificación numérica/factor en TRAIN
is_num_train  <- sapply(train, is.numeric)
is_fact_train <- sapply(train, is.factor)

# Medianas (num) y modas (factor) en TRAIN
medianas <- vapply(train[, is_num_train, drop = FALSE],
                   function(v) stats::median(v, na.rm = TRUE), numeric(1))

moda <- function(v){
  v2 <- v[!is.na(v)]
  if (!length(v2)) return(NA)
  names(sort(table(v2), decreasing = TRUE))[1]
}
modas <- vapply(train[, is_fact_train, drop = FALSE], moda, character(1))

# Función de imputación
imputa_df <- function(df){
  # Numéricas
  for (nm in names(medianas)) if (nm %in% names(df)) {
    v <- df[[nm]]; v[is.na(v)] <- medianas[[nm]]; df[[nm]] <- v
  }
  # Factores
  for (nm in names(modas)) if (nm %in% names(df)) {
    v <- df[[nm]]; repl <- modas[[nm]]
    v[is.na(v)] <- repl
    if (is.factor(v) && !is.na(repl) && !(repl %in% levels(v))) {
      levels(v) <- c(levels(v), repl)
    }
    df[[nm]] <- v
  }
  df
}

train_imp <- imputa_df(train)
test_imp  <- imputa_df(test)

cat("[6] Imputación realizada con mediana (num) y moda (factor) usando TRAIN.\n")
# Conjunto de predictores para modelado
# =====================================================================
preds_final <- setdiff(names(train_imp), c("ID", "song_popularity"))
cat("[7] Predictores finales:", length(preds_final), "\n")
```
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
```


# 6. Feature engineering

## 6.1 Feature selection

### 6.1.1 Null variance

Se identifican aquellas variables cuya varianza es prácticamente o completamente nula, ya que, al tener valores muy similares entre ellos, se consideran constantes y pueden estar altamente correlacionadas con el término independiente del modelo, lo que solo introduciría ruido.  

```{r, warning=FALSE}
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)

numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
```


Esta salida muestra que no hay ninguna variable con varianza exactamente igual a cero ni cercana a 0, lo cual indica que no hay necesidad de eliminar ninguna variable por falta de información. Sin embargo, las variables referentes a $liveness$, $danceability$, $audio_valence$, $energy$, $speechiness$ y, especialmente, $time_signature$, $key$ y $audio_mode$, tienen un *percentUnique* relativamente bajo, dando a entender que tienen pocos valores distintos. 

### 6.1.2 Feature correlation

Una manera alternativa de mirar si hay variables redundantes es calculando la correlación entre las variables predictoras. Un coeficiente alto de correlación puede generar un problema de multicolinealidad, que puede afectar negativamente a las predicciones del modelo. Anteriormente, mediante un gráfico den correlaciones entre las variables numéricas, se observó una elevada correlación entre **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Se pretende encontrar correlaciones que superen un coeficiente de 0.6:
```{r, warning=FALSE}
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
```

Hay 8 correlaciones que superan un valor de 0.6, es decir, hay 4 variables altamente correlacionadas a pares. 
A través de la visualización del gráfico de correlaciones deducimos de qué variables se trata.

```{r, warning=FALSE}
library("corrplot")

matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
```

La representación gráfica muestra una evidencia clara de correlación entre las variables previamente mencionadas. Este hecho nos impulsa a eliminar alguna de estas variables que ya están explicadas por otras. 

### 6.1.3 Linear combinations

Identificamos si existen combinaciones lineales entre las variables predictoras, otra medida que nos informa acerca de la posible correlación existente entre ellas. 

```{r, warning=FALSE}
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
```

Mediante este método, no se detectan combinaciones lineales entre las variables. Esto conlleva a deducir que no hay ninguna variable que contenga esencialmente la misma información que otra desde una perspectiva lineal.

### 6.1.4 Wrapper

Se aplica el método **Wrapper** para seleccionar las variables más relevantes del modelo.

```{r, warning=FALSE}
# install.packages("caret")
library(caret)

# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)

# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]

# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")

if(length(variables_redundantes) > 0) {
  input <- input[, -variables_redundantes]
  cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}

# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
                      method = "cv",        # Validación cruzada
                      number = 5)           # 5-fold CV

# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
                      sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
                      rfeControl = control)

# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
```

La técnica de Eliminación Recursiva de Características (*RFE: Recursive Feature Elimination*) es un tipo de método **Wrapper** que selecciona las variables más relevantes para un modelo entrenándolo varias veces mediante un proceso de validación cruzada y eliminando en cada iteración las variables menos importantes. El algoritmo termina hasta encontrar el subconjunto que ofrece el mejor rendimiento. Cabe destacar que dicho método ha tenido en cuenta la correlación entre las variables predictoras. De esta manera, se ha obtenido un conjunto de variables explicativas que no superan un coeficiente de correlación de 0,6. 
Estas variables son: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$.



El análisis exhaustivo de selección de variables reveló un **conjunto óptimo** de **7 predictores** para el modelo de popularidad musical. Tras eliminar variables redundantes por alta correlación (> 0.6), el método **Wrapper RFE** identificó las características más relevantes: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Este conjunto representa una reducción del 46% en dimensionalidad mientras mantiene el poder predictivo, asegurando la estabilidad del modelo mediante la eliminación de multicolinealidad.


## 6.2 Feature transformation

El objetivo de este apartado es mejorar la distribución de las variables, reducir el sesgo y preparar los datos para los algoritmos de modelado.

### 6.2.1 Análisis inicial de distribuciones

```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
  skewness_values <- sapply(datos, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                          ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                            "liveness", "instrumentalness", "acousticness", "loudness")

datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
```

Mientras que las variables correspondientes a $danceability$ y $audio_valence$ presentan un sesgo bajo, el resto de variables obtienen un valor de sesgo potencialmente alto, a excepción de la variable $acousticness$, donde el sesgo es moderado. 

### 6.2.2 Visualización de distribuciones originales

```{r, warning=FALSE}
cat("\n VISUALIZACIÓN DE DISTRIBUCIONES ORIGINALES:\n")

# Función para graficar distribuciones
graficar_distribuciones <- function(datos, titulo) {
  datos_long <- datos %>%
    pivot_longer(everything(), names_to = "Variable", values_to = "Valor")
  
  ggplot(datos_long, aes(x = Valor)) +
    geom_histogram(aes(y = ..density..), fill = "steelblue", alpha = 0.7, bins = 30) +
    geom_density(color = "red", linewidth = 1) +
    facet_wrap(~ Variable, scales = "free") +
    labs(title = titulo,
         x = "Valor", y = "Densidad") +
    theme_minimal()
}

# Graficar distribuciones originales
graficar_distribuciones(datos_transformar, "Distribuciones Originales - Antes de Transformación")
```

### 6.2.3 Aplicación de las transformaciones

```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(datos_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(datos_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
      datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(datos_transformados[[var]])
        datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
```

Las variables con sesgo alto (mayor que 0.5 en valor absoluto), son: $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Se aplican 3 transformaciones a cada una:

  **1.** `log`: Permite reducir el sesgo positivo fuerte y funciona mejor cuando hay valores extremos positivos.
  
  **2.** `sqrt`: Se usa especialmente cuando el sesgo es moderado, y es menos agresiva que log, pero buena para datos con ceros.

  **3.** `box-cox`: Encuentra la transformación óptima automáticamente, con el requisito de que los valores sean estrictamente positivos. 

```{r}
any(datos_transformados$instrumentalness==0)
```

Dado que $instrumentalness$ contiene algún valor igual a 0, se han aplicado solo las dos primeras transformaciones. 

### 6.2.4 Evaluación de Transformaciones

A continuación, se realiza una evaluativa 

```{r evaluacion_transformaciones, warning=FALSE}
# Función para evaluar efectividad de transformaciones
evaluar_transformaciones <- function(var_original, vars_transformadas, nombre_original) {
  resultados <- data.frame()
  
  sesgo_original <- e1071::skewness(var_original, na.rm = TRUE)
  
  for(transform_name in names(vars_transformadas)) {
    if(transform_name != nombre_original) {
      var_transformada <- vars_transformadas[[transform_name]]
      sesgo_transformado <- e1071::skewness(var_transformada, na.rm = TRUE)
      
      reduccion_sesgo <- abs(sesgo_original) - abs(sesgo_transformado)
      
      resultados <- rbind(resultados, data.frame(
        Variable = nombre_original,
        Transformacion = transform_name,
        Sesgo_Original = round(sesgo_original, 3),
        Sesgo_Transformado = round(sesgo_transformado, 3),
        Reduccion_Sesgo = round(reduccion_sesgo, 3),
        Efectiva = ifelse(abs(sesgo_transformado) < abs(sesgo_original) & 
                          abs(sesgo_transformado) < 1, "SÍ", "NO")
      ))
    }
  }
  
  return(resultados)
}

# Evaluar todas las transformaciones
resultados_evaluacion <- data.frame()

for(var in variables_a_transformar) {
  # Obtener todas las versiones de la variable
  vars_relacionadas <- datos_transformados %>% 
    select(starts_with(var))
  
  eval_var <- evaluar_transformaciones(
    datos_transformar[[var]], 
    vars_relacionadas,
    var
  )
  
  resultados_evaluacion <- rbind(resultados_evaluacion, eval_var)
}

cat("EVALUACIÓN DE TRANSFORMACIONES:\n")
print(resultados_evaluacion)

# Seleccionar la mejor transformación para cada variable
mejores_transformaciones <- resultados_evaluacion %>%
  filter(Efectiva == "SÍ") %>%
  group_by(Variable) %>%
  filter(abs(Sesgo_Transformado) == min(abs(Sesgo_Transformado))) %>%
  ungroup()

cat("\n MEJORES TRANSFORMACIONES SELECCIONADAS:\n")
print(mejores_transformaciones)
```


El análisis de transformaciones reveló que **Box-Cox** fue **óptima** para **variables** con **alto sesgo** ($speechiness$, $liveness$), logrando reducciones superiores al 90%, mientras que la **raíz cuadrada** resultó más **efectiva** para **sesgos moderados** ($acousticness$). En cambio, para la variable $instrumentalness$ no se salió efectiva ninguna de las dos modificaciones aplicadas. 
Las transformaciones seleccionadas aseguran que todas las variables cumplan con $|sesgo| < 0.5$, mejorando significativamente los supuestos de normalidad para el modelado posterior.

## 6.3 Feature extraction

### 6.3.1 Preparar datos para PCA

```{r extraccion_variables, warning=FALSE}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)
variables_para_analisis <- c("danceability", "audio_valence", "speechiness", "liveness", 
                             "instrumentalness", "acousticness", "loudness")

# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
```

Se estudian las 7 variables que el método **Wrapper** había proporcionado como conjunto óptimo, pero teniendo en cuenta la transformación de las 3 variables anteriores. 

### 6.3.2 Análisis de Componentes Principales (PCA)

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
```

Dado que se disponen de 7 variables, existen 7 componentes principales. 

### 6.3.3 Determinar Componentes Principales

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
  cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
```

El **análisis** de **componentes principales** sobre las **7 variables óptimas** revela una estructura dimensional bien definida. Los primeros **5 componentes** explican individualmente **más del 10% de la varianza**, acumulando **86.7%** de la **información total**.

PC1 (29%) emerge como la dimensión principal, capturando casi un tercio de la variabilidad del conjunto de datos. La **distribución gradual** de la **varianza** entre los componentes indica que **ninguna** **variable** **domina** excesivamente el **análisis**, sino que **todas** **contribuyen** de manera **balanceada** a las diferentes dimensiones latentes.

Este resultado confirma que las **7 variables seleccionadas** por **RFE** contienen información complementaria y valiosa para el modelo, justificando la retención de múltiples componentes que capturan distintas facetas de las características musicales relevantes para predecir la popularidad.

### 6.3.4 Crear nuevas variables (componentes)

Se crean las nuevas variables, las cuales corresponden a las 5 primeras componentes principales, que no son más que combinaciones lineales de las variables originales. 

```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
  
  # Dar nombres descriptivos
  nombres_descriptivos <- c()
  for(i in 1:length(componentes_importantes)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos <- c(nombres_descriptivos, nombre)
  }
  colnames(nuevos_componentes) <- nombres_descriptivos
  
  # Añadir al dataset original
  datos_final <- cbind(datos_transformados, nuevos_componentes)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
  cat("   • Total variables en dataset final:", ncol(datos_final), "\n\n")
  
} else {
  datos_final <- datos_final_transformado
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```

El **proceso de extracción** logró una compresión **eficiente** (86.3% de varianza con 5 componentes). 

Las variables seleccionadas son: $\text{speechiness_boxcox, liveness_boxcox, acousticness_sqrt, danceability, audio_valence y loudness}$.

# KNN

Base de datos con las variables seleccionadas y sus transformaciones
```{r}
datos_prepro<-datos_final[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence", "loudness")]
datos_prepro$song_popularity<-datos_num$song_popularity
# View(datos_prepro)
```


### Elección del valor óptimo de k
```{r}
library(FNN)
set.seed(1994)

# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro

train <- datos[default_idx, ]
test  <- datos[-default_idx, ]

X_train <- train[, -7]
X_test  <- test[, -7]
y_train <- train[, 7]
y_test  <- test[, 7]

# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))

# Predicción simple con k = 1
pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = 1)
head(pred$pred)

# Función para calcular RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Vector de valores de k a probar
k <- c(1,3,5,7,9,11,13,15,17,19)   # puedes ampliar si quieres

# Calcular RMSE para cada k
rmse_values <- sapply(k_values, function(k) {
  make_knn_pred(k = k,
                training = X_train,
                predicting = X_test,
                valueTarget = y_train,
                actual = y_test)
})

# Mostrar los resultados
data.frame(k = k_values, RMSE = rmse_values)

# Encontrar el k con menor RMSE
best_k <- k_values[which.min(rmse_values)]
best_rmse <- min(rmse_values)

cat("Mejor k:", best_k, "\nRMSE mínimo:", best_rmse, "\n")
```

Hemos probado valores impares para k, concretamente: 1,3,5,7,9,11,13,15,17,19 y hemos visto que para k=5 el *rmse* es mínimo. 

### Modelo con k óptimo 

```{r}
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test

# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)

# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 5, ":", rmse, "\n")
```

**Cálculo del MAPE**
```{r}

```

