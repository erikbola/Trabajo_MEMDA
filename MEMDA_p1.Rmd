---
title: "proyecto regression"
author: "Erik Bola, Vega Carmona, Emma Carretero, Sandra González y Raquel Purroy"
date: 
output:
  html_document: default
  pdf_document: default
---
# 0. Paquetes y Datos
```{r setup, include=FALSE}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
library(DataExplorer)
library(inspectdf)
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)

# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
```

```{r}
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# 1. Descripción del problema
```{r, warning=FALSE}
str(datos)
```
Preliminares
```{r}
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)

clases <- sapply(datos, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```

Comprobación
```{r}
str(datos)
```

# 2. Análisis Exploratorio

## 2.1. Univariant Analysis

### 2.1.1. Numerical

Descrpition
```{r}
library(psych)
psych::describe(datos[, varNum])
```

En base a este resumen estadístico se puede afirmar que la mayoría de las canciones no son en vivo, dado que el promedio de la variable $liveness$ es bastante bajo. Respecto a el volumen de las canciones, teniendo en cuenta que $loudness$ es una característica que oscila entre -36.73dB y 1.34dB, su valor medio parece indicar que es relativamente alto. 

La media de $danceability$ apunta que las canciones son bastante bailables, y el valor esperado de $audio_valence$ muestra que las canciones tienden a tener emociones más alegres y positivas, ya que es ligeramente superior a 0,5. 

Asimismo, haciendo referencia a cómo de energéticas o animadas son dichas canciones, $energy$ señala que, en promedio, puede decirse que lo son bastante. 

En cuanto al grado acústico, observando que el valor medio es bajo, se podría deducir que mayoritariamente las canciones son producidas electrónicamente. 

Por último, se podría añadir que gran parte de las canciones no son habladas, pues la variable $speechiness$ tiene un promedio bajo. Destaca la variabilidad del nivel de popularidad ($song_popularity$), con canciones desde poco conocidas hasta muy populares.

Graphics (base)
```{r}
for (var in varNum[-1]) {
  # Acceder a la columna por nombre y asegurarse de que es numérica
  column_data <- as.numeric(datos[[var]])
  
  # Comprobar si la columna es numérica
  if (is.numeric(column_data)) {
    # Eliminar NAs
    column_data <- na.omit(column_data)
    
    # Comprobar si quedan datos después de eliminar NAs
    if (length(column_data) > 0) {
      hist(column_data, main = paste0("Histograma variable ", var))
      boxplot(column_data, main = paste0("Boxplot variable ", var))
    } else {
      warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
    }
  } else {
    warning(paste("La variable", var, "no es numérica y será ignorada"))
  }
}

par(mfrow = c(1, 1))


```

Graphics (ggplot2 + patchwork)
```{r, eval=FALSE}
library(ggplot2)
library(patchwork)

plots <- list()

for (var in varNum) {
  histo <- ggplot(datos, aes(x = .data[[var]])) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
    geom_density(alpha = .2, fill = "#FF6666") +
    geom_vline(aes(xintercept = mean(.data[[var]], na.rm = TRUE)),
               color = "blue", linetype = "dashed", linewidth = 1) +
    ggtitle(paste("Histograma de", var))
  boxp <- ggplot(datos, aes(x = .data[[var]])) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) +
    ggtitle(paste("Boxplot de", var))

  plots <- append(plots, list(histo, boxp))
  print(plots)
}
```

Combinar en un grid automático con 2 columnas
```{r, eval=FALSE, include=FALSE}
final_plot <- Reduce(`+`, plots) + plot_layout(ncol = 2)
final_plot
```

### 2.1.2. Categorical

Description
```{r}
for (var in varCat) {
  tablaAbs <- data.frame(table(datos[, var]))
  tablaFreq <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  m <- match(tablaAbs$Var1, tablaFreq$Var1)
  tablaAbs[, "FreqRel"] <- tablaFreq[m, "Freq"]
  colnames(tablaAbs) <- c("Categoria", "FreqAbs", "FreqRel")
  cat("===============", var, "===================================/n")
  print(tablaAbs)
  cat("==================================================/n")
}
```
Las frecuencias absolutas de las variables categóricas, muestran, en general, una distribución equitativa de las modalidades. Sin embargo, la variable $time_signature$ posee un mayor número de casos en la cuarta categoría.

Graphics (base)
```{r}
# Configurar la ventana gráfica para mostrar 2 filas y 3 columnas
par(mfrow = c(2, 3))

# Crear gráficos de barras con títulos personalizados
for (var in varCat) {
  if (var == "time_signature") {
    barplot(table(datos[, var]),
            main = "Distribución de Time Signature",
            col = "skyblue")
  } else if (var == "key") {
    barplot(table(datos[, var]),
            main = "Distribución de Key",
            col = "lightgreen")
  } else if (var == "audio_mode") {
    barplot(table(datos[, var]),
            main = "Distribución de Audio Mode",
            col = "lightcoral")
  } else {
    barplot(table(datos[, var]),
            main = var,  # título genérico si hay más variables
            col = "gray80")
  }
}

# Restaurar la configuración original
par(mfrow = c(1, 1))
```

-   **time_signature**:

la categoría 4 es la más frecuente, con casi 8000 canciones. Esto corresponde al compás 4/4, el más común en música popular. El resto de niveles, 0, 1, 3 y 5 son mucho menos frecuentes, lo que indica que compases alternativos o inusuales son raros en esta colección de canciones.

-   **key**:

La distribución es bastante uniforme, con cada tonalidad apareciendo entre 1000 y 2000 veces. No hay una tonalidad claramente dominante, lo que sugiere diversidad armónica en el conjunto de datos.

-   **audio_mode**:

Predominan las canciones en modo mayor, lo que puede reflejar una tendencia hacia música más optimista o comercial.

Graphics (ggplot2 + gridExtra)
```{r}
library(gridExtra)

plots <- list()  # lista vacía
i <- 1           # índice

for (var in varCat) {
  tabla <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  p <- ggplot(data = tabla, aes(x = Var1, y = Freq)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(round(Freq * 100, 2), "%")),
              vjust = 1.6, color = "white", size = 3.5) +
    theme_minimal() +
    labs(title = paste("Distribución de", var), x = var, y = "Proporción")

  plots[[i]] <- p
  i <- i + 1
}
# Mostrar todos los gráficos en un grid (ejemplo con 2 columnas)
# grid.arrange(grobs = plots, ncol = 2)
```

## 2.2. Bivariant Analysis

#### 2.2.1 Numerical vs. numerical

Description

```{r}
cor(na.omit(datos[, varNum]))
```

Las correlaciones con valores más extremos se encuentran entre las variables **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Graphics (base / PerformanceAnalytics)

```{r}
library(PerformanceAnalytics)
chart.Correlation(as.matrix(datos[, varNum]), histogram = TRUE, pch = 12)
```

Graphics (ggplot2 / ggcorrplot)

```{r, include=FALSE, eval=FALSE}
#install.packages("ggcorrplot")
library(ggcorrplot)
corr <- round(cor(datos[, varNum]), 1)
ggcorrplot(corr, lab = TRUE)
```

#### 2.2.2 Numerical vs. categorical

Description

```{r}
for (varN in varNum) {
  for (varC in varCat) {
   print(psych::describeBy(datos[, varN], group = datos[, varC]))
  }
}
```

Graphics (ggplot2)

```{r}
library(ggplot2)
library(gridExtra)

plots <- list()
i <- 1

for (varC in varCat) {
  for (varN in varNum) {
    grafico <- ggplot(datos, aes(x = .data[[varN]], fill = .data[[varC]])) +
      geom_histogram(colour = "black",
                     lwd = 0.75,
                     linetype = 1,
                     position = "identity",
                     alpha = 0.5) +
      labs(title = paste("Histograma de", varN, "por", varC),
           x = varN, y = "Frecuencia", fill = varC) +
      theme_minimal()
    plots[[i]] <- grafico
    i <- i + 1
  }
}
# Mostrar todos en un grid (2 columnas)
grid.arrange(grobs = plots, ncol = 2)
```

#### 2.2.3 Categorical vs. categorical

Description

```{r}
for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      prop_table <- prop.table(table(datos[[varc1]], datos[[varc2]]))
      cat("=============", varc1, " vs. ", varc2, "=========================\n")
      print(prop_table)
    }
  }
}

```

Graphics (base)

```{r}
par(mfrow = c(3, 3))
par(mar = c(3, 3, 3, 1))  

for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      tab <- table(datos[[varc1]], datos[[varc2]], useNA = "no")
      prop_tab <- prop.table(tab, margin = 2)

      barplot(prop_tab,
              beside = TRUE,
              ylim = c(0, 1),
              main  = paste(varc1, "vs", varc2),
              xlab  = varc2,
              ylab  = paste("Proporción dentro de", varc2))
      legend("topright", legend = rownames(prop_tab), cex = 0.7, bty = "n")
    }
  }
}

par(mfrow = c(1, 1))

```

# 3. EDA (Automatic Descriptive Analysis)

## 3.1. Skim

```{r}
library(skimr)
library(tidyverse)

# Podem visualitzar un descriptiu de les dades
skim(datos)
```

```{r}
# Visualitzem exclusivament les variables numériques
skim(datos) %>% yank("numeric")
```

```{r}
skim(datos) %>% yank("character")
```

## 3.2 Vis

```{r}
library(visdat)
## Busquem per a variables numériques o categóriques si hi ha NA's
vis_dat(datos)
```

Este gráfico representa que la mayoría de las variables son numéricas, con menos cantidad de variables categóricas. También se osberva que hay un gran número de valores missings en los dos tipos de variables.

```{r}
## Visualitzem percentatges de NA's en les variables
vis_miss(datos)
```

Concretamente, hay un total de 26% de valores perdidos.

```{r}
## Generem la matriu de correlacions
datos %>% dplyr::select(where(is.numeric)) %>% vis_cor()
```

Mediante la visualización de este gráfico de correlaciones, se refuerzan las relaciones lineales entre los diferentes pares de variables.

```{r}
## Podem visualitzar condicionants de les dades. En aquest cas, mirem si tenim mes de
## 2 clases
vis_expect(datos, ~ .x > 2)
```

## 3.3. Inspect df

```{r}
library(inspectdf)

## Tipus de dades
inspect_types(datos) %>% show_plot()
```

La base de datos contiene 3 variables factor, y 11 cuantitativas.

```{r}
## Utilització de la memoria
inspect_mem(datos) %>% show_plot()
```

```{r}
# Paquetes
library(dplyr)
library(inspectdf)
library(rlang)

# --- 1) Crear una categórica desde una numérica y comparar (High/Low) ---
# Usamos 'song_popularity' como ejemplo con umbral 50.
umbral <- 50
num_target <- "song_popularity"

stopifnot(num_target %in% c(
  "liveness","loudness","danceability","song_duration_ms","audio_valence",
  "energy","tempo","acousticness","speechiness","instrumentalness","song_popularity"
))

# Convertir a numérico por si viniera como texto
datos[[num_target]] <- suppressWarnings(as.numeric(datos[[num_target]]))

data_price_dummy <- datos %>%
  mutate(price_dummy = if_else(.data[[num_target]] > umbral, "High", "Low") %>% factor())

# Comparativa de NA entre High y Low
inspect_na(
  data_price_dummy %>% filter(price_dummy == "High"),
  data_price_dummy %>% filter(price_dummy == "Low")
) %>% show_plot()

```

```{r}
## Comprovem la distribució de les variables
inspect_num(datos) %>% show_plot()
```

```{r}
## check categorical variable distribution
inspect_imb(datos) %>% show_plot()
```

```{r}
## check two categorical
inspect_imb(data_price_dummy %>% dplyr::filter(price_dummy == "High"),
            data_price_dummy %>% dplyr::filter(price_dummy == "Low")) %>%
  show_plot() + theme(legend.position = "none")
```

```{r}
## similiar to inspect_imb, but for all levels
inspect_cat(datos) %>% show_plot()
```

```{r, eval=FALSE, include=FALSE}
inspect_cor(datos) %>% show_plot()
```

## 3.4 DataExplorer

```{r}
library(DataExplorer)
plot_str(datos)
introduce(datos)
```

```{r}
plot_intro(datos)
```

```{r}
plot_missing(datos)
```

Tal y como se muestra en esta ilustración, se ha asignado un 30% de valores missings en cada variable.

```{r}
plot_bar(datos)
```

Cabe destacar que la variable $key$ posee una gran cantidad de valores faltantes, algo que deberá de tratarse en el preprocesamiento de los datos para evitar el sesgo y que este hecho pueda cambiar significativamente los resultados. Asimismo, $time_signature$ también presenta bastantes valores perdidos.

```{r}
plot_histogram(datos)
```

Muchas de las distribuciones de las variables parecen tener asimetría, bien sea por la izquierda o por la derecha. Esto induce a pensar que variables como $acousticness$, $instrumentalness$, $speechiness$ o $loudness$ tienden a tomar valores más extremos.

```{r}
library(DataExplorer)

# Opción 1: omitir filas con NA
plot_correlation(na.omit(datos), type = "all", maxcat = 5L)
```

## 3.5. SmartEDA
```{r}
library("SmartEDA")
## Overview of the data
ExpData(data = datos,type = 1)

## structure of the data    
ExpData(data = datos,type = 2)
```

```{r}
SmartEDA::ExpCTable(datos,Target=NULL,margin=1,clim=10,nlim=5,round=2,bin=NULL,per=T)
```

# 4. Outliers treatment

```{r}
options(scipen = 999)
diagnose_numeric(datos)
```

Vemos que nos identifica muchisimos outliers (mas de 300 en 5 de las variables numericas) lo que inidca que el criterio de selección de anomalias debe ser mas estricto.

```{r}
diagnose_category(datos)
```

## 4.1. Univariate

### 4.1.1. Max and Min

```{r}
mapply(function(x, name) {
  cat("var. ", name, ": \n\t min: ", min(x, na.rm = TRUE),
      "\n\t max: ", max(x, na.rm = TRUE), "\n")
  invisible(NULL)
}, datos[, varNum], colnames(datos[, varNum]))
```

Vemos que existen outliers en todas la variables en tanto que el maximo o el minimo de estas se encuentra altamente alejado del valor del tercer o el primer quantil respectivamente. És una primer hipòtesis que nos lleva a ejecutar la identificación de estos outliers.

### 4.1.2. IQR

$$[Q1-1.5*IQR,Q3+1.5*IQR]\\IQR=[Q3-Q1]$$

```{r}
IQROutlier <- function(variable, rmnas = TRUE) {
  IQ <- IQR(variable, na.rm = rmnas)
  intInf <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[1]] - 1.5 * IQ
  intSup <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[2]] + 1.5 * IQ
  posicions <- which(variable >= intSup | variable <= intInf)
  if (length(posicions) > 0) {
    cat("Existeixen outliers en les posicions:", paste0(posicions, collapse = ", "), "\n")
  } else {
    cat("No existeixen outliers\n")
  }
  return(posicions)
}
```

Aplicamos para todas las variables numéricas

```{r}
mapply(function(x, name) {
  cat("\n\nVariable:", name, "\n")
  IQROutlier(x)
}, datos[, varNum], varNum)
```

Tenemos muchisimos outliers por variable y esta visualización es poco usable para tratar con ellos. Vemos que el metodo IQR detecta como outliers muchas observaciones en aquellas variables que presentan asimetria en sus distribuciones. Recordemos los histogramas de estas variables y observamos como a mayor es la asimetria del grafico en una variable mas outliers detecta el metodo IQR:

```{r}
plot_histogram(datos)
```

Conluimos para este apartados que podriamos considerar realmente outliers aquellos detectados por IQR en distribuciones gaussianas como en danceability por ejemplo.

### 4.1.3. Boxplot

```{r, warning=FALSE}
library(patchwork)

plots <- lapply(varNum, function(v) {
  ggplot(datos, aes(y = datos[[v]])) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = v, y = v) +
    theme_minimal()
})

wrap_plots(plots)
```

Graficando los boxplots vemos de forma más evidente esta relación entre la asimetria y la detección sobrepasada de valores anomalos. Concretamente, nos fijamos en las variables instrumentalness, speechiness, liveness, song_duration i loudness, que en este orden son las variables que más outliers presentan de más a menos y, por otro lado, estan ordenadas segun la asimetria presentada en sus boxplots y histogramas.

El problema está en que hay una cantidad abudante de valores que se encuentran fuera del intervalo [Q1,Q3], reflejado en los boxplots como puntos, y estos són detectados como outliers incorrectamene. Una posible solución seria escalar estas variables.

### 4.1.4. Z-Score

```{r}
# Función para un histograma z-score con líneas ±3
hist_z3 <- function(vec, var_name, binwidth = NULL) {
  # Filtrar no finitos
  x <- vec[is.finite(vec)]
  # Si no hay datos válidos o sd = 0, devolver un panel informativo
  if (length(x) == 0 || sd(x, na.rm = TRUE) == 0) {
    return(
      ggplot() +
        annotate("text", x = 0, y = 0, label = paste0(var_name, "\nSin variación o sin datos válidos")) +
        theme_void() + labs(title = var_name)
    )
  }
  # z-scores (scale devuelve matriz -> convertir a numérico)
  z <- as.numeric(scale(x))

  # Binwidth automático si no se especifica
  if (is.null(binwidth)) {
    r <- diff(range(z))
    binwidth <- if (is.finite(r) && r > 0) r / 30 else 0.5
  }

  ggplot(data.frame(z = z), aes(x = z)) +
    geom_histogram(binwidth = binwidth, fill = "skyblue", color = "black", boundary = 0) +
    geom_vline(xintercept = c(-3, 3), linetype = "dashed", color = "red", linewidth = 0.8) +
    theme_minimal() +
    labs(title = var_name, x = "z-score", y = "Frecuencia")
}

# Construir todos los plots
plots <- lapply(varNum, function(v) hist_z3(datos[[v]], v))

# Mostrar en cuadrícula (ajusta ncol según prefieras)
wrap_plots(plots, ncol = 4) + plot_annotation(title = "Histogramas (z-score)")

```

```{r}
library(tidyr)

# Función auxiliar: métricas y outliers z>|3| por variable 
calc_z_outliers <- function(x, var_name) {
  # Mantener solo valores finitos
  xf <- x[is.finite(x)]
  n_total   <- length(x)
  n_finite  <- sum(is.finite(x))
  mu <- mean(xf, na.rm = TRUE)
  s  <- sd(xf, na.rm = TRUE)

  if (!is.finite(s) || s == 0 || n_finite == 0) {
    return(list(
      summary = tibble(
        variable = var_name, n = n_total, n_finite = n_finite,
        mean = mu, sd = s,
        thr_low = NA_real_, thr_high = NA_real_,
        n_out_z3 = 0, pct_out_z3 = 0
      ),
      detail = tibble(
        variable = character(0), row_id = integer(0),
        value = numeric(0), z = numeric(0)
      )
    ))
  }

  # Umbrales en escala ORIGINAL equivalentes a |z|>3
  thr_low  <- mu - 3*s
  thr_high <- mu + 3*s

  # Índices (en el data.frame original) de outliers por z>|3|
  idx_finite <- which(is.finite(x))
  z_vals <- (xf - mu) / s
  out_mask <- abs(z_vals) > 3
  out_idx  <- idx_finite[out_mask]

  summary_tbl <- tibble(
    variable = var_name,
    n = n_total,
    n_finite = n_finite,
    mean = mu, sd = s,
    thr_low = thr_low, thr_high = thr_high,   # μ±3σ en escala original
    n_out_z3 = length(out_idx),
    pct_out_z3 = ifelse(n_finite > 0, 100*length(out_idx)/n_finite, 0)
  )

  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value  = x[out_idx],
    z      = (x[out_idx] - mu) / s
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas 
res_list <- map(varNum, ~ calc_z_outliers(datos[[.x]], .x))

tabla_resumen_z3 <- bind_rows(map(res_list, "summary")) %>%
  arrange(desc(n_out_z3))

tabla_detalle_z3 <- bind_rows(map(res_list, "detail")) %>%
  arrange(variable, desc(abs(z)))

#  Resultados:
tabla_resumen_z3
# head(tabla_detalle_z3)  # si solo quieres una vista
```

Vemos que con este escalado parece que hemos solucionado el problema que teniamos con la detección de outliers via IQR. Ahora tenemos outliers en 7 de las 10 variables y en todos suponen menos del 6% de las observaciones y menos de 2% en todos menos en los 2 con mas valores anómalos.

### 4.1.5. Hampel Identifier

Utilizamos la mediana y la desviación absoluta meidana en vez de la media.

```{r}
# Función para detectar outliers con Hampel
hampel_outliers <- function(x, var_name) {
  # Filtrar valores finitos
  x_finite <- x[is.finite(x)]
  if (length(x_finite) == 0) {
    return(list(
      summary = tibble(variable = var_name, median = NA_real_, mad = NA_real_,
                       lower_bound = NA_real_, upper_bound = NA_real_,
                       n_outliers = 0, pct_outliers = 0),
      detail = tibble(variable = character(0), row_id = integer(0), value = numeric(0))
    ))
  }

  med <- median(x_finite, na.rm = TRUE)
  madv <- mad(x_finite, constant = 1, na.rm = TRUE)
  lower <- med - 3 * madv
  upper <- med + 3 * madv

  idx_finite <- which(is.finite(x))
  out_idx <- idx_finite[which(x_finite < lower | x_finite > upper)]

  # Tabla resumen
  summary_tbl <- tibble(
    variable = var_name,
    median = med,
    mad = madv,
    lower_bound = lower,
    upper_bound = upper,
    n_outliers = length(out_idx),
    pct_outliers = 100 * length(out_idx) / length(x_finite)
  )

  # Tabla detallada
  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value = x[out_idx]
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas
res_hampel <- map(varNum, ~ hampel_outliers(datos[[.x]], .x))

# Combinar resultados en tablas 
tabla_resumen_hampel <- bind_rows(map(res_hampel, "summary")) %>%
  arrange(desc(n_outliers))

tabla_detalle_hampel <- bind_rows(map(res_hampel, "detail")) %>%
  arrange(variable, row_id)

#  Mostrar resultados
tabla_resumen_hampel        # resumen por variable
# head(tabla_detalle_hampel)  # detalle de los outliers (índices y valores)

```
Este método queda descartado para nuestra base de datos ya que se detecta como outlier más del 20% de las observaciones en cuatro variables y más del 5 en 8 de ellas. 

Ademmás, antes de su aplicación ya intuiamos que no iba a resultar productivo este metodo ya que operar con la mediana y la desvación absoluta mediana en distribuciones asimétricas no es compatible.

### 4.1.6. Statistics Tests

#### 4.1.6.1. Grubb's Test

Detección de valores extremos en distribuciones Gaussianas.

Por lo tanto, primero estudiamos que variables admiten este test.
```{r}
library(nortest)  

# Anderson–Darling para cada variable
ad_por_variable <- function(x, var_name) {
  x <- x[is.finite(x)]
  if (length(x) < 8) {
    return(tibble(variable = var_name, n = length(x), p_ad = NA_real_, decision = "No test (<8)"))
  }
  p <- tryCatch(nortest::ad.test(x)$p.value, error = function(e) NA_real_)
  tibble(
    variable = var_name,
    n = length(x),
    p_ad = p,
    decision = ifelse(!is.na(p) && p >= 0.05, "≈ Normal (apto Grubbs)", "No normal (No apto Grubbs)")
  )
}

resultado_AD <- map_dfr(varNum, ~ ad_por_variable(datos[[.x]], .x)) %>%
  arrange(decision, desc(p_ad))

resultado_AD

```

No hay ninguna variable que acepte este test, ya que en todas no se puede asumir normalidad, por lo tanto no será usado en nuestro estudio.

#### 4.1.6.2. Dixon's Test

Este test también queda descartado ya que solo es posible aplicarlo en bbdd pequeñas de entre 3 y 30 observaciones. Teniendo 13186 observaciones el uso de esta prueba no es viable.

#### 4.1.6.3. Rosner's Test

Este test parece ser aplicable ya que es aplicable para muestras grandes y además cuenta con la ventaja de la detección múltiple de valores atípicos. No obstante, este asume normalidad en los datos y ya hemos comprobado que ninguna variable la cumple en el primer subapartado de esta sección. 

Por lo tanto, no hemos podido aplicar ninguno de los tests en nuestra bbdd pero hemos concluido de este apartado que contamos con distribuciones no normales.

### 4.1.7. Conclusiones

Después de haber aplicado todos los métodos de tratamiento de valores anomalos en nuestra base de datos hemos visto que con las tres primeras secciones detectabamos muchos outliers incorrectamente debido a problmas de asimetria y de escala.

Ha sido al escalar las variables (Z-Score) cuando hemos podido recoger un conjunto de posibles valores atípicos más coherente en el que poder imputar aquellos que concluyamos como outliers definitivos. Estos estan guardados con su respectivos ID y variable correspondiente en tabla_detalle_z3.

```{r}
head(tabla_detalle_z3,6)
table(tabla_detalle_z3$variable)
```

## 4.2. Multivariate

Scatter plot de los trios de variable más interesantes (correlacionados):
```{r}
library(scatterplot3d)

num <- datos[, varNum, drop = FALSE]

# Matriz de correlaciones (por pares, ignorando NA)
C <- cor(num, use = "pairwise.complete.obs")

# Puntuar todos los tríos por media de |cor| en sus 3 pares
comb3  <- combn(varNum, 3, simplify = FALSE)
scores <- vapply(comb3, function(v) {
  mean(abs(c(C[v[1], v[2]], C[v[1], v[3]], C[v[2], v[3]])), na.rm = TRUE)
}, numeric(1))

# Seleccionar los N tríos “más interesantes”
N   <- 6 # cambia si quieres más/menos
ord <- order(scores, decreasing = TRUE)
top_trios <- comb3[ord][seq_len(min(N, length(comb3)))]

#  Mostrar ranking
ranking <- data.frame(
  rank = seq_along(top_trios),
  trio = sapply(top_trios, paste, collapse = " · "),
  score = round(scores[ord][seq_along(top_trios)], 3),
  row.names = NULL
)
print(ranking)

# Graficar (con escalado para comparabilidad)
scale_axes <- TRUE
old_par <- par(mfrow = c(ceiling(length(top_trios) / 3), min(3, length(top_trios))))
on.exit(par(old_par), add = TRUE)

for (tri in top_trios) {
  x <- num[[tri[1]]]; y <- num[[tri[2]]]; z <- num[[tri[3]]]
  if (scale_axes) { x <- scale(x)[,1]; y <- scale(y)[,1]; z <- scale(z)[,1] }
  scatterplot3d(
    x, y, z,
    main = paste(tri, collapse = " | "),
    xlab = tri[1], ylab = tri[2], zlab = tri[3],
    pch = 16
  )
}

```
Hacemos el gafico interactivo 3D para el unico trio que presenta una correlación conjunta superior a 0.5 (loudness, energy, acousticness)
```{r, warning=FALSE}
library(plotly)

(fig <- plotly::plot_ly(datos, x = ~loudness, y = ~energy, z = ~acousticness, size = 1) %>% 
       add_markers())
```
### 4.2.1. Caso general

Como contamos con una matriz de covarianza singular con colinealidad fuerte entre algunas de nuestras variables, debemos aplicar fallback a PCA en vez de la forma común dada en la teoria:
```{r}
library(mvoutlier)

# Subset y saneo de columnas 
X <- datos[, varNum, drop = FALSE]
# Asegurar numéricas
X <- X[, sapply(X, is.numeric), drop = FALSE]

# Quitar columnas con varianza 0 o todo NA
sd_cols <- sapply(X, function(x) sd(x, na.rm = TRUE))
X <- X[, sd_cols > 0 & !is.na(sd_cols), drop = FALSE]

# Quitar colinealidad casi perfecta (|r| >= 0.999)
if (ncol(X) > 1) {
  repeat {
    C <- suppressWarnings(cor(X, use = "pairwise.complete.obs"))
    up <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
    if (!nrow(up)) break
    # elimina la de menor varianza del par
    to_drop <- unique(colnames(X)[apply(up, 1, function(idx){
      v1 <- var(X[[idx[1]]], na.rm = TRUE)
      v2 <- var(X[[idx[2]]], na.rm = TRUE)
      if (v1 <= v2) idx[1] else idx[2]
    })])
    X <- X[, setdiff(colnames(X), to_drop), drop = FALSE]
  }
}

# Filas completas
Xc <- X[complete.cases(X), , drop = FALSE]

# Garantizar n > p
if (nrow(Xc) <= ncol(Xc)) {
  ord <- order(apply(Xc, 2, var, na.rm = TRUE), decreasing = TRUE)
  p_new <- max(1, nrow(Xc) - 1)
  Xc <- Xc[, ord[seq_len(p_new)], drop = FALSE]
}

##  DD-Plot con fallback robusto
dd_with_fallback <- function(M, quan = 1/2, alpha = 0.025) {
  M <- as.matrix(M)
  tryCatch(
    dd.plot(M, quan = quan, alpha = alpha),
    error = function(e) {
      message("Covarianza singular: hago fallback a PCA (scores). Detalle: ", e$message)
      pcs <- prcomp(M, center = TRUE, scale. = TRUE)
      # nº de componentes: hasta 95% var o n-1, lo que ocurra antes
      var_exp <- cumsum(pcs$sdev^2) / sum(pcs$sdev^2)
      k <- min(which(var_exp >= 0.95))
      k <- min(k, nrow(M) - 1, ncol(M))
      dd.plot(pcs$x[, seq_len(k), drop = FALSE], quan = quan, alpha = alpha)
    }
  )
}

distances <- dd_with_fallback(Xc, quan = 1/2, alpha = 0.025)

```

Las observaciones distribuidas en la rectangulo inferior izquierdo del dd plot són aquellas observaciónes con valores bajos en ambas distancias (no outliers).

Los que se encuentran en la parte superior izquierda, al contar con un valor de distancia robusta alto, són outliers robustos con una fuerte influencia.

Todos aquellos que se encuentran en el lado derecho de la linea vertica central són aquellos llamados "leverage points", observaciones que influyen mucho en la covarianza clásica.

Ahora, obtenemos los índices de aquellos puntos considerados outliers según las anteriores características.
```{r}
outliers<-distances$outliers
indOut<-which(outliers==TRUE)
indOut # 141 outliers
```

Visualizamos todos los outliers detectados como true
```{r}
head(distances$md.cla)
```
```{r}
head(distances$md.rob)
```

```{r}
head(outliers)
```
```{r}
table(outliers)
```
```{r}
distances_df<-as.data.frame(distances)
distances_df<-distances_df[order(-distances_df$md.rob),]
head(distances_df,10)
```
```{r}
p <- 10   
alpha <- 0.025
umbral2 <- qchisq(1 - alpha, df = p) # sobre D^2
umbral  <- sqrt(umbral2)             # sobre D (md.rob)

plot(distances_df$md.rob, 
     type = "h", 
     ylab = "Distancia de Mahalanobis robusta", 
     xlab = "Índice de observación",
     main = sprintf("Outliers multivariantes (p=%d)", p))
abline(h = umbral, col = "red", lwd = 2, lty = 2)
```

PROBLEMA: Con este metodo solo me deja calcular las distancias de observaciones sin NA's (376). El resto las excluye y solo puedo estudiar valores anomalos dentro de este subconjunto muy pequeño excluyendo las otras 12mil y pico obs.

### 4.2.2. PCA

Hay metodos basados en correlaciones que nos permiten detectar outliers. De momento no los ejecutaremos pero que sepamos que exite esa posibilidad en caso de encontrar dificultades si concluimos que nuestras variables regressoras presentan problemas de multicol·linealidad. Hasta ahora no lo parece.

### 4.2.3. Distancia de Mahalanobis 

```{r}
distancia_mahalanobis <- mahalanobis(datos[,varNum], colMeans(datos[,varNum]), cov(datos[,varNum]))
summary(distancia_mahalanobis)
```
Como vemos tenemos problemas de calculo con la distancia de mahalanobis debido a la cantidad de NA's que tenemos en nuestra bbdd. Haremos este metodo más adelante cuando Dante nos guie en si usar algun tipo de metodo de imputacion de NA's previo para calcular estas distancias.

### 4.2.4. Regresió Lineal i residus 

Un punto con un residuo grande puede considerarse un valor anomalo.

### 4.2.5. Distancia de Cook

Podemos calcular las distancias de Cook para discriminar como outliers aquellas observaciones que cuenten con un valor superior a 1.

### 4.2.6. K-Nearest Neighbors (KNN) Outlier Score

```{r}
library(adamethods)
res_knn<-do_knno(datos[,varNum], k=1, top_n=3000)
```
Con este metodo conseguimos el identificador de las 3000 observaciones con un outliers scoore mas alto.

```{r}
## --- Parámetros ---
k <- 1              # usa el mismo k que en do_knno()
idx_targets <- res_knn  # tus 100 índices

## --- 1) Preparar matriz X con variables numéricas ---
X <- datos[, varNum, drop = FALSE]
p <- ncol(X)

## --- 2) Estandarizar por columna ignorando NAs ---
std_col <- function(v) {
  mu <- mean(v, na.rm = TRUE)
  sdv <- sd(v, na.rm = TRUE)
  if (is.na(sdv) || sdv == 0) return((v - mu))   # evita dividir por 0
  (v - mu) / sdv
}
Xs <- as.data.frame(lapply(X, std_col))
Xs <- as.matrix(Xs)  # para operaciones más rápidas

n <- nrow(Xs)

## --- 2b) Identificar variables con outliers ---

# Función auxiliar: detecta outliers por variable numérica
detect_outliers <- function(x, method = c("zscore", "iqr"), threshold = 3) {
  method <- match.arg(method)
  x_noNA <- x[!is.na(x)]
  
  if (method == "zscore") {
    z <- (x_noNA - mean(x_noNA)) / sd(x_noNA)
    outliers <- which(abs(z) > threshold)
  } else if (method == "iqr") {
    q1 <- quantile(x_noNA, 0.25)
    q3 <- quantile(x_noNA, 0.75)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    outliers <- which(x_noNA < lower | x_noNA > upper)
  }
  
  return(length(outliers))
}

# Aplicar a cada variable numérica
outlier_counts <- sapply(X, detect_outliers, method = "iqr")  # puedes cambiar "iqr" por "zscore"
outlier_vars <- names(outlier_counts[outlier_counts > 0])

## Resumen
cat("Variables con outliers detectados:\n")
print(outlier_vars)

cat("\nNúmero de outliers por variable:\n")
print(outlier_counts[outlier_counts > 0])

## (Opcional) Crear un data.frame resumen
resumen_outliers <- data.frame(
  variable = names(outlier_counts),
  n_outliers = outlier_counts,
  prop_outliers = outlier_counts / nrow(X)
)

# Ver las más afectadas
head(resumen_outliers[order(-resumen_outliers$n_outliers), ])

## --- 2c) Identificar y marcar outliers por observación y variable ---

# Función que devuelve un vector lógico (TRUE si es outlier)
is_outlier <- function(x, method = c("zscore", "iqr"), threshold = 3) {
  method <- match.arg(method)
  res <- rep(FALSE, length(x))
  
  if (method == "zscore") {
    z <- (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
    res <- abs(z) > threshold
  } else if (method == "iqr") {
    q1 <- quantile(x, 0.25, na.rm = TRUE)
    q3 <- quantile(x, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower <- q1 - 1.5 * iqr
    upper <- q3 + 1.5 * iqr
    res <- x < lower | x > upper
  }
  
  res[is.na(x)] <- FALSE  # los NA originales no cuentan como outliers
  return(res)
}

# Aplicar a cada variable (columna)
outlier_matrix <- as.data.frame(lapply(X, is_outlier, method = "iqr"))

# ¿Cuántos outliers por variable?
outlier_counts <- colSums(outlier_matrix)
outlier_vars <- names(outlier_counts[outlier_counts > 0])

cat("Variables con outliers detectados:\n")
print(outlier_vars)

# --- Crear resumen por observación ---
outlier_summary <- data.frame(
  obs = 1:nrow(X),
  n_outlier_vars = rowSums(outlier_matrix),
  vars_outlier = apply(outlier_matrix, 1, function(row) {
    paste(names(X)[which(row)], collapse = ", ")
  })
)

# Mostrar observaciones con al menos un outlier
outlier_summary_nonzero <- subset(outlier_summary, n_outlier_vars > 0)
head(outlier_summary_nonzero, 10)

# --- (Opcional) Reemplazar los outliers por NA en X ---
X_no_outliers <- X
for (v in names(X)) {
  X_no_outliers[outlier_matrix[[v]], v] <- NA
}

# Si luego quieres usar la versión sin outliers:
#   -> sustituye X por X_no_outliers en los pasos siguientes

## --- Recalcular matriz estandarizada después de imputar outliers ---
Xs <- as.data.frame(lapply(X_no_outliers, std_col))
Xs <- as.matrix(Xs)

n <- nrow(Xs)  # número de filas actual

## --- Validar índices ---
idx_targets <- idx_targets[idx_targets <= n]

if (length(idx_targets) == 0) {
  stop("Ningún índice válido en idx_targets; revisa el contenido de res_knn o el dataset.")
}


## --- 3) Función de distancia euclídea "pairwise" ajustada por missingness ---
## d_ij = sqrt( sum((xi-xj)^2 over m) * (p / m) ), con m vars observadas en el par
pairwise_dist_row <- function(i, Xs) {
  xi <- Xs[i, ]
  # Máscara NA por columnas respecto xi (TRUE si xi es no-NA)
  mask_i <- !is.na(xi)
  d <- rep(NA_real_, n)
  for (j in 1:n) {
    if (j == i) next
    xj <- Xs[j, ]
    mask <- mask_i & !is.na(xj)
    m <- sum(mask)
    if (m == 0) {
      d[j] <- NA_real_
    } else {
      diff2 <- xi[mask] - xj[mask]
      d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
    }
  }
  d
}

## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
  d <- pairwise_dist_row(i, Xs)
  # Ordenar ignorando NAs y excluyendo la propia observación
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  vec[k]
}

## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))

resultado_res_knn <- data.frame(
  fila = idx_targets,
  score_knn = scores_res
)

## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
  d <- pairwise_dist_row(i, Xs)
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))

## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)

medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv

## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]

## Ver top
head(resultado_res_knn, 10)
```


```{r}
plot(resultado_res_knn$score_robust_z)
abline(h=3)
```

```{r}
outliers <- resultado_res_knn$fila[resultado_res_knn$score_robust_z > 3]
length(outliers)
length(outliers)/13186
```
Debido a que la cantidad de outliers supongo apenas un 3,15% de las observaciones totales, por simplicidad procedemos a eliminarlos.
```{r}
datos<-datos[-outliers,]
```

# 5.NA's
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(datos,m=5,maxit=50,meth='pmm',seed=500)
```

# 6. Feature engineering

## 6.1 Feature selection

### 6.1.1 Null variance

Se identifican aquellas variables cuya varianza es prácticamente o completamente nula, ya que, al tener valores muy similares entre ellos, se consideran constantes y pueden estar altamente correlacionadas con el término independiente del modelo, lo que solo introduciría ruido.  

```{r, warning=FALSE}
library(caret)
# install.packages("idealista18")
require(idealista18)
library(tidyverse)

numeric_cols <- sapply(datos, is.numeric)
datos_num <- datos[, numeric_cols]
# ncol(datos_num); ncol(datos)
varianza <- nearZeroVar(datos_num, saveMetrics = T)
varianza
```


Esta salida muestra que no hay ninguna variable con varianza exactamente igual a cero ni cercana a 0, lo cual indica que no hay necesidad de eliminar ninguna variable por falta de información. Sin embargo, las variables referentes a $liveness$, $danceability$, $audio_valence$, $energy$, $speechiness$ y, especialmente, $time_signature$, $key$ y $audio_mode$, tienen un *percentUnique* relativamente bajo, dando a entender que tienen pocos valores distintos. 

### 6.1.2 Feature correlation

Una manera alternativa de mirar si hay variables redundantes es calculando la correlación entre las variables predictoras. Un coeficiente alto de correlación puede generar un problema de multicolinealidad, que puede afectar negativamente a las predicciones del modelo. Anteriormente, mediante un gráfico den correlaciones entre las variables numéricas, se observó una elevada correlación entre **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Se pretende encontrar correlaciones que superen un coeficiente de 0.6:
```{r, warning=FALSE}
datos_cor <- cor(na.omit(datos_num))
(alta_cor <- findCorrelation(datos_cor, cutoff = 0.6))
```

A través de la visualización del gráfico de correlaciones deducimos qué variables presentan una elevada correlación. 

```{r, warning=FALSE}
library("corrplot")

matriz_corr <- cor(datos_cor)
corrplot(matriz_corr, method = "circle")
```

La representación gráfica muestra una evidencia clara de correlación entre las variables previamente mencionadas. Este hecho nos impulsa a eliminar alguna de estas variables que ya están explicadas por otras. 

### 6.1.3 Linear combinations

Identificamos si existen combinaciones lineales entre las variables predictoras, otra medida que nos informa acerca de la posible correlación existente entre ellas. 

```{r, warning=FALSE}
datos_num_na <- tidyr::drop_na(datos_num) # Es necesario eliminar los NA.
(combos <- findLinearCombos(datos_num_na[,-c(1,13)]))
```

Mediante este método, no se detectan combinaciones lineales entre las variables. Esto conlleva a deducir que no hay ninguna variable que contenga esencialmente la misma información que otra desde una perspectiva lineal.

### 6.1.4 Wrapper

Se aplica el método **Wrapper** para seleccionar las variables más relevantes del modelo.

```{r, warning=FALSE}
# install.packages("caret")
library(caret)

# Paso 1: Preparar los datos
datos_num <- datos[, sapply(datos, is.numeric)]
datos_num <- na.omit(datos_num)

# Separar variable objetivo
target <- datos_num$song_popularity
input <- datos_num[, colnames(datos_num) != "song_popularity"]

# Eliminar variables altamente correlacionadas (correlación > 0.6)
matriz_cor <- cor(input)
variables_redundantes <- findCorrelation(matriz_cor, cutoff = 0.6)
cat("Variables eliminadas por alta correlación:", length(variables_redundantes), "\n")

if(length(variables_redundantes) > 0) {
  input <- input[, -variables_redundantes]
  cat("Variables restantes después de filtrar correlación:", ncol(input), "\n")
}

# Paso 2: Configurar control de RFE (técnica para seleccionar las mejores variables eliminando iterativamente las menos importantes)
control <- rfeControl(functions = lmFuncs,  # Usa regresión lineal
                      method = "cv",        # Validación cruzada
                      number = 5)           # 5-fold CV

# Paso 3: Ejecutar RFE
set.seed(123)
resultados_rfe <- rfe(input, target,
                      sizes = 1:(ncol(input)),  # Probar con 1 hasta todas las variables
                      rfeControl = control)

# Paso 4: Ver variables seleccionadas
print(resultados_rfe$optVariables)
```

La técnica de Eliminación Recursiva de Características (*RFE: Recursive Feature Elimination*) es un tipo de método **Wrapper** que selecciona las variables más relevantes para un modelo entrenándolo varias veces mediante un proceso de validación cruzada y eliminando en cada iteración las variables menos importantes. El algoritmo termina hasta encontrar el subconjunto que ofrece el mejor rendimiento. Cabe destacar que dicho método ha tenido en cuenta la correlación entre las variables predictoras. De esta manera, se ha obtenido un conjunto de variables explicativas que no superan un coeficiente de correlación de 0,6. 
Estas variables son: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$.



El análisis exhaustivo de selección de variables reveló un **conjunto óptimo** de **7 predictores** para el modelo de popularidad musical. Tras eliminar variables redundantes por alta correlación (> 0.6), el método **Wrapper RFE** identificó las características más relevantes: $danceability$, $audio_valence$, $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Este conjunto representa una reducción del 46% en dimensionalidad mientras mantiene el poder predictivo, asegurando la estabilidad del modelo mediante la eliminación de multicolinealidad.


## 6.2 Feature transformation

El objetivo de este apartado es mejorar la distribución de las variables, reducir el sesgo y preparar los datos para los algoritmos de modelado.

### 6.2.1 Análisis inicial de distribuciones

```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
  skewness_values <- sapply(datos, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                          ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                            "liveness", "instrumentalness", "acousticness", "loudness")

datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
```

Mientras que las variables correspondientes a $danceability$ y $audio_valence$ presentan un sesgo bajo, el resto de variables obtienen un valor de sesgo potencialmente alto, a excepción de la variable $acousticness$, donde el sesgo es moderado. 

### 6.2.2 Visualización de distribuciones originales

```{r, warning=FALSE}
cat("\n VISUALIZACIÓN DE DISTRIBUCIONES ORIGINALES:\n")

# Función para graficar distribuciones
graficar_distribuciones <- function(datos, titulo) {
  datos_long <- datos %>%
    pivot_longer(everything(), names_to = "Variable", values_to = "Valor")
  
  ggplot(datos_long, aes(x = Valor)) +
    geom_histogram(aes(y = ..density..), fill = "steelblue", alpha = 0.7, bins = 30) +
    geom_density(color = "red", linewidth = 1) +
    facet_wrap(~ Variable, scales = "free") +
    labs(title = titulo,
         x = "Valor", y = "Densidad") +
    theme_minimal()
}

# Graficar distribuciones originales
graficar_distribuciones(datos_transformar, "Distribuciones Originales - Antes de Transformación")
```

### 6.2.3 Aplicación de las transformaciones

```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(datos_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(datos_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
      datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(datos_transformados[[var]])
        datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
```

Las variables con sesgo alto (mayor que 0.5 en valor absoluto), son: $speechiness$, $liveness$, $instrumentalness$, $acousticness$ y $loudness$. Se aplican 3 transformaciones a cada una:

  **1.** `log`: Permite reducir el sesgo positivo fuerte y funciona mejor cuando hay valores extremos positivos.
  
  **2.** `sqrt`: Se usa especialmente cuando el sesgo es moderado, y es menos agresiva que log, pero buena para datos con ceros.

  **3.** `box-cox`: Encuentra la transformación óptima automáticamente, con el requisito de que los valores sean estrictamente positivos. 

```{r}
any(datos_transformados$instrumentalness==0)
```

Dado que $instrumentalness$ contiene algún valor igual a 0, se han aplicado solo las dos primeras transformaciones. 

### 6.2.4 Evaluación de Transformaciones

A continuación, se realiza una evaluativa 

```{r evaluacion_transformaciones, warning=FALSE}
# Función para evaluar efectividad de transformaciones
evaluar_transformaciones <- function(var_original, vars_transformadas, nombre_original) {
  resultados <- data.frame()
  
  sesgo_original <- e1071::skewness(var_original, na.rm = TRUE)
  
  for(transform_name in names(vars_transformadas)) {
    if(transform_name != nombre_original) {
      var_transformada <- vars_transformadas[[transform_name]]
      sesgo_transformado <- e1071::skewness(var_transformada, na.rm = TRUE)
      
      reduccion_sesgo <- abs(sesgo_original) - abs(sesgo_transformado)
      
      resultados <- rbind(resultados, data.frame(
        Variable = nombre_original,
        Transformacion = transform_name,
        Sesgo_Original = round(sesgo_original, 3),
        Sesgo_Transformado = round(sesgo_transformado, 3),
        Reduccion_Sesgo = round(reduccion_sesgo, 3),
        Efectiva = ifelse(abs(sesgo_transformado) < abs(sesgo_original) & 
                          abs(sesgo_transformado) < 1, "SÍ", "NO")
      ))
    }
  }
  
  return(resultados)
}

# Evaluar todas las transformaciones
resultados_evaluacion <- data.frame()

for(var in variables_a_transformar) {
  # Obtener todas las versiones de la variable
  vars_relacionadas <- datos_transformados %>% 
    select(starts_with(var))
  
  eval_var <- evaluar_transformaciones(
    datos_transformar[[var]], 
    vars_relacionadas,
    var
  )
  
  resultados_evaluacion <- rbind(resultados_evaluacion, eval_var)
}

cat("EVALUACIÓN DE TRANSFORMACIONES:\n")
print(resultados_evaluacion)

# Seleccionar la mejor transformación para cada variable
mejores_transformaciones <- resultados_evaluacion %>%
  filter(Efectiva == "SÍ") %>%
  group_by(Variable) %>%
  filter(abs(Sesgo_Transformado) == min(abs(Sesgo_Transformado))) %>%
  ungroup()

cat("\n MEJORES TRANSFORMACIONES SELECCIONADAS:\n")
print(mejores_transformaciones)
```


El análisis de transformaciones reveló que **Box-Cox** fue **óptima** para **variables** con **alto sesgo** ($speechiness$, $liveness$), logrando reducciones superiores al 90%, mientras que la **raíz cuadrada** resultó más **efectiva** para **sesgos moderados** ($acousticness$). En cambio, para la variable $instrumentalness$ no se salió efectiva ninguna de las dos modificaciones aplicadas. 
Las transformaciones seleccionadas aseguran que todas las variables cumplan con $|sesgo| < 0.5$, mejorando significativamente los supuestos de normalidad para el modelado posterior.


## 6.3 Feature extraction

### 6.3.1 Preparar datos para PCA

```{r extraccion_variables, warning=FALSE}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)

variables_para_analisis <- c("danceability", "speechiness_boxcox", "liveness_boxcox", "instrumentalness", "acousticness_sqrt", "loudness", "audio_valence")

# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
```

Se estudian las 7 variables que el método **Wrapper** había proporcionado como conjunto óptimo, pero teniendo en cuenta la transformación de las 3 variables anteriores. 

### 6.3.2 Análisis de Componentes Principales (PCA)

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
```

Dado que se disponen de 7 variables, existen 7 componentes principales. 

### 6.3.3 Determinar Componentes Principales

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
  cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
```

El **análisis** de **componentes principales** sobre las **7 variables óptimas** revela una estructura dimensional bien definida. Los primeros **5 componentes** explican individualmente **más del 10% de la varianza**, acumulando **86.7%** de la **información total**.

PC1 (29%) emerge como la dimensión principal, capturando casi un tercio de la variabilidad del conjunto de datos. La **distribución gradual** de la **varianza** entre los componentes indica que **ninguna** **variable** **domina** excesivamente el **análisis**, sino que **todas** **contribuyen** de manera **balanceada** a las diferentes dimensiones latentes.

Este resultado confirma que las **7 variables seleccionadas** por **RFE** contienen información complementaria y valiosa para el modelo, justificando la retención de múltiples componentes que capturan distintas facetas de las características musicales relevantes para predecir la popularidad.

### 6.3.4 Crear nuevas variables (componentes)

Se crean las nuevas variables, las cuales corresponden a las 5 primeras componentes principales, que no son más que combinaciones lineales de las variables originales. De este modo, se consigue explicar más de un 85% de la variabilidad de $song_popularity$.

```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
  
  # Dar nombres descriptivos
  nombres_descriptivos <- c()
  for(i in 1:length(componentes_importantes)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos <- c(nombres_descriptivos, nombre)
  }
  colnames(nuevos_componentes) <- nombres_descriptivos
  
  # Añadir al dataset original
  datos_final <- cbind(datos_transformados, nuevos_componentes)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
  
} else {
  datos_final <- datos_final_transformado
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```


```{r}
dat<-datos_transformados[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence", "loudness", "instrumentalness")]

library("corrplot")

matriz_corr <- cor(dat)
corrplot(matriz_corr, method = "circle")
```

Dado que en el Análisis de Componentes Principales ha dado como resultado que con **5 componentes principales** se explica más de un 85% de la variabilidad de la variable de respuesta, solo es necesario considerar 5 variables como predictoras. En este gráfico de correlaciones se observa que entre $loudness$ y $acousticness_sqrt$ la correlación sigue siendo bastante elevada. Esto significa que una variable ya está explicada por otra, así que se decide suprimir **loudness**. 

Por otra parte, como la variable $instrumentalness$ no reduce significativamente el sesgo mediante ninguna transformación, para evitar que este efecto pueda repercutir negativamente en la predicción del modelo, se decide eliminarla. 

Por lo tanto, las 5 variables seleccionadas son: $\text{speechiness_boxcox, liveness_boxcox, acousticness_sqrt, danceability, audio_valence}$

# 7. KNN

Base de datos con las variables seleccionadas y sus transformaciones
```{r}
datos_prepro<-datos_final[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence")]
datos_prepro$song_popularity<-datos_num$song_popularity
# View(datos_prepro)
```


## 7.1 Elección del valor óptimo de k

Para discutir cuál es el valor óptimo de k, se toman por referencia los cálculos del *RMSE* (**Raíz del Error Cuadrático Medio**) y del *MAPE* (**Error Percentual Absoluto Medio**). Las fórmulas de ambas expresiones se describen a continuación:

\[
\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n}}
\]

\[
\text{MAPE} = 100 \times \frac{1}{n} \sum_{t=1}^{n} \left| \frac{A_t - F_t}{A_t} \right|
\]

El **Error Percentual Absoluto Medio** (**MAPE**: *Mean Absolute Percentual Error*) mide el error porcentual medio entre los valores reales  ($A_t$) y los valores pronosticados ($F_t$). Una ventaja que tiene esta medida frente a la **Raíz del Error Cuadrático Medio**, es que, al ser porcentual, es adimensional y, por tanto, no depende de las unidades. De esta manera facilita su interpretación. 

Se usan distintos valores impares para *k* y se calcula el **RMSE** y el **MAPE** en cada uno de ellos para escoger aquél que proporciona un error mínimo.

```{r, warning=FALSE}
library(FNN)
set.seed(1994)

# División del conjunto
default_idx <- sample(nrow(datos_prepro), nrow(datos_prepro) * 0.7)
datos <- datos_prepro

train <- datos[default_idx, ]
test  <- datos[-default_idx, ]

X_train <- train[, -6]
X_test  <- test[, -6]
y_train <- train[, 6]
y_test  <- test[, 6]

# Convertimos todo a numérico
X_train <- data.frame(lapply(X_train, as.numeric))
X_test  <- data.frame(lapply(X_test, as.numeric))

# Función para calcular RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Función para calcular MAPE
mape <- function(actual, predicted) {
  non_zero <- actual != 0  # filtrar valores distintos de 0
  mean(abs((actual[non_zero] - predicted[non_zero]) / actual[non_zero])) * 100
}

# Vector de valores de k a probar
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)

# Calcular RMSE y MAPE para cada k
results <- data.frame(
  k = k_values,
  RMSE = numeric(length(k_values)),
  MAPE = numeric(length(k_values))
)

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  pred <- knn.reg(train = X_train, test = X_test, y = y_train, k = k)$pred
  
  results$RMSE[i] <- rmse(y_test, pred)
  results$MAPE[i] <- mape(y_test, pred)
}

# Mostrar los resultados
print(results)

# Encontrar el k con menor RMSE
best_rmse_k <- results$k[which.min(results$RMSE)]
best_rmse <- min(results$RMSE)

# Encontrar el k con menor MAPE
best_mape_k <- results$k[which.min(results$MAPE)]
best_mape <- min(results$MAPE)

cat("Mejor k según RMSE:", best_rmse_k, " (RMSE =", best_rmse, ")\n")
cat("Mejor k según MAPE:", best_mape_k, " (MAPE =", best_mape, ")\n")
```

Dado que el valor de k que minimiza el **RMSE** es 5, y el que minimiza el **MAPE** es 3, se procede a calcular un criterio combinado para elegir el que minimice ambos errores balanceados:

```{r}
results$score <- scale(results$RMSE) + scale(results$MAPE)
best_combined_k <- results$k[which.min(results$score)]
cat("Mejor k combinado:", best_combined_k, "\n")
```

Por lo tanto, el valor óptimo de k es **k=17**. 

## 7.2 Modelo con k óptimo 

Se usa **k=17** como valor óptimo para realizar el *KNN*.

```{r}
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test

# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)

# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
```

# 8. Association Rules

```{r, include=FALSE, warning=FALSE, message=FALSE}
# INSTALLING AND LOADING THE ARULES PACKAGE

# install.packages("arules")
# install.packages("arulesViz")
# install.packages("tidyverse")
# install.packages("arulesViz")
library(arules)
library(arulesViz)
library(FactoMineR)
library(tidyverse)

# Configurar opciones para mejor visualización
options(digits = 3)
```

```{r}
# Visión general de la base de datos preprocesada
names(datos_prepro) ## 5 predictoras
head(datos_prepro)
summary(datos_prepro)
str(datos_prepro)
```
## 8.1 Preparación y exploración de datos

Transformamos la base de datos en un archivo de transacciones. Para ello, todas las variables deben ser categóricas. 

```{r}
# Verificar estructura de los datos
cat("=== ESTRUCTURA INICIAL DE LOS DATOS ===\n")
str(datos_prepro)
summary(datos_prepro)

# Convertir variables lógicas a factores (si aplica)
cat("\n=== CONVERSIÓN DE VARIABLES ===\n")
for (j in 1:ncol(datos_prepro)) {
  if(class(datos_prepro[,j]) == "logical") {
    datos_prepro[,j] <- as.factor(datos_prepro[,j])
    cat("Convertida columna", j, names(datos_prepro)[j], "de logical a factor\n")
  }
}

# Verificar estructura final
cat("\n=== ESTRUCTURA FINAL ===\n")
str(datos_prepro)
```

## 8.2 Conversión a transacciones

```{r}
cat("=== CONVERSIÓN A TRANSACCIONES ===\n")

# Método robusto de conversión
tryCatch({
  datos_transacciones <- as(datos_prepro, "transactions")
}, error = function(e) {
  cat("Error en conversión directa, usando método alternativo...\n")
  # Convertir todas las columnas a factores primero
  datos_prepro_factors <- as.data.frame(lapply(datos_prepro, as.factor))
  datos_transacciones <- as(datos_prepro_factors, "transactions")
})

# Resumen de las transacciones
cat("\n=== RESUMEN DE TRANSACCIONES ===\n")
summary(datos_transacciones)

# Visualizar items más frecuentes
itemFrequencyPlot(datos_transacciones, 
                  topN = 15,
                  main = "15 Características Musicales Más Frecuentes",
                  col = "steelblue",
                  type = "relative")
```

```{r}
# Cantidad de transacciones e ítems
datos_transacciones
summary(datos_transacciones)
class(datos_transacciones)

# FUNCTION inspect
inspect(datos_transacciones[1:6])
transactionInfo(datos_transacciones[1:10])

# Distribución del tamaño de transacciones
SIZE <- size(datos_transacciones)
summary(SIZE)
quantile(SIZE, probs = seq(0,1,0.1))
```
Se puede comprobar que la base de datos transaccional contiene 373 transacciones y 1484 ítems. Del mismo modo, se observa que se podrán hacer itemsets de hasta tamaño 5. 

## 8.3 Análisis de itemsets frecuentes

```{r}
cat("=== ITEMSETS FRECUENTES ===\n")

# Probar con diferentes valores de support
supports <- c(0.01, 0.02, 0.03, 0.05)
itemsets_encontrados <- FALSE

for (sup in supports) {
  itemsets_frecuentes <- apriori(datos_transacciones,
                                parameter = list(support = sup,
                                               minlen = 2,
                                               maxlen = 3,  # Reducir longitud máxima
                                               target = "frequent itemsets"))
  
  cat("Support =", sup, "-> Itemsets encontrados:", length(itemsets_frecuentes), "\n")
  
  if (length(itemsets_frecuentes) > 0) {
    itemsets_encontrados <- TRUE
    break
  }
}

if (itemsets_encontrados) {
  cat("\n=== TOP ITEMSETS POR SUPPORT ===\n")
  # Mostrar todos los itemsets encontrados (hasta 10)
  n_mostrar <- min(10, length(itemsets_frecuentes))
  inspect(sort(itemsets_frecuentes, by = "support")[1:n_mostrar])
} else {
  cat("\n*** No se encontraron itemsets frecuentes. Probando con parámetros más flexibles... ***\n")
  
  # Intentar con parámetros más flexibles
  itemsets_frecuentes <- apriori(datos_transacciones,
                                parameter = list(support = 0.005,  # Más bajo
                                               minlen = 1,        # Permitir itemsets de 1
                                               maxlen = 2,
                                               target = "frequent itemsets"))
  
  cat("Con support = 0.005 -> Itemsets encontrados:", length(itemsets_frecuentes), "\n")
  
  if (length(itemsets_frecuentes) > 0) {
    n_mostrar <- min(10, length(itemsets_frecuentes))
    inspect(sort(itemsets_frecuentes, by = "support")[1:n_mostrar])
  } else {
    cat("*** No se pudieron encontrar itemsets frecuentes con los parámetros actuales ***\n")
    cat("*** Revisar la estructura de los datos transaccionales ***\n")
    print(summary(datos_transacciones))
  }
}
```

## 8.4 Generación de Reglas de asociación

```{r}
cat("=== GENERACIÓN DE REGLAS DE ASOCIACIÓN ===\n")

# Probar diferentes combinaciones de parámetros
parametros_list <- list(
  list(support = 0.01, confidence = 0.3),
  list(support = 0.005, confidence = 0.2),
  list(support = 0.02, confidence = 0.4)
)

reglas_encontradas <- FALSE

for (params in parametros_list) {
  reglas <- apriori(datos_transacciones,
                   parameter = list(support = params[[1]],
                                  confidence = params[[2]],
                                  minlen = 1,  # Permitir reglas más simples
                                  maxlen = 3))
  
  cat("Support =", params[[1]], "Confidence =", params[[2]], 
      "-> Reglas encontradas:", length(reglas), "\n")
  
  if (length(reglas) > 0) {
    reglas_encontradas <- TRUE
    break
  }
}

if (reglas_encontradas) {
  cat("Total de reglas generadas:", length(reglas), "\n")
  
  # Filtrar reglas interesantes (parámetros más flexibles)
  if (length(reglas) > 0) {
    calidad_reglas <- quality(reglas)
    reglas_interesantes <- reglas[
      calidad_reglas$lift > 1.0 &  # Más flexible
      calidad_reglas$confidence > 0.3  # Más flexible
    ]
    
    cat("Reglas interesantes (lift > 1.0, confidence > 0.3):", length(reglas_interesantes), "\n")
    
    if (length(reglas_interesantes) > 0) {
      # Ordenar y mostrar
      reglas_ordenadas <- sort(reglas_interesantes, by = "lift", decreasing = TRUE)
      n_mostrar <- min(15, length(reglas_ordenadas))
      cat("\n=== TOP", n_mostrar, "REGLAS POR LIFT ===\n")
      inspect(reglas_ordenadas[1:n_mostrar])
    }
  }
  
} else {
  cat("*** No se pudieron generar reglas con los parámetros estándar ***\n")
  reglas_interesantes <- reglas  # Vacío pero evita error
}
```


## 8.5 Análisis de Popularidad

```{r}
cat("=== ANÁLISIS SIMPLIFICADO DE POPULARIDAD ===\n")

# Enfoque directo - solo las mejores reglas
labels_transacciones <- itemLabels(datos_transacciones)
patron_popularidad <- grep("popular", labels_transacciones, ignore.case = TRUE, value = TRUE)[1]

if (!is.na(patron_popularidad)) {
  cat("Analizando variable:", patron_popularidad, "\n")
  
  # Solo generar reglas muy específicas y de alta calidad
  reglas_popularidad <- tryCatch({
    reglas <- apriori(datos_transacciones,
                     parameter = list(
                       support = 0.03,     # Solo patrones comunes
                       confidence = 0.7,   # Solo muy confiables  
                       minlen = 2,
                       maxlen = 3
                     ),
                     appearance = list(rhs = patron_popularidad, default = "lhs"))
    
    # Filtrar y limitar
    reglas_filtradas <- reglas[quality(reglas)$lift > 1.5]
    head(sort(reglas_filtradas, by = "lift", decreasing = TRUE), 10)  # TOP 10
    
  }, error = function(e) {
    cat("No se pudieron generar reglas específicas\n")
    return(reglas_interesantes)
  })
  
} else {
  reglas_popularidad <- head(reglas_interesantes, 10)  # Solo 10 mejores generales
}

cat("Reglas finales para análisis:", length(reglas_popularidad), "\n")

if (length(reglas_popularidad) > 0) {
  cat("\n=== REGLAS DE POPULARIDAD ===\n")
  inspect(reglas_popularidad)
}
```
## 8.6 Evaluación y visualización de resultados

```{r, warning=FALSE, message=FALSE}
# install.packages("arulesViz")
library(arulesViz)
cat("=== EVALUACIÓN DE REGLAS ===\n")

# Mostrar las mejores reglas
cat("\n=== MEJORES REGLAS POR LIFT ===\n")
mejores_reglas <- head(sort(reglas_popularidad, by = "lift", decreasing = TRUE), 15)
inspect(mejores_reglas)

# Métricas de calidad
cat("\n=== MÉTRICAS DE CALIDAD ===\n")
calidad <- quality(mejores_reglas)
print(summary(calidad))

# Visualizaciones
cat("\n=== CREANDO VISUALIZACIONES ===\n")

# Scatter plot de las reglas
plot(mejores_reglas, 
     method = "scatter", 
     main = "Distribución de Reglas - Support vs Confidence")

# Gráfico de matriz
plot(mejores_reglas, 
     method = "matrix", 
     main = "Matriz de Reglas")

# Gráfico interactivo (si está disponible)
tryCatch({
  plot(mejores_reglas, 
       method = "graph", 
       main = "Grafo de Reglas de Asociación")
}, error = function(e) {
  cat("Gráfico interactivo no disponible\n")
})
```

## 8.7 Exportación de resultados

```{r}
cat("=== EXPORTACIÓN DE RESULTADOS ===\n")

# Crear dataframe con las reglas
resultados_df <- as(mejores_reglas, "data.frame")

# Exportar a CSV
write.csv(resultados_df, "reglas_asociacion_musica.csv", row.names = FALSE)
cat("Resultados exportados a 'reglas_asociacion_musica.csv'\n")

# Mostrar resumen ejecutivo
cat("\n=== RESUMEN EJECUTIVO ===\n")
cat("Total reglas generadas:", length(reglas), "\n")
cat("Reglas filtradas interesantes:", length(reglas_interesantes), "\n")
cat("Mejores reglas identificadas:", length(mejores_reglas), "\n")
cat("Lift máximo:", max(quality(mejores_reglas)$lift), "\n")
cat("Confidence máximo:", max(quality(mejores_reglas)$confidence), "\n")
```

**Resultados**

```{r}
cat("=== INTERPRETACIÓN ===\n")
cat("Las reglas muestran patrones entre características musicales y popularidad.\n")
cat("Lift > 1 indica asociación positiva significativa.\n")
cat("Confidence > 0.6 indica reglas confiables.\n")
cat("Support indica la frecuencia del patrón en las transacciones.\n")
```


# 9. Submission

Preliminares
```{r}
test$key <- factor(test$key)
test$audio_mode <- factor(test$audio_mode) # 0 = menor, 1 = mayor
test$time_signature <- factor(test$time_signature)

clases <- sapply(test, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```

Outliers
```{r}

```

Na's
```{r, warning=FALSE, include=FALSE}
library(mice)
tempData <- mice(test,m=5,maxit=50,meth='pmm',seed=500)
```

Ingenieria de variables

Analisis
```{r transformacion_variables, warning=FALSE}
library(caret)
library(e1071)
library(recipes)
library(tidyverse)

# Usar el dataset con las variables seleccionadas del paso anterior
# Asumimos que tenemos: datos_final con las 7 variables seleccionadas

cat("ANÁLISIS INICIAL DE DISTRIBUCIONES Y SESGO:\n")

# Función para calcular estadísticas de sesgo
analizar_sesgo <- function(datos) {
  skewness_values <- sapply(datos, function(x) {
    if(is.numeric(x)) {
      sesgo <- e1071::skewness(x, na.rm = TRUE)
      tipo_sesgo <- ifelse(abs(sesgo) > 1, "ALTO SESGO",
                           ifelse(abs(sesgo) > 0.5, "SESGO MODERADO", "BAJO SESGO"))
      return(c(Sesgo = round(sesgo, 3), Tipo = tipo_sesgo))
    } else {
      return(c(Sesgo = NA, Tipo = "NO NUMÉRICA"))
    }
  })
  
  t(skewness_values) %>% as.data.frame() %>% 
    rownames_to_column("Variable")
}

# Aplicar análisis de sesgo a nuestras variables seleccionadas
variables_seleccionadas <- c("danceability", "audio_valence", "speechiness", 
                             "liveness", "instrumentalness", "acousticness", "loudness")

datos_transformar <- datos_num[, variables_seleccionadas]
resultados_sesgo <- analizar_sesgo(datos_transformar)
print(resultados_sesgo)
```

Aplicacion transformacion
```{r aplicacion_transformaciones, warning=FALSE}
# Identificar variables que necesitan transformación (|sesgo| > 0.5)
variables_a_transformar <- resultados_sesgo %>%
  filter(Tipo %in% c("ALTO SESGO", "SESGO MODERADO")) %>%
  pull(Variable)

cat("VARIABLES A TRANSFORMAR (|sesgo| > 0.5):\n")
print(variables_a_transformar)

# Aplicar diferentes transformaciones
datos_transformados <- datos_transformar

for(var in variables_a_transformar) {
  if(var %in% colnames(datos_transformados)) {
    
    # Obtener valores mínimos para ajustar transformaciones
    min_val <- min(datos_transformados[[var]], na.rm = TRUE)
    
    # Aplicar transformaciones según el tipo de variable
    if(min_val >= 0) {
      # Para variables con valores positivos
      datos_transformados[[paste0(var, "_log")]] <- log1p(datos_transformados[[var]])
      datos_transformados[[paste0(var, "_sqrt")]] <- sqrt(datos_transformados[[var]])
      
      # Transformación Box-Cox (requiere valores estrictamente positivos)
      if(min_val > 0) {
        bc_transform <- BoxCoxTrans(datos_transformados[[var]])
        datos_transformados[[paste0(var, "_boxcox")]] <- predict(bc_transform, datos_transformados[[var]])
      }
    }
    
  }
}

cat("TRANSFORMACIONES APLICADAS:\n")
cat("Variables originales:", ncol(datos_transformar), "\n")
```

PCA
```{r extraccion_variables, warning=FALSE}
# INSTALAR Y CARGAR LIBRERÍAS
#install.packages(c("FactoMineR", "factoextra", "nFactors"))
library(FactoMineR)
library(factoextra)
library(nFactors)

cat("COMIENZO EXTRACCIÓN DE VARIABLES\n")
cat("===================================\n\n")


cat("1. PREPARANDO DATOS PARA ANÁLISIS...\n")

# Usar las 7 variables óptimas del método RFE (con las transformaciones aplicadas)

variables_para_analisis <- c("danceability", "speechiness_boxcox", "liveness_boxcox", "instrumentalness", "acousticness_sqrt", "loudness", "audio_valence")

# Crear dataset solo con estas variables (excluir objetivo)
datos_analisis <- datos_transformados[, variables_para_analisis]

# Estandarizar (importante para PCA)
datos_estandarizados <- scale(datos_analisis)

cat("   • Variables analizadas:", paste(variables_para_analisis, collapse = ", "), "\n")
cat("   • Dimensiones datos:", dim(datos_estandarizados), "\n\n")
```

```{r, warning=FALSE}
cat("2. EJECUTANDO ANÁLISIS DE COMPONENTES PRINCIPALES...\n")

pca_resultado <- prcomp(datos_estandarizados, scale. = TRUE)

# Resumen simple
cat("   • PCA completado\n")
cat("   • Número de componentes:", length(pca_resultado$sdev), "\n\n")
```

```{r, warning=FALSE}
cat("3. IDENTIFICANDO COMPONENTES IMPORTANTES...\n")

# Calcular varianza explicada
varianza_explicada <- pca_resultado$sdev^2 / sum(pca_resultado$sdev^2) * 100

# Mostrar varianza por componente
cat("   Varianza explicada por componente:\n")
for(i in 1:length(varianza_explicada)) {
  cat("   • PC", i, ": ", round(varianza_explicada[i], 1), "%\n", sep = "")
}

# Regla simple: componentes que explican >10% de varianza
componentes_importantes <- which(varianza_explicada > 10)
cat("\n   • Componentes que explican >10% varianza: PC", 
    paste(componentes_importantes, collapse = ", PC"), "\n", sep = "")
```

Creacion variables finales
```{r, warning=FALSE}
cat("\n4. CREANDO NUEVAS VARIABLES...\n")

if(length(componentes_importantes) > 0) {
  # Extraer scores de componentes importantes
  nuevos_componentes <- as.data.frame(pca_resultado$x[, componentes_importantes])
  
  # Dar nombres descriptivos
  nombres_descriptivos <- c()
  for(i in 1:length(componentes_importantes)) {
    nombre <- paste0("Componente_", i)
    nombres_descriptivos <- c(nombres_descriptivos, nombre)
  }
  colnames(nuevos_componentes) <- nombres_descriptivos
  
  # Añadir al dataset original
  datos_final <- cbind(datos_transformados, nuevos_componentes)
  
  cat("   • Nuevas variables creadas:", paste(nombres_descriptivos, collapse = ", "), "\n")
  
} else {
  datos_final <- datos_final_transformado
  cat("   • No se crearon nuevos componentes (poca varianza explicada)\n\n")
}
```

```{r}
datos_prepro<-datos_final[,c("speechiness_boxcox", "liveness_boxcox", "acousticness_sqrt", "danceability", "audio_valence")]
```

Modelado
```{r}
modelo_knn <- knn.reg(train = X_train, test = X_test, y = y_train, k = 5)
yp <- modelo_knn$pred
y  <- y_test

# --- Cálculo de errores ---
e1 <- (y - yp)^2
e2 <- abs(y - yp)

# --- RMSE final ---
rmse <- sqrt(mean(e1))
cat("RMSE del modelo con k =", 17, ":", rmse, "\n")
```






`

