---
title: "proyecto regression"
author: "vega carmona"
date: "2025-09-30"
output:
  html_document: default
  pdf_document: default
---
# 0. Paquetes y Datos
```{r setup, include=FALSE}
# Paquetes básicos que usas
library(tidyverse)   # incluye ggplot2 y dplyr (%>%)
library(DataExplorer)
library(inspectdf)
library(skimr)
library(SmartEDA)
library(naniar)
library(forcats)# utilidades para factors
library(readr)
library(dlookr)# para diagnose()
library(EnvStats) # IQR
library(dplyr)
library(purrr)

# Evitar el warning de xts vs dplyr::lag (si lo usas)
options(xts.warn_dplyr_breaks_lag = FALSE)
```

```{r}
# setwd("C:/Users/Vega Carmona/OneDrive/ESTADISTICA/QUART/MEMDA/projecte")
# setwd("C:/Users/Usuario/OneDrive/Documentos/ESTADÍSTICA/4-1 ESTADÍSTICA/MEMDA/TRABAJO")
#setwd("C:/Users/raque/OneDrive/Documentos/GitHub/Trabajo_MEMDA")
datos <- read_csv("train.csv")
test <- read_csv("test.csv")
```

# 1. Descripción del problema
```{r, warning=FALSE}
str(datos)
```
Preliminares
```{r}
# Conversión a factores
datos$key <- factor(datos$key)
datos$audio_mode <- factor(datos$audio_mode) # 0 = menor, 1 = mayor
datos$time_signature <- factor(datos$time_signature)

clases <- sapply(datos, class)

varNum <- names(clases)[which(clases %in% c("numeric", "integer"))]
varNum<-varNum[!varNum %in% c("ID","song_popularity")]
varCat <- names(clases)[which(clases %in% c("character", "factor"))]
```

Comprobación
```{r}
str(datos)
```

# 2. Análisis Exploratorio

## 2.1. Univariant Analysis

### 2.1.1. Numerical

Descrpition
```{r}
library(psych)
psych::describe(datos[, varNum])
```

En base a este resumen estadístico se puede afirmar que la mayoría de las canciones no son en vivo, dado que el promedio de la variable $liveness$ es bastante bajo. Respecto a el volumen de las canciones, teniendo en cuenta que $loudness$ es una característica que oscila entre -36.73dB y 1.34dB, su valor medio parece indicar que es relativamente alto. 

La media de $danceability$ apunta que las canciones son bastante bailables, y el valor esperado de $audio_valence$ muestra que las canciones tienden a tener emociones más alegres y positivas, ya que es ligeramente superior a 0,5. 

Asimismo, haciendo referencia a cómo de energéticas o animadas son dichas canciones, $energy$ señala que, en promedio, puede decirse que lo son bastante. 

En cuanto al grado acústico, observando que el valor medio es bajo, se podría deducir que mayoritariamente las canciones son producidas electrónicamente. 

Por último, se podría añadir que gran parte de las canciones no son habladas, pues la variable $speechiness$ tiene un promedio bajo. Destaca la variabilidad del nivel de popularidad ($song_popularity$), con canciones desde poco conocidas hasta muy populares.

Graphics (base)
```{r}
for (var in varNum[-1]) {
  # Acceder a la columna por nombre y asegurarse de que es numérica
  column_data <- as.numeric(datos[[var]])
  
  # Comprobar si la columna es numérica
  if (is.numeric(column_data)) {
    # Eliminar NAs
    column_data <- na.omit(column_data)
    
    # Comprobar si quedan datos después de eliminar NAs
    if (length(column_data) > 0) {
      hist(column_data, main = paste0("Histograma variable ", var))
      boxplot(column_data, main = paste0("Boxplot variable ", var))
    } else {
      warning(paste("La variable", var, "está vacía o tiene solo NA y será ignorada"))
    }
  } else {
    warning(paste("La variable", var, "no es numérica y será ignorada"))
  }
}

par(mfrow = c(1, 1))


```

Graphics (ggplot2 + patchwork)
```{r, eval=FALSE}
library(ggplot2)
library(patchwork)

plots <- list()

for (var in varNum) {
  histo <- ggplot(datos, aes(x = .data[[var]])) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
    geom_density(alpha = .2, fill = "#FF6666") +
    geom_vline(aes(xintercept = mean(.data[[var]], na.rm = TRUE)),
               color = "blue", linetype = "dashed", linewidth = 1) +
    ggtitle(paste("Histograma de", var))
  boxp <- ggplot(datos, aes(x = .data[[var]])) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) +
    ggtitle(paste("Boxplot de", var))

  plots <- append(plots, list(histo, boxp))
  print(plots)
}
```

Combinar en un grid automático con 2 columnas
```{r, eval=FALSE, include=FALSE}
final_plot <- Reduce(`+`, plots) + plot_layout(ncol = 2)
final_plot
```

### 2.1.2. Categorical

Description
```{r}
for (var in varCat) {
  tablaAbs <- data.frame(table(datos[, var]))
  tablaFreq <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  m <- match(tablaAbs$Var1, tablaFreq$Var1)
  tablaAbs[, "FreqRel"] <- tablaFreq[m, "Freq"]
  colnames(tablaAbs) <- c("Categoria", "FreqAbs", "FreqRel")
  cat("===============", var, "===================================/n")
  print(tablaAbs)
  cat("==================================================/n")
}
```
Las frecuencias absolutas de las variables categóricas, muestran, en general, una distribución equitativa de las modalidades. Sin embargo, la variable $time_signature$ posee un mayor número de casos en la cuarta categoría.

Graphics (base)
```{r}
# Configurar la ventana gráfica para mostrar 2 filas y 3 columnas
par(mfrow = c(2, 3))

# Crear gráficos de barras con títulos personalizados
for (var in varCat) {
  if (var == "time_signature") {
    barplot(table(datos[, var]),
            main = "Distribución de Time Signature",
            col = "skyblue")
  } else if (var == "key") {
    barplot(table(datos[, var]),
            main = "Distribución de Key",
            col = "lightgreen")
  } else if (var == "audio_mode") {
    barplot(table(datos[, var]),
            main = "Distribución de Audio Mode",
            col = "lightcoral")
  } else {
    barplot(table(datos[, var]),
            main = var,  # título genérico si hay más variables
            col = "gray80")
  }
}

# Restaurar la configuración original
par(mfrow = c(1, 1))
```

-   **time_signature**:

la categoría 4 es la más frecuente, con casi 8000 canciones. Esto corresponde al compás 4/4, el más común en música popular. El resto de niveles, 0, 1, 3 y 5 son mucho menos frecuentes, lo que indica que compases alternativos o inusuales son raros en esta colección de canciones.

-   **key**:

La distribución es bastante uniforme, con cada tonalidad apareciendo entre 1000 y 2000 veces. No hay una tonalidad claramente dominante, lo que sugiere diversidad armónica en el conjunto de datos.

-   **audio_mode**:

Predominan las canciones en modo mayor, lo que puede reflejar una tendencia hacia música más optimista o comercial.

Graphics (ggplot2 + gridExtra)
```{r}
library(gridExtra)

plots <- list()  # lista vacía
i <- 1           # índice

for (var in varCat) {
  tabla <- data.frame(table(datos[, var]) / sum(table(datos[, var])))
  p <- ggplot(data = tabla, aes(x = Var1, y = Freq)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(round(Freq * 100, 2), "%")),
              vjust = 1.6, color = "white", size = 3.5) +
    theme_minimal() +
    labs(title = paste("Distribución de", var), x = var, y = "Proporción")

  plots[[i]] <- p
  i <- i + 1
}
# Mostrar todos los gráficos en un grid (ejemplo con 2 columnas)
# grid.arrange(grobs = plots, ncol = 2)
```

## 2.2. Bivariant Analysis

#### 2.2.1 Numerical vs. numerical

Description

```{r}
cor(na.omit(datos[, varNum]))
```

Las correlaciones con valores más extremos se encuentran entre las variables **acousticness** y **loudness**, **instrumentalness** y **loudness**, **loudness** y **energy**, **acousticness** y **energy**.

Graphics (base / PerformanceAnalytics)

```{r}
library(PerformanceAnalytics)
chart.Correlation(as.matrix(datos[, varNum]), histogram = TRUE, pch = 12)
```

Graphics (ggplot2 / ggcorrplot)

```{r, include=FALSE, eval=FALSE}
library(ggcorrplot)
corr <- round(cor(datos[, varNum]), 1)
ggcorrplot(corr, lab = TRUE)
```

#### 2.2.2 Numerical vs. categorical

Description

```{r}
for (varN in varNum) {
  for (varC in varCat) {
   print(psych::describeBy(datos[, varN], group = datos[, varC]))
  }
}
```

Graphics (ggplot2)

```{r}
library(ggplot2)
library(gridExtra)

plots <- list()
i <- 1

for (varC in varCat) {
  for (varN in varNum) {
    grafico <- ggplot(datos, aes(x = .data[[varN]], fill = .data[[varC]])) +
      geom_histogram(colour = "black",
                     lwd = 0.75,
                     linetype = 1,
                     position = "identity",
                     alpha = 0.5) +
      labs(title = paste("Histograma de", varN, "por", varC),
           x = varN, y = "Frecuencia", fill = varC) +
      theme_minimal()
    plots[[i]] <- grafico
    i <- i + 1
  }
}
# Mostrar todos en un grid (2 columnas)
grid.arrange(grobs = plots, ncol = 2)
```

#### 2.2.3 Categorical vs. categorical

Description

```{r}
for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      prop_table <- prop.table(table(datos[, varc1], datos[, varc2]))
      cat("=============", varc1, " vs. ", varc2, "=========================/n")
      print(prop_table)
    }
  }
}
```

Graphics (base)

```{r}
par(mfrow = c(3, 3))
for (varc1 in varCat) {
  for (varc2 in varCat) {
    if (varc1 != varc2) {
      prop_table <- prop.table(table(datos[, varc1], datos[, varc2]))
      barplot(prop_table, beside = TRUE)
    }
  }
}
par(mfrow = c(1, 1))
```

# 3. EDA (Automatic Descriptive Analysis)

## 3.1. Skim

```{r}
library(skimr)
library(tidyverse)

# Podem visualitzar un descriptiu de les dades
skim(datos)
```

```{r}
# Visualitzem exclusivament les variables numériques
skim(datos) %>% yank("numeric")
```

```{r}
skim(datos) %>% yank("character")
```

## 3.2 Vis

```{r}
library(visdat)
## Busquem per a variables numériques o categóriques si hi ha NA's
vis_dat(datos)
```

Este gráfico representa que la mayoría de las variables son numéricas, con menos cantidad de variables categóricas. También se osberva que hay un gran número de valores missings en los dos tipos de variables.

```{r}
## Visualitzem percentatges de NA's en les variables
vis_miss(datos)
```

Concretamente, hay un total de 26% de valores perdidos.

```{r}
## Generem la matriu de correlacions
datos %>% dplyr::select(where(is.numeric)) %>% vis_cor()
```

Mediante la visualización de este gráfico de correlaciones, se refuerzan las relaciones lineales entre los diferentes pares de variables.

```{r}
## Podem visualitzar condicionants de les dades. En aquest cas, mirem si tenim mes de
## 2 clases
vis_expect(datos, ~ .x > 2)
```

## 3.3. Inspect df

```{r}
library(inspectdf)

## Tipus de dades
inspect_types(datos) %>% show_plot()
```

La base de datos contiene 3 variables factor, y 11 cuantitativas.

```{r}
## Utilització de la memoria
inspect_mem(datos) %>% show_plot()
```

```{r}
# Paquetes
library(dplyr)
library(inspectdf)
library(rlang)

# --- 1) Crear una categórica desde una numérica y comparar (High/Low) ---
# Usamos 'song_popularity' como ejemplo con umbral 50.
umbral <- 50
num_target <- "song_popularity"

stopifnot(num_target %in% c(
  "liveness","loudness","danceability","song_duration_ms","audio_valence",
  "energy","tempo","acousticness","speechiness","instrumentalness","song_popularity"
))

# Convertir a numérico por si viniera como texto
datos[[num_target]] <- suppressWarnings(as.numeric(datos[[num_target]]))

data_price_dummy <- datos %>%
  mutate(price_dummy = if_else(.data[[num_target]] > umbral, "High", "Low") %>% factor())

# Comparativa de NA entre High y Low
inspect_na(
  data_price_dummy %>% filter(price_dummy == "High"),
  data_price_dummy %>% filter(price_dummy == "Low")
) %>% show_plot()

```

```{r}
## Comprovem la distribució de les variables
inspect_num(datos) %>% show_plot()
```

```{r}
## check categorical variable distribution
inspect_imb(datos) %>% show_plot()
```

```{r}
## check two categorical
inspect_imb(data_price_dummy %>% dplyr::filter(price_dummy == "High"),
            data_price_dummy %>% dplyr::filter(price_dummy == "Low")) %>%
  show_plot() + theme(legend.position = "none")
```

```{r}
## similiar to inspect_imb, but for all levels
inspect_cat(datos) %>% show_plot()
```

```{r, eval=FALSE, include=FALSE}
inspect_cor(datos) %>% show_plot()
```

## 3.4 DataExplorer

```{r}
library(DataExplorer)
plot_str(datos)
introduce(datos)
```

```{r}
plot_intro(datos)
```

```{r}
plot_missing(datos)
```

Tal y como se muestra en esta ilustración, se ha asignado un 30% de valores missings en cada variable.

```{r}
plot_bar(datos)
```

Cabe destacar que la variable $key$ posee una gran cantidad de valores faltantes, algo que deberá de tratarse en el preprocesamiento de los datos para evitar el sesgo y que este hecho pueda cambiar significativamente los resultados. Asimismo, $time_signature$ también presenta bastantes valores perdidos.

```{r}
plot_histogram(datos)
```

Muchas de las distribuciones de las variables parecen tener asimetría, bien sea por la izquierda o por la derecha. Esto induce a pensar que variables como $acousticness$, $instrumentalness$, $speechiness$ o $loudness$ tienden a tomar valores más extremos.

```{r}
library(DataExplorer)

# Opción 1: omitir filas con NA
plot_correlation(na.omit(datos), type = "all", maxcat = 5L)
```

## 3.5. SmartEDA
```{r}
library("SmartEDA")
## Overview of the data
ExpData(data = datos,type = 1)

## structure of the data    
ExpData(data = datos,type = 2)
```

```{r}
SmartEDA::ExpCTable(datos,Target=NULL,margin=1,clim=10,nlim=5,round=2,bin=NULL,per=T)
```

# 4. Outliers treatment

```{r}
options(scipen = 999)
diagnose_numeric(datos)
```

Vemos que nos identifica muchisimos outliers (mas de 300 en 5 de las variables numericas) lo que inidca que el criterio de selección de anomalias debe ser mas estricto.

```{r}
diagnose_category(datos)
```

## 4.1. Univariate

### 4.1.1. Max and Min

```{r}
mapply(function(x, name) {
  cat("var. ", name, ": \n\t min: ", min(x, na.rm = TRUE),
      "\n\t max: ", max(x, na.rm = TRUE), "\n")
  invisible(NULL)
}, datos[, varNum], colnames(datos[, varNum]))
```

Vemos que existen outliers en todas la variables en tanto que el maximo o el minimo de estas se encuentra altamente alejado del valor del tercer o el primer quantil respectivamente. És una primer hipòtesis que nos lleva a ejecutar la identificación de estos outliers.

### 4.1.2. IQR

$$[Q1-1.5*IQR,Q3+1.5*IQR]\\IQR=[Q3-Q1]$$

```{r}
IQROutlier <- function(variable, rmnas = TRUE) {
  IQ <- IQR(variable, na.rm = rmnas)
  intInf <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[1]] - 1.5 * IQ
  intSup <- quantile(variable, probs = c(0.25, 0.75), na.rm = rmnas)[[2]] + 1.5 * IQ
  posicions <- which(variable >= intSup | variable <= intInf)
  if (length(posicions) > 0) {
    cat("Existeixen outliers en les posicions:", paste0(posicions, collapse = ", "), "\n")
  } else {
    cat("No existeixen outliers\n")
  }
  return(posicions)
}
```

Aplicamos para todas las variables numéricas

```{r}
mapply(function(x, name) {
  cat("\n\nVariable:", name, "\n")
  IQROutlier(x)
}, datos[, varNum], varNum)
```

Tenemos muchisimos outliers por variable y esta visualización es poco usable para tratar con ellos. Vemos que el metodo IQR detecta como outliers muchas observaciones en aquellas variables que presentan asimetria en sus distribuciones. Recordemos los histogramas de estas variables y observamos como a mayor es la asimetria del grafico en una variable mas outliers detecta el metodo IQR:

```{r}
plot_histogram(datos)
```

Conluimos para este apartados que podriamos considerar realmente outliers aquellos detectados por IQR en distribuciones gaussianas como en danceability por ejemplo.

### 4.1.3. Boxplot

```{r, warning=FALSE}
library(patchwork)

plots <- lapply(varNum, function(v) {
  ggplot(datos, aes(y = datos[[v]])) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = v, y = v) +
    theme_minimal()
})

wrap_plots(plots)
```

Graficando los boxplots vemos de forma más evidente esta relación entre la asimetria y la detección sobrepasada de valores anomalos. Concretamente, nos fijamos en las variables instrumentalness, speechiness, liveness, song_duration i loudness, que en este orden son las variables que más outliers presentan de más a menos y, por otro lado, estan ordenadas segun la asimetria presentada en sus boxplots y histogramas.

El problema está en que hay una cantidad abudante de valores que se encuentran fuera del intervalo [Q1,Q3], reflejado en los boxplots como puntos, y estos són detectados como outliers incorrectamene. Una posible solución seria escalar estas variables.

### 4.1.4. Z-Score

```{r}
# Función para un histograma z-score con líneas ±3
hist_z3 <- function(vec, var_name, binwidth = NULL) {
  # Filtrar no finitos
  x <- vec[is.finite(vec)]
  # Si no hay datos válidos o sd = 0, devolver un panel informativo
  if (length(x) == 0 || sd(x, na.rm = TRUE) == 0) {
    return(
      ggplot() +
        annotate("text", x = 0, y = 0, label = paste0(var_name, "\nSin variación o sin datos válidos")) +
        theme_void() + labs(title = var_name)
    )
  }
  # z-scores (scale devuelve matriz -> convertir a numérico)
  z <- as.numeric(scale(x))

  # Binwidth automático si no se especifica
  if (is.null(binwidth)) {
    r <- diff(range(z))
    binwidth <- if (is.finite(r) && r > 0) r / 30 else 0.5
  }

  ggplot(data.frame(z = z), aes(x = z)) +
    geom_histogram(binwidth = binwidth, fill = "skyblue", color = "black", boundary = 0) +
    geom_vline(xintercept = c(-3, 3), linetype = "dashed", color = "red", linewidth = 0.8) +
    theme_minimal() +
    labs(title = var_name, x = "z-score", y = "Frecuencia")
}

# Construir todos los plots
plots <- lapply(varNum, function(v) hist_z3(datos[[v]], v))

# Mostrar en cuadrícula (ajusta ncol según prefieras)
wrap_plots(plots, ncol = 4) + plot_annotation(title = "Histogramas (z-score)")

```
```{r}
library(tidyr)

# Función auxiliar: métricas y outliers z>|3| por variable 
calc_z_outliers <- function(x, var_name) {
  # Mantener solo valores finitos
  xf <- x[is.finite(x)]
  n_total   <- length(x)
  n_finite  <- sum(is.finite(x))
  mu <- mean(xf, na.rm = TRUE)
  s  <- sd(xf, na.rm = TRUE)

  if (!is.finite(s) || s == 0 || n_finite == 0) {
    return(list(
      summary = tibble(
        variable = var_name, n = n_total, n_finite = n_finite,
        mean = mu, sd = s,
        thr_low = NA_real_, thr_high = NA_real_,
        n_out_z3 = 0, pct_out_z3 = 0
      ),
      detail = tibble(
        variable = character(0), row_id = integer(0),
        value = numeric(0), z = numeric(0)
      )
    ))
  }

  # Umbrales en escala ORIGINAL equivalentes a |z|>3
  thr_low  <- mu - 3*s
  thr_high <- mu + 3*s

  # Índices (en el data.frame original) de outliers por z>|3|
  idx_finite <- which(is.finite(x))
  z_vals <- (xf - mu) / s
  out_mask <- abs(z_vals) > 3
  out_idx  <- idx_finite[out_mask]

  summary_tbl <- tibble(
    variable = var_name,
    n = n_total,
    n_finite = n_finite,
    mean = mu, sd = s,
    thr_low = thr_low, thr_high = thr_high,   # μ±3σ en escala original
    n_out_z3 = length(out_idx),
    pct_out_z3 = ifelse(n_finite > 0, 100*length(out_idx)/n_finite, 0)
  )

  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value  = x[out_idx],
    z      = (x[out_idx] - mu) / s
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas 
res_list <- map(varNum, ~ calc_z_outliers(datos[[.x]], .x))

tabla_resumen_z3 <- bind_rows(map(res_list, "summary")) %>%
  arrange(desc(n_out_z3))

tabla_detalle_z3 <- bind_rows(map(res_list, "detail")) %>%
  arrange(variable, desc(abs(z)))

#  Resultados:
tabla_resumen_z3
# head(tabla_detalle_z3)  # si solo quieres una vista
```

Vemos que con este escalado parece que hemos solucionado el problema que teniamos con la detección de outliers via IQR. Ahora tenemos outliers en 7 de las 10 variables y en todos suponen menos del 6% de las observaciones y menos de 2% en todos menos en los 2 con mas valores anómalos.

### 4.1.5. Hampel Identifier

Utilizamos la mediana y la desviación absoluta meidana en vez de la media.

```{r}
# Función para detectar outliers con Hampel
hampel_outliers <- function(x, var_name) {
  # Filtrar valores finitos
  x_finite <- x[is.finite(x)]
  if (length(x_finite) == 0) {
    return(list(
      summary = tibble(variable = var_name, median = NA_real_, mad = NA_real_,
                       lower_bound = NA_real_, upper_bound = NA_real_,
                       n_outliers = 0, pct_outliers = 0),
      detail = tibble(variable = character(0), row_id = integer(0), value = numeric(0))
    ))
  }

  med <- median(x_finite, na.rm = TRUE)
  madv <- mad(x_finite, constant = 1, na.rm = TRUE)
  lower <- med - 3 * madv
  upper <- med + 3 * madv

  idx_finite <- which(is.finite(x))
  out_idx <- idx_finite[which(x_finite < lower | x_finite > upper)]

  # Tabla resumen
  summary_tbl <- tibble(
    variable = var_name,
    median = med,
    mad = madv,
    lower_bound = lower,
    upper_bound = upper,
    n_outliers = length(out_idx),
    pct_outliers = 100 * length(out_idx) / length(x_finite)
  )

  # Tabla detallada
  detail_tbl <- tibble(
    variable = var_name,
    row_id = out_idx,
    value = x[out_idx]
  )

  list(summary = summary_tbl, detail = detail_tbl)
}

# Aplicar a todas las variables numéricas
res_hampel <- map(varNum, ~ hampel_outliers(datos[[.x]], .x))

# Combinar resultados en tablas 
tabla_resumen_hampel <- bind_rows(map(res_hampel, "summary")) %>%
  arrange(desc(n_outliers))

tabla_detalle_hampel <- bind_rows(map(res_hampel, "detail")) %>%
  arrange(variable, row_id)

#  Mostrar resultados
tabla_resumen_hampel        # resumen por variable
# head(tabla_detalle_hampel)  # detalle de los outliers (índices y valores)

```
Este método queda descartado para nuestra base de datos ya que se detecta como outlier más del 20% de las observaciones en cuatro variables y más del 5 en 8 de ellas. 

Ademmás, antes de su aplicación ya intuiamos que no iba a resultar productivo este metodo ya que operar con la mediana y la desvación absoluta mediana en distribuciones asimétricas no es compatible.

### 4.1.6. Statistics Tests

#### 4.1.6.1. Grubb's Test

Detección de valores extremos en distribuciones Gaussianas.

Por lo tanto, primero estudiamos que variables admiten este test.
```{r}
library(nortest)  

# Anderson–Darling para cada variable
ad_por_variable <- function(x, var_name) {
  x <- x[is.finite(x)]
  if (length(x) < 8) {
    return(tibble(variable = var_name, n = length(x), p_ad = NA_real_, decision = "No test (<8)"))
  }
  p <- tryCatch(nortest::ad.test(x)$p.value, error = function(e) NA_real_)
  tibble(
    variable = var_name,
    n = length(x),
    p_ad = p,
    decision = ifelse(!is.na(p) && p >= 0.05, "≈ Normal (apto Grubbs)", "No normal (No apto Grubbs)")
  )
}

resultado_AD <- map_dfr(varNum, ~ ad_por_variable(datos[[.x]], .x)) %>%
  arrange(decision, desc(p_ad))

resultado_AD

```

No hay ninguna variable que acepte este test, ya que en todas no se puede asumir normalidad, por lo tanto no será usado en nuestro estudio.

#### 4.1.6.2. Dixon's Test

Este test también queda descartado ya que solo es posible aplicarlo en bbdd pequeñas de entre 3 y 30 observaciones. Teniendo 13186 observaciones el uso de esta prueba no es viable.

#### 4.1.6.3. Rosner's Test

Este test parece ser aplicable ya que es aplicable para muestras grandes y además cuenta con la ventaja de la detección múltiple de valores atípicos. No obstante, este asume normalidad en los datos y ya hemos comprobado que ninguna variable la cumple en el primer subapartado de esta sección. 

Por lo tanto, no hemos podido aplicar ninguno de los tests en nuestra bbdd pero hemos concluido de este apartado que contamos con distribuciones no normales.

### 4.1.7. Conclusiones

Después de haber aplicado todos los métodos de tratamiento de valores anomalos en nuestra base de datos hemos visto que con las tres primeras secciones detectabamos muchos outliers incorrectamente debido a problmas de asimetria y de escala.

Ha sido al escalar las variables (Z-Score) cuando hemos podido recoger un conjunto de posibles valores atípicos más coherente en el que poder imputar aquellos que concluyamos como outliers definitivos. Estos estan guardados con su respectivos ID y variable correspondiente en tabla_detalle_z3.

```{r}
head(tabla_detalle_z3,6)
table(tabla_detalle_z3$variable)
```

## 4.2. Multivariate

Scatter plot de los trios de variable más interesantes (correlacionados):
```{r}
library(scatterplot3d)

num <- datos[, varNum, drop = FALSE]

# Matriz de correlaciones (por pares, ignorando NA)
C <- cor(num, use = "pairwise.complete.obs")

# Puntuar todos los tríos por media de |cor| en sus 3 pares
comb3  <- combn(varNum, 3, simplify = FALSE)
scores <- vapply(comb3, function(v) {
  mean(abs(c(C[v[1], v[2]], C[v[1], v[3]], C[v[2], v[3]])), na.rm = TRUE)
}, numeric(1))

# Seleccionar los N tríos “más interesantes”
N   <- 6 # cambia si quieres más/menos
ord <- order(scores, decreasing = TRUE)
top_trios <- comb3[ord][seq_len(min(N, length(comb3)))]

#  Mostrar ranking
ranking <- data.frame(
  rank = seq_along(top_trios),
  trio = sapply(top_trios, paste, collapse = " · "),
  score = round(scores[ord][seq_along(top_trios)], 3),
  row.names = NULL
)
print(ranking)

# Graficar (con escalado para comparabilidad)
scale_axes <- TRUE
old_par <- par(mfrow = c(ceiling(length(top_trios) / 3), min(3, length(top_trios))))
on.exit(par(old_par), add = TRUE)

for (tri in top_trios) {
  x <- num[[tri[1]]]; y <- num[[tri[2]]]; z <- num[[tri[3]]]
  if (scale_axes) { x <- scale(x)[,1]; y <- scale(y)[,1]; z <- scale(z)[,1] }
  scatterplot3d(
    x, y, z,
    main = paste(tri, collapse = " | "),
    xlab = tri[1], ylab = tri[2], zlab = tri[3],
    pch = 16
  )
}

```
Hacemos el gafico interactivo 3D para el unico trio que presenta una correlación conjunta superior a 0.5 (loudness, energy, acousticness)
```{r, warning=FALSE}
library(plotly)

(fig <- plotly::plot_ly(datos, x = ~loudness, y = ~energy, z = ~acousticness, size = 1) %>% 
       add_markers())
```
### 4.2.1. Caso general

Como contamos con una matriz de covarianza singular con colinealidad fuerte entre algunas de nuestras variables, debemos aplicar fallback a PCA en vez de la forma común dada en la teoria:
```{r}
library(mvoutlier)

# Subset y saneo de columnas 
X <- datos[, varNum, drop = FALSE]
# Asegurar numéricas
X <- X[, sapply(X, is.numeric), drop = FALSE]

# Quitar columnas con varianza 0 o todo NA
sd_cols <- sapply(X, function(x) sd(x, na.rm = TRUE))
X <- X[, sd_cols > 0 & !is.na(sd_cols), drop = FALSE]

# Quitar colinealidad casi perfecta (|r| >= 0.999)
if (ncol(X) > 1) {
  repeat {
    C <- suppressWarnings(cor(X, use = "pairwise.complete.obs"))
    up <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
    if (!nrow(up)) break
    # elimina la de menor varianza del par
    to_drop <- unique(colnames(X)[apply(up, 1, function(idx){
      v1 <- var(X[[idx[1]]], na.rm = TRUE)
      v2 <- var(X[[idx[2]]], na.rm = TRUE)
      if (v1 <= v2) idx[1] else idx[2]
    })])
    X <- X[, setdiff(colnames(X), to_drop), drop = FALSE]
  }
}

# Filas completas
Xc <- X[complete.cases(X), , drop = FALSE]

# Garantizar n > p
if (nrow(Xc) <= ncol(Xc)) {
  ord <- order(apply(Xc, 2, var, na.rm = TRUE), decreasing = TRUE)
  p_new <- max(1, nrow(Xc) - 1)
  Xc <- Xc[, ord[seq_len(p_new)], drop = FALSE]
}

##  DD-Plot con fallback robusto
dd_with_fallback <- function(M, quan = 1/2, alpha = 0.025) {
  M <- as.matrix(M)
  tryCatch(
    dd.plot(M, quan = quan, alpha = alpha),
    error = function(e) {
      message("Covarianza singular: hago fallback a PCA (scores). Detalle: ", e$message)
      pcs <- prcomp(M, center = TRUE, scale. = TRUE)
      # nº de componentes: hasta 95% var o n-1, lo que ocurra antes
      var_exp <- cumsum(pcs$sdev^2) / sum(pcs$sdev^2)
      k <- min(which(var_exp >= 0.95))
      k <- min(k, nrow(M) - 1, ncol(M))
      dd.plot(pcs$x[, seq_len(k), drop = FALSE], quan = quan, alpha = alpha)
    }
  )
}

distances <- dd_with_fallback(Xc, quan = 1/2, alpha = 0.025)

```

Las observaciones distribuidas en la rectangulo inferior izquierdo del dd plot són aquellas observaciónes con valores bajos en ambas distancias (no outliers).

Los que se encuentran en la parte superior izquierda, al contar con un valor de distancia robusta alto, són outliers robustos con una fuerte influencia.

Todos aquellos que se encuentran en el lado derecho de la linea vertica central són aquellos llamados "leverage points", observaciones que influyen mucho en la covarianza clásica.

Ahora, obtenemos los índices de aquellos puntos considerados outliers según las anteriores características.
```{r}
outliers<-distances$outliers
indOut<-which(outliers==TRUE)
indOut # 141 outliers
```

Visualizamos todos los outliers detectados como true
```{r}
head(distances$md.cla)
```
```{r}
head(distances$md.rob)
```

```{r}
head(outliers)
```
```{r}
table(outliers)
```
```{r}
distances_df<-as.data.frame(distances)
distances_df<-distances_df[order(-distances_df$md.rob),]
head(distances_df,10)
```
```{r}
p <- 10   
alpha <- 0.025
umbral2 <- qchisq(1 - alpha, df = p) # sobre D^2
umbral  <- sqrt(umbral2)             # sobre D (md.rob)

plot(distances_df$md.rob, 
     type = "h", 
     ylab = "Distancia de Mahalanobis robusta", 
     xlab = "Índice de observación",
     main = sprintf("Outliers multivariantes (p=%d)", p))
abline(h = umbral, col = "red", lwd = 2, lty = 2)
```

PROBLEMA: Con este metodo solo me deja calcular las distancias de observaciones sin NA's (376). El resto las excluye y solo puedo estudiar valores anomalos dentro de este subconjunto muy pequeño excluyendo las otras 12mil y pico obs.

### 4.2.2. PCA

Hay metodos basados en correlaciones que nos permiten detectar outliers. De momento no los ejecutaremos pero que sepamos que exite esa posibilidad en caso de encontrar dificultades si concluimos que nuestras variables regressoras presentan problemas de multicol·linealidad. Hasta ahora no lo parece.

### 4.2.3. Distancia de Mahalanobis 

```{r}
distancia_mahalanobis <- mahalanobis(datos[,varNum], colMeans(datos[,varNum]), cov(datos[,varNum]))
summary(distancia_mahalanobis)
```
Como vemos tenemos problemas de calculo con la distancia de mahalanobis debido a la cantidad de NA's que tenemos en nuestra bbdd. Haremos este metodo más adelante cuando Dante nos guie en si usar algun tipo de metodo de imputacion de NA's previo para calcular estas distancias.

### 4.2.4. Regresió Lineal i residus 

Un punto con un residuo grande puede considerarse un valor anomalo.

### 4.2.5. Distancia de Cook

Podemos calcular las distancias de Cook para discriminar como outliers aquellas observaciones que cuenten con un valor superior a 1.

### 4.2.6. K-Nearest Neighbors (KNN) Outlier Score

```{r}
library(adamethods)
res_knn<-do_knno(datos[,varNum], k=1, top_n=100)
```
Con este metodo conseguimos el identificador de las 100 observaciones con un outliers scoore mas alto.

```{r}
## --- Parámetros ---
k <- 1              # usa el mismo k que en do_knno()
idx_targets <- res_knn  # tus 100 índices

## --- 1) Preparar matriz X con variables numéricas ---
X <- datos[, varNum, drop = FALSE]
p <- ncol(X)

## --- 2) Estandarizar por columna ignorando NAs ---
std_col <- function(v) {
  mu <- mean(v, na.rm = TRUE)
  sdv <- sd(v, na.rm = TRUE)
  if (is.na(sdv) || sdv == 0) return((v - mu))   # evita dividir por 0
  (v - mu) / sdv
}
Xs <- as.data.frame(lapply(X, std_col))
Xs <- as.matrix(Xs)  # para operaciones más rápidas

n <- nrow(Xs)

## --- 3) Función de distancia euclídea "pairwise" ajustada por missingness ---
## d_ij = sqrt( sum((xi-xj)^2 over m) * (p / m) ), con m vars observadas en el par
pairwise_dist_row <- function(i, Xs) {
  xi <- Xs[i, ]
  # Máscara NA por columnas respecto xi (TRUE si xi es no-NA)
  mask_i <- !is.na(xi)
  d <- rep(NA_real_, n)
  for (j in 1:n) {
    if (j == i) next
    xj <- Xs[j, ]
    mask <- mask_i & !is.na(xj)
    m <- sum(mask)
    if (m == 0) {
      d[j] <- NA_real_
    } else {
      diff2 <- xi[mask] - xj[mask]
      d[j] <- sqrt(sum(diff2 * diff2) * (p / m))
    }
  }
  d
}

## --- 4) Obtener el k-NN distance para cada índice objetivo respecto a TODO el dataset ---
get_knn_dist <- function(i, Xs, k = 1) {
  d <- pairwise_dist_row(i, Xs)
  # Ordenar ignorando NAs y excluyendo la propia observación
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  vec[k]
}

## --- 5) Calcular scores para tus 100 observaciones ---
scores_res <- vapply(idx_targets, function(i) get_knn_dist(i, Xs, k = k), numeric(1))

resultado_res_knn <- data.frame(
  fila = idx_targets,
  score_knn = scores_res
)

## --- (Opcional) Si quieres una versión 'avg_k' (media de las k distancias más cercanas) ---
get_knn_avgdist <- function(i, Xs, k = 5) {
  d <- pairwise_dist_row(i, Xs)
  vec <- sort(d[!is.na(d)], partial = k)
  if (length(vec) < k) return(NA_real_)
  mean(vec[1:k])
}
# ejemplo: avg_scores_res <- vapply(idx_targets, function(i) get_knn_avgdist(i, Xs, k = 5), numeric(1))

## --- (Opcional) Normalizaciones monótonas para alinear la "escala" de do_knno ---
## Mantienen el ranking, por si do_knno aplica alguna de estas:
minv <- min(scores_res, na.rm = TRUE); maxv <- max(scores_res, na.rm = TRUE)
resultado_res_knn$score_minmax_0_1 <- (resultado_res_knn$score_knn - minv) / (maxv - minv)

medv <- median(scores_res, na.rm = TRUE); madv <- mad(scores_res, constant = 1, na.rm = TRUE)
resultado_res_knn$score_robust_z <- (resultado_res_knn$score_knn - medv) / madv

## --- Ordenar por mayor sospecha (score más grande) ---
resultado_res_knn <- resultado_res_knn[order(-resultado_res_knn$score_knn), ]

## Ver top
head(resultado_res_knn, 10)
```
```{r}
plot(resultado_res_knn$score_knn)
```

PROBLEMA: Solo tenemos los outliers scores de las primeras 100 observaciones con mayor valor. Podemos hacerlo para las primeras 100 y estudiar donde encontramos el codo en el grafico de dispersion y escoger un umbral de decisión como criterio de selección de outliers.

### 4.2.7. Local Outlier Factor (LOF)

```{r}
library(DMwR2)
library(dplyr)

outlier.scores <- lofactor(datos[, varNum], k = 5)
par(mfrow=c(1,1))
plot(density(outlier.scores))
outlier.scores
outliers <- order(outlier.scores, decreasing=T)
outliers <- order(outlier.scores, decreasing=T)[1:5]
```


#### 4.2.7.1. Nueva versión de LOF

### 4.2.8. Isolation Forest














